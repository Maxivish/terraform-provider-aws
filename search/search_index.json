{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome # The Terraform AWS Provider is the work of thousands of contributors, and is maintained by a small team within HashiCorp. This site contains extensive instructions about how to contribute and how the AWS provider works. Please Note: This documentation is intended for Terraform AWS Provider code developers. Typical operators writing and applying Terraform configurations do not need to read or understand this material. Contribute # Please follow the following steps to ensure your contribution goes smoothly. 1. Configure Development Environment # Install Terraform and Go. Clone the repository, compile the provider, and set up testing. Refer to Configure Development Environment . 2. Write Code # Follow the guide for your contribution type and refer to the Development Reference materials as needed for additional details about provider design , expected naming conventions , guidance for error handling , etc. Contribution Guide Description Resources Allow the management of a logical resource within AWS by adding a new resource to the Terraform AWS Provider. Data Source Let your Terraform configurations use data from resources not under local management by creating ready only data sources. Services Allow Terraform (via the AWS Provider) to manage an entirely new AWS service by introducing the resources and data sources required to manage configuration of the service. AWS Region New regions are immediately usable with the provider with the caveat that a configuration workaround is required to skip validation of the region during cli operations. A small set of changes are required to makes this workaround necessary. Bug Fix or Enhancement These constitute the majority of pull requests submitted, many of which we address and merge regardless of priority in our regular internal gardening days. Resource Name Generation Allow a resource to either fully, or partially, generate its own resource names. This can be useful in cases where the resource name uniquely identifes the resource and it needs to be recreated. It can also be used when a name is required, but the specific name is not important. Tagging Support Many AWS resources allow assigning metadata via tags. However, frequently AWS services are launched without tagging support so this will often need to be added later. Import Support Adding import support allows terraform import to be run targeting an existing unmanaged resource and pulling its configuration into Terraform state. Typically import support is added during initial resource implementation but in some cases this will need to be added later. Documentation Changes The provider documentation is displayed on the Terraform Registry and is sourced and refreshed from the provider repository during the release process. 3. Write Tests # We require changes to be covered by acceptance tests for all contributions. If you are unable to pay for acceptance tests for your contributions, mention this in your pull request. We will happily accept \"best effort\" acceptance tests implementations and run them for you on our side. Your PR may take longer to merge, but this is not a blocker for contributions. 4. Update the Changelog # HashiCorp's open-source projects have always maintained a user-friendly, readable CHANGELOG.md that allows users to tell at a glance whether a release should have any effect on them, and to gauge the risk of an upgrade. Not all changes require an entry in the changelog, refer to our Changelog Process for details about when and how to create a changelog. 5. Create a Pull Request # When your contribution is ready, Create a Pull Request in the AWS provider repository. Pull requests are usually triaged within a few days of creation and are prioritized based on community reactions. Our Prioritization Guides provides more details about the process. Submit an Issue # In addition to contributions, we welcome bug reports and feature requests .","title":"Welcome"},{"location":"#welcome","text":"The Terraform AWS Provider is the work of thousands of contributors, and is maintained by a small team within HashiCorp. This site contains extensive instructions about how to contribute and how the AWS provider works. Please Note: This documentation is intended for Terraform AWS Provider code developers. Typical operators writing and applying Terraform configurations do not need to read or understand this material.","title":"Welcome"},{"location":"#contribute","text":"Please follow the following steps to ensure your contribution goes smoothly.","title":"Contribute"},{"location":"#1-configure-development-environment","text":"Install Terraform and Go. Clone the repository, compile the provider, and set up testing. Refer to Configure Development Environment .","title":"1. Configure Development Environment"},{"location":"#2-write-code","text":"Follow the guide for your contribution type and refer to the Development Reference materials as needed for additional details about provider design , expected naming conventions , guidance for error handling , etc. Contribution Guide Description Resources Allow the management of a logical resource within AWS by adding a new resource to the Terraform AWS Provider. Data Source Let your Terraform configurations use data from resources not under local management by creating ready only data sources. Services Allow Terraform (via the AWS Provider) to manage an entirely new AWS service by introducing the resources and data sources required to manage configuration of the service. AWS Region New regions are immediately usable with the provider with the caveat that a configuration workaround is required to skip validation of the region during cli operations. A small set of changes are required to makes this workaround necessary. Bug Fix or Enhancement These constitute the majority of pull requests submitted, many of which we address and merge regardless of priority in our regular internal gardening days. Resource Name Generation Allow a resource to either fully, or partially, generate its own resource names. This can be useful in cases where the resource name uniquely identifes the resource and it needs to be recreated. It can also be used when a name is required, but the specific name is not important. Tagging Support Many AWS resources allow assigning metadata via tags. However, frequently AWS services are launched without tagging support so this will often need to be added later. Import Support Adding import support allows terraform import to be run targeting an existing unmanaged resource and pulling its configuration into Terraform state. Typically import support is added during initial resource implementation but in some cases this will need to be added later. Documentation Changes The provider documentation is displayed on the Terraform Registry and is sourced and refreshed from the provider repository during the release process.","title":"2. Write Code"},{"location":"#3-write-tests","text":"We require changes to be covered by acceptance tests for all contributions. If you are unable to pay for acceptance tests for your contributions, mention this in your pull request. We will happily accept \"best effort\" acceptance tests implementations and run them for you on our side. Your PR may take longer to merge, but this is not a blocker for contributions.","title":"3. Write Tests"},{"location":"#4-update-the-changelog","text":"HashiCorp's open-source projects have always maintained a user-friendly, readable CHANGELOG.md that allows users to tell at a glance whether a release should have any effect on them, and to gauge the risk of an upgrade. Not all changes require an entry in the changelog, refer to our Changelog Process for details about when and how to create a changelog.","title":"4. Update the Changelog"},{"location":"#5-create-a-pull-request","text":"When your contribution is ready, Create a Pull Request in the AWS provider repository. Pull requests are usually triaged within a few days of creation and are prioritized based on community reactions. Our Prioritization Guides provides more details about the process.","title":"5. Create a Pull Request"},{"location":"#submit-an-issue","text":"In addition to contributions, we welcome bug reports and feature requests .","title":"Submit an Issue"},{"location":"acc-test-environment-variables/","text":"Acceptance Testing Environment Variable Dictionary # Environment variables (beyond standard AWS Go SDK ones) used by acceptance testing. See also the internal/acctest package. Variable Description ACM_CERTIFICATE_ROOT_DOMAIN Root domain name to use with ACM Certificate testing. ACM_CERTIFICATE_MULTIPLE_ISSUED_DOMAIN Domain name of ACM Certificate with a multiple issued certificates. DEPRECATED: Should be replaced with aws_acm_certficate resource usage in tests. ACM_CERTIFICATE_MULTIPLE_ISSUED_MOST_RECENT_ARN Amazon Resource Name of most recent ACM Certificate with a multiple issued certificates. DEPRECATED: Should be replaced with aws_acm_certficate resource usage in tests. ACM_CERTIFICATE_SINGLE_ISSUED_DOMAIN Domain name of ACM Certificate with a single issued certificate. DEPRECATED: Should be replaced with aws_acm_certficate resource usage in tests. ACM_CERTIFICATE_SINGLE_ISSUED_MOST_RECENT_ARN Amazon Resource Name of most recent ACM Certificate with a single issued certificate. DEPRECATED: Should be replaced with aws_acm_certficate resource usage in tests. ADM_CLIENT_ID Identifier for Amazon Device Manager Client in Pinpoint testing. AMPLIFY_DOMAIN_NAME Domain name to use for Amplify domain association testing. AMPLIFY_GITHUB_ACCESS_TOKEN GitHub access token used for AWS Amplify testing. AMPLIFY_GITHUB_REPOSITORY GitHub repository used for AWS Amplify testing. ADM_CLIENT_SECRET Secret for Amazon Device Manager Client in Pinpoint testing. APNS_BUNDLE_ID Identifier for Apple Push Notification Service Bundle in Pinpoint testing. APNS_CERTIFICATE Certificate (PEM format) for Apple Push Notification Service in Pinpoint testing. APNS_CERTIFICATE_PRIVATE_KEY Private key for Apple Push Notification Service in Pinpoint testing. APNS_SANDBOX_BUNDLE_ID Identifier for Sandbox Apple Push Notification Service Bundle in Pinpoint testing. APNS_SANDBOX_CERTIFICATE Certificate (PEM format) for Sandbox Apple Push Notification Service in Pinpoint testing. APNS_SANDBOX_CERTIFICATE_PRIVATE_KEY Private key for Sandbox Apple Push Notification Service in Pinpoint testing. APNS_SANDBOX_CREDENTIAL Credential contents for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with APNS_SANDBOX_CREDENTIAL_PATH . APNS_SANDBOX_CREDENTIAL_PATH Path to credential for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with APNS_SANDBOX_CREDENTIAL . APNS_SANDBOX_PRINCIPAL Principal contents for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with APNS_SANDBOX_PRINCIPAL_PATH . APNS_SANDBOX_PRINCIPAL_PATH Path to principal for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with APNS_SANDBOX_PRINCIPAL . APNS_SANDBOX_TEAM_ID Identifier for Sandbox Apple Push Notification Service Team in Pinpoint testing. APNS_SANDBOX_TOKEN_KEY Token key file content (.p8 format) for Sandbox Apple Push Notification Service in Pinpoint testing. APNS_SANDBOX_TOKEN_KEY_ID Identifier for Sandbox Apple Push Notification Service Token Key in Pinpoint testing. APNS_TEAM_ID Identifier for Apple Push Notification Service Team in Pinpoint testing. APNS_TOKEN_KEY Token key file content (.p8 format) for Apple Push Notification Service in Pinpoint testing. APNS_TOKEN_KEY_ID Identifier for Apple Push Notification Service Token Key in Pinpoint testing. APNS_VOIP_BUNDLE_ID Identifier for VOIP Apple Push Notification Service Bundle in Pinpoint testing. APNS_VOIP_CERTIFICATE Certificate (PEM format) for VOIP Apple Push Notification Service in Pinpoint testing. APNS_VOIP_CERTIFICATE_PRIVATE_KEY Private key for VOIP Apple Push Notification Service in Pinpoint testing. APNS_VOIP_TEAM_ID Identifier for VOIP Apple Push Notification Service Team in Pinpoint testing. APNS_VOIP_TOKEN_KEY Token key file content (.p8 format) for VOIP Apple Push Notification Service in Pinpoint testing. APNS_VOIP_TOKEN_KEY_ID Identifier for VOIP Apple Push Notification Service Token Key in Pinpoint testing. APPRUNNER_CUSTOM_DOMAIN A custom domain endpoint (root domain, subdomain, or wildcard) for AppRunner Custom Domain Association testing. AWS_ALTERNATE_ACCESS_KEY_ID AWS access key ID with access to a secondary AWS account for tests requiring multiple accounts. Requires AWS_ALTERNATE_SECRET_ACCESS_KEY . Conflicts with AWS_ALTERNATE_PROFILE . AWS_ALTERNATE_SECRET_ACCESS_KEY AWS secret access key with access to a secondary AWS account for tests requiring multiple accounts. Requires AWS_ALTERNATE_ACCESS_KEY_ID . Conflicts with AWS_ALTERNATE_PROFILE . AWS_ALTERNATE_PROFILE AWS profile with access to a secondary AWS account for tests requiring multiple accounts. Conflicts with AWS_ALTERNATE_ACCESS_KEY_ID and AWS_ALTERNATE_SECRET_ACCESS_KEY . AWS_ALTERNATE_REGION Secondary AWS region for tests requiring multiple regions. Defaults to us-east-1 . AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_BODY Certificate body of publicly trusted certificate for API Gateway Domain Name testing. AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_CHAIN Certificate chain of publicly trusted certificate for API Gateway Domain Name testing. AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_PRIVATE_KEY Private key of publicly trusted certificate for API Gateway Domain Name testing. AWS_API_GATEWAY_DOMAIN_NAME_REGIONAL_CERTIFICATE_NAME_ENABLED Flag to enable API Gateway Domain Name regional certificate upload testing. AWS_CODEBUILD_BITBUCKET_SOURCE_LOCATION BitBucket source URL for CodeBuild testing. CodeBuild must have access to this repository via OAuth or Source Credentials. Defaults to https://terraform@bitbucket.org/terraform/aws-test.git . AWS_CODEBUILD_GITHUB_SOURCE_LOCATION GitHub source URL for CodeBuild testing. CodeBuild must have access to this repository via OAuth or Source Credentials. Defaults to https://github.com/hashibot-test/aws-test.git . AWS_DEFAULT_REGION Primary AWS region for tests. Defaults to us-west-2 . AWS_DETECTIVE_MEMBER_EMAIL Email address for Detective Member testing. A valid email address associated with an AWS root account is required for tests to pass. AWS_EC2_CLASSIC_REGION AWS region for EC2-Classic testing. Defaults to us-east-1 in AWS Commercial and AWS_DEFAULT_REGION otherwise. AWS_EC2_CLIENT_VPN_LIMIT Concurrency limit for Client VPN acceptance tests. Default is 5 if not specified. AWS_EC2_EIP_PUBLIC_IPV4_POOL Identifier for EC2 Public IPv4 Pool for EC2 EIP testing. AWS_GUARDDUTY_MEMBER_ACCOUNT_ID Identifier of AWS Account for GuardDuty Member testing. DEPRECATED: Should be replaced with standard alternate account handling for tests. AWS_GUARDDUTY_MEMBER_EMAIL Email address for GuardDuty Member testing. DEPRECATED: It may be possible to use a placeholder email address instead. AWS_LAMBDA_IMAGE_LATEST_ID ECR repository image URI (tagged as latest ) for Lambda container image acceptance tests. AWS_LAMBDA_IMAGE_V1_ID ECR repository image URI (tagged as v1 ) for Lambda container image acceptance tests. AWS_LAMBDA_IMAGE_V2_ID ECR repository image URI (tagged as v2 ) for Lambda container image acceptance tests. DX_CONNECTION_ID Identifier for Direct Connect Connection testing. DX_VIRTUAL_INTERFACE_ID Identifier for Direct Connect Virtual Interface testing. EC2_SECURITY_GROUP_RULES_PER_GROUP_LIMIT EC2 Quota for Rules per Security Group. Defaults to 50. DEPRECATED: Can be augmented or replaced with Service Quotas lookup. EVENT_BRIDGE_PARTNER_EVENT_BUS_NAME Amazon EventBridge partner event bus name. EVENT_BRIDGE_PARTNER_EVENT_SOURCE_NAME Amazon EventBridge partner event source name. GCM_API_KEY API Key for Google Cloud Messaging in Pinpoint and SNS Platform Application testing. GITHUB_TOKEN GitHub token for CodePipeline testing. GRAFANA_SSO_GROUP_ID AWS SSO group ID for Grafana testing. GRAFANA_SSO_USER_ID AWS SSO user ID for Grafana testing. MACIE_MEMBER_ACCOUNT_ID Identifier of AWS Account for Macie Member testing. DEPRECATED: Should be replaced with standard alternate account handling for tests. QUICKSIGHT_NAMESPACE QuickSight namespace name for testing. ROUTE53DOMAINS_DOMAIN_NAME Registered domain for Route 53 Domains testing. SAGEMAKER_IMAGE_VERSION_BASE_IMAGE SageMaker base image to use for tests. SERVICEQUOTAS_INCREASE_ON_CREATE_QUOTA_CODE Quota Code for Service Quotas testing (submits support case). SERVICEQUOTAS_INCREASE_ON_CREATE_SERVICE_CODE Service Code for Service Quotas testing (submits support case). SERVICEQUOTAS_INCREASE_ON_CREATE_VALUE Value of quota increase for Service Quotas testing (submits support case). SES_DOMAIN_IDENTITY_ROOT_DOMAIN Root domain name of publicly accessible and Route 53 configurable domain for SES Domain Identity testing. SWF_DOMAIN_TESTING_ENABLED Enables SWF Domain testing (API does not support deletions). TEST_AWS_ORGANIZATION_ACCOUNT_EMAIL_DOMAIN Email address for Organizations Account testing. TEST_AWS_SES_VERIFIED_EMAIL_ARN Verified SES Email Identity for use in Cognito User Pool testing. TF_ACC Enables Go tests containing resource.Test() and resource.ParallelTest() . TF_ACC_ASSUME_ROLE_ARN Amazon Resource Name of existing IAM Role to use for limited permissions acceptance testing. TF_TEST_CLOUDFRONT_RETAIN Flag to disable but dangle CloudFront Distributions during testing to reduce feedback time (must be manually destroyed afterwards)","title":"Acceptance Test Environment Variables"},{"location":"acc-test-environment-variables/#acceptance-testing-environment-variable-dictionary","text":"Environment variables (beyond standard AWS Go SDK ones) used by acceptance testing. See also the internal/acctest package. Variable Description ACM_CERTIFICATE_ROOT_DOMAIN Root domain name to use with ACM Certificate testing. ACM_CERTIFICATE_MULTIPLE_ISSUED_DOMAIN Domain name of ACM Certificate with a multiple issued certificates. DEPRECATED: Should be replaced with aws_acm_certficate resource usage in tests. ACM_CERTIFICATE_MULTIPLE_ISSUED_MOST_RECENT_ARN Amazon Resource Name of most recent ACM Certificate with a multiple issued certificates. DEPRECATED: Should be replaced with aws_acm_certficate resource usage in tests. ACM_CERTIFICATE_SINGLE_ISSUED_DOMAIN Domain name of ACM Certificate with a single issued certificate. DEPRECATED: Should be replaced with aws_acm_certficate resource usage in tests. ACM_CERTIFICATE_SINGLE_ISSUED_MOST_RECENT_ARN Amazon Resource Name of most recent ACM Certificate with a single issued certificate. DEPRECATED: Should be replaced with aws_acm_certficate resource usage in tests. ADM_CLIENT_ID Identifier for Amazon Device Manager Client in Pinpoint testing. AMPLIFY_DOMAIN_NAME Domain name to use for Amplify domain association testing. AMPLIFY_GITHUB_ACCESS_TOKEN GitHub access token used for AWS Amplify testing. AMPLIFY_GITHUB_REPOSITORY GitHub repository used for AWS Amplify testing. ADM_CLIENT_SECRET Secret for Amazon Device Manager Client in Pinpoint testing. APNS_BUNDLE_ID Identifier for Apple Push Notification Service Bundle in Pinpoint testing. APNS_CERTIFICATE Certificate (PEM format) for Apple Push Notification Service in Pinpoint testing. APNS_CERTIFICATE_PRIVATE_KEY Private key for Apple Push Notification Service in Pinpoint testing. APNS_SANDBOX_BUNDLE_ID Identifier for Sandbox Apple Push Notification Service Bundle in Pinpoint testing. APNS_SANDBOX_CERTIFICATE Certificate (PEM format) for Sandbox Apple Push Notification Service in Pinpoint testing. APNS_SANDBOX_CERTIFICATE_PRIVATE_KEY Private key for Sandbox Apple Push Notification Service in Pinpoint testing. APNS_SANDBOX_CREDENTIAL Credential contents for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with APNS_SANDBOX_CREDENTIAL_PATH . APNS_SANDBOX_CREDENTIAL_PATH Path to credential for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with APNS_SANDBOX_CREDENTIAL . APNS_SANDBOX_PRINCIPAL Principal contents for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with APNS_SANDBOX_PRINCIPAL_PATH . APNS_SANDBOX_PRINCIPAL_PATH Path to principal for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with APNS_SANDBOX_PRINCIPAL . APNS_SANDBOX_TEAM_ID Identifier for Sandbox Apple Push Notification Service Team in Pinpoint testing. APNS_SANDBOX_TOKEN_KEY Token key file content (.p8 format) for Sandbox Apple Push Notification Service in Pinpoint testing. APNS_SANDBOX_TOKEN_KEY_ID Identifier for Sandbox Apple Push Notification Service Token Key in Pinpoint testing. APNS_TEAM_ID Identifier for Apple Push Notification Service Team in Pinpoint testing. APNS_TOKEN_KEY Token key file content (.p8 format) for Apple Push Notification Service in Pinpoint testing. APNS_TOKEN_KEY_ID Identifier for Apple Push Notification Service Token Key in Pinpoint testing. APNS_VOIP_BUNDLE_ID Identifier for VOIP Apple Push Notification Service Bundle in Pinpoint testing. APNS_VOIP_CERTIFICATE Certificate (PEM format) for VOIP Apple Push Notification Service in Pinpoint testing. APNS_VOIP_CERTIFICATE_PRIVATE_KEY Private key for VOIP Apple Push Notification Service in Pinpoint testing. APNS_VOIP_TEAM_ID Identifier for VOIP Apple Push Notification Service Team in Pinpoint testing. APNS_VOIP_TOKEN_KEY Token key file content (.p8 format) for VOIP Apple Push Notification Service in Pinpoint testing. APNS_VOIP_TOKEN_KEY_ID Identifier for VOIP Apple Push Notification Service Token Key in Pinpoint testing. APPRUNNER_CUSTOM_DOMAIN A custom domain endpoint (root domain, subdomain, or wildcard) for AppRunner Custom Domain Association testing. AWS_ALTERNATE_ACCESS_KEY_ID AWS access key ID with access to a secondary AWS account for tests requiring multiple accounts. Requires AWS_ALTERNATE_SECRET_ACCESS_KEY . Conflicts with AWS_ALTERNATE_PROFILE . AWS_ALTERNATE_SECRET_ACCESS_KEY AWS secret access key with access to a secondary AWS account for tests requiring multiple accounts. Requires AWS_ALTERNATE_ACCESS_KEY_ID . Conflicts with AWS_ALTERNATE_PROFILE . AWS_ALTERNATE_PROFILE AWS profile with access to a secondary AWS account for tests requiring multiple accounts. Conflicts with AWS_ALTERNATE_ACCESS_KEY_ID and AWS_ALTERNATE_SECRET_ACCESS_KEY . AWS_ALTERNATE_REGION Secondary AWS region for tests requiring multiple regions. Defaults to us-east-1 . AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_BODY Certificate body of publicly trusted certificate for API Gateway Domain Name testing. AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_CHAIN Certificate chain of publicly trusted certificate for API Gateway Domain Name testing. AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_PRIVATE_KEY Private key of publicly trusted certificate for API Gateway Domain Name testing. AWS_API_GATEWAY_DOMAIN_NAME_REGIONAL_CERTIFICATE_NAME_ENABLED Flag to enable API Gateway Domain Name regional certificate upload testing. AWS_CODEBUILD_BITBUCKET_SOURCE_LOCATION BitBucket source URL for CodeBuild testing. CodeBuild must have access to this repository via OAuth or Source Credentials. Defaults to https://terraform@bitbucket.org/terraform/aws-test.git . AWS_CODEBUILD_GITHUB_SOURCE_LOCATION GitHub source URL for CodeBuild testing. CodeBuild must have access to this repository via OAuth or Source Credentials. Defaults to https://github.com/hashibot-test/aws-test.git . AWS_DEFAULT_REGION Primary AWS region for tests. Defaults to us-west-2 . AWS_DETECTIVE_MEMBER_EMAIL Email address for Detective Member testing. A valid email address associated with an AWS root account is required for tests to pass. AWS_EC2_CLASSIC_REGION AWS region for EC2-Classic testing. Defaults to us-east-1 in AWS Commercial and AWS_DEFAULT_REGION otherwise. AWS_EC2_CLIENT_VPN_LIMIT Concurrency limit for Client VPN acceptance tests. Default is 5 if not specified. AWS_EC2_EIP_PUBLIC_IPV4_POOL Identifier for EC2 Public IPv4 Pool for EC2 EIP testing. AWS_GUARDDUTY_MEMBER_ACCOUNT_ID Identifier of AWS Account for GuardDuty Member testing. DEPRECATED: Should be replaced with standard alternate account handling for tests. AWS_GUARDDUTY_MEMBER_EMAIL Email address for GuardDuty Member testing. DEPRECATED: It may be possible to use a placeholder email address instead. AWS_LAMBDA_IMAGE_LATEST_ID ECR repository image URI (tagged as latest ) for Lambda container image acceptance tests. AWS_LAMBDA_IMAGE_V1_ID ECR repository image URI (tagged as v1 ) for Lambda container image acceptance tests. AWS_LAMBDA_IMAGE_V2_ID ECR repository image URI (tagged as v2 ) for Lambda container image acceptance tests. DX_CONNECTION_ID Identifier for Direct Connect Connection testing. DX_VIRTUAL_INTERFACE_ID Identifier for Direct Connect Virtual Interface testing. EC2_SECURITY_GROUP_RULES_PER_GROUP_LIMIT EC2 Quota for Rules per Security Group. Defaults to 50. DEPRECATED: Can be augmented or replaced with Service Quotas lookup. EVENT_BRIDGE_PARTNER_EVENT_BUS_NAME Amazon EventBridge partner event bus name. EVENT_BRIDGE_PARTNER_EVENT_SOURCE_NAME Amazon EventBridge partner event source name. GCM_API_KEY API Key for Google Cloud Messaging in Pinpoint and SNS Platform Application testing. GITHUB_TOKEN GitHub token for CodePipeline testing. GRAFANA_SSO_GROUP_ID AWS SSO group ID for Grafana testing. GRAFANA_SSO_USER_ID AWS SSO user ID for Grafana testing. MACIE_MEMBER_ACCOUNT_ID Identifier of AWS Account for Macie Member testing. DEPRECATED: Should be replaced with standard alternate account handling for tests. QUICKSIGHT_NAMESPACE QuickSight namespace name for testing. ROUTE53DOMAINS_DOMAIN_NAME Registered domain for Route 53 Domains testing. SAGEMAKER_IMAGE_VERSION_BASE_IMAGE SageMaker base image to use for tests. SERVICEQUOTAS_INCREASE_ON_CREATE_QUOTA_CODE Quota Code for Service Quotas testing (submits support case). SERVICEQUOTAS_INCREASE_ON_CREATE_SERVICE_CODE Service Code for Service Quotas testing (submits support case). SERVICEQUOTAS_INCREASE_ON_CREATE_VALUE Value of quota increase for Service Quotas testing (submits support case). SES_DOMAIN_IDENTITY_ROOT_DOMAIN Root domain name of publicly accessible and Route 53 configurable domain for SES Domain Identity testing. SWF_DOMAIN_TESTING_ENABLED Enables SWF Domain testing (API does not support deletions). TEST_AWS_ORGANIZATION_ACCOUNT_EMAIL_DOMAIN Email address for Organizations Account testing. TEST_AWS_SES_VERIFIED_EMAIL_ARN Verified SES Email Identity for use in Cognito User Pool testing. TF_ACC Enables Go tests containing resource.Test() and resource.ParallelTest() . TF_ACC_ASSUME_ROLE_ARN Amazon Resource Name of existing IAM Role to use for limited permissions acceptance testing. TF_TEST_CLOUDFRONT_RETAIN Flag to disable but dangle CloudFront Distributions during testing to reduce feedback time (must be manually destroyed afterwards)","title":"Acceptance Testing Environment Variable Dictionary"},{"location":"add-a-new-datasource/","text":"Adding a New Data Source # New data sources are required when AWS adds a new service, or adds new features within an existing service which would require a new data source to allow practitioners to query existing resources of that type for use in their configurations. Anything with a Describe or Get endpoint could make a data source, but some are more useful than others. Each data source should be submitted for review in isolation, pull requests containing multiple data sources and/or resources are harder to review and the maintainers will normally ask for them to be broken apart. Prerequisites # If this is the first addition of a data source for a new service, please ensure the Service Client for the new service has been added and merged. See Adding a new Service for details. Determine which version of the AWS SDK for Go the resource will be built upon. For more information and instructions on how to determine this choice, please read AWS SDK for Go Versions Steps to Add a Data Source # Fork the Provider and Create a Feature Branch # For a new data source use a branch named f-{datasource name} for example: f-ec2-vpc . See Raising a Pull Request for more details. Create and Name the Data Source # See the Naming Guide for details on how to name the new resource and the resource file. Not following the naming standards will cause extra delay as maintainers request that you make changes. Use the skaff provider scaffolding tool to generate new resource and test templates using your chosen name ensuring you provide the v1 flag if you are targeting version 1 of the aws-go-sdk . Doing so will ensure that any boilerplate code, structural best practices and repetitive naming is done for you and always represents our most current standards. Fill out the Data Source Schema # In the internal/service/<service>/<service>_data_source.go file you will see a Schema property which exists as a map of Schema objects. This relates the AWS API data model with the Terraform resource itself. For each property you want to make available in Terraform, you will need to add it as an attribute, and choose the correct data type. Attribute names are to specified in snake_case as opposed to the AWS API which is CamelCase Implement Read Handler # These will map the AWS API response to the data source schema. You will also need to handle different response types (including errors correctly). For complex attributes you will need to implement Flattener or Expander functions. The Data Handling and Conversion Guide covers everything you need to know for mapping AWS API responses to Terraform State and vice-versa. The Error Handling Guide covers everything you need to know about handling AWS API responses consistently. Write Passing Acceptance Tests # In order to adequately test the data source we will need to write a complete set of Acceptance Tests. You will need an AWS account for this which allows the provider to read to state of the associated resource. See Writing Acceptance Tests for a detailed guide on how to approach these. You will need at minimum: Basic Test - Tests full lifecycle (CRUD + Import) of a minimal configuration (all required fields, no optional). Disappears Test - Tests what Terraform does if a resource it is tracking can no longer be found. Per Attribute Tests - For each attribute a test should exists which tests that particular attribute in isolation alongside any required fields. Create Documentation for the Data Source # Add a file covering the use of the new data source in website/docs/d/<service>_<name>.md . You may want to also add examples of the data source in use particularly if its use is complex, or relies on resources in another service. This documentation will appear on the Terraform Registry when the data source is made available in a provider release. It is fine to link out to AWS Documentation where appropriate, particularly for values which are likely to change. Ensure Format and Lint Checks are Passing Locally # Run go fmt to format your code, and install and run all linters to detect and resolve any structural issues with the implementation or documentation. make fmt make tools # install linters and dependencies make lint # run provider linters make docs-lint # run documentation linters make website-lint # run website documentation linters Raise a Pull Request # See Raising a Pull Request . Wait for Prioritization # In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our prioritization guide for full details of the process.","title":"Data source"},{"location":"add-a-new-datasource/#adding-a-new-data-source","text":"New data sources are required when AWS adds a new service, or adds new features within an existing service which would require a new data source to allow practitioners to query existing resources of that type for use in their configurations. Anything with a Describe or Get endpoint could make a data source, but some are more useful than others. Each data source should be submitted for review in isolation, pull requests containing multiple data sources and/or resources are harder to review and the maintainers will normally ask for them to be broken apart.","title":"Adding a New Data Source"},{"location":"add-a-new-datasource/#prerequisites","text":"If this is the first addition of a data source for a new service, please ensure the Service Client for the new service has been added and merged. See Adding a new Service for details. Determine which version of the AWS SDK for Go the resource will be built upon. For more information and instructions on how to determine this choice, please read AWS SDK for Go Versions","title":"Prerequisites"},{"location":"add-a-new-datasource/#steps-to-add-a-data-source","text":"","title":"Steps to Add a Data Source"},{"location":"add-a-new-datasource/#fork-the-provider-and-create-a-feature-branch","text":"For a new data source use a branch named f-{datasource name} for example: f-ec2-vpc . See Raising a Pull Request for more details.","title":"Fork the Provider and Create a Feature Branch"},{"location":"add-a-new-datasource/#create-and-name-the-data-source","text":"See the Naming Guide for details on how to name the new resource and the resource file. Not following the naming standards will cause extra delay as maintainers request that you make changes. Use the skaff provider scaffolding tool to generate new resource and test templates using your chosen name ensuring you provide the v1 flag if you are targeting version 1 of the aws-go-sdk . Doing so will ensure that any boilerplate code, structural best practices and repetitive naming is done for you and always represents our most current standards.","title":"Create and Name the Data Source"},{"location":"add-a-new-datasource/#fill-out-the-data-source-schema","text":"In the internal/service/<service>/<service>_data_source.go file you will see a Schema property which exists as a map of Schema objects. This relates the AWS API data model with the Terraform resource itself. For each property you want to make available in Terraform, you will need to add it as an attribute, and choose the correct data type. Attribute names are to specified in snake_case as opposed to the AWS API which is CamelCase","title":"Fill out the Data Source Schema"},{"location":"add-a-new-datasource/#implement-read-handler","text":"These will map the AWS API response to the data source schema. You will also need to handle different response types (including errors correctly). For complex attributes you will need to implement Flattener or Expander functions. The Data Handling and Conversion Guide covers everything you need to know for mapping AWS API responses to Terraform State and vice-versa. The Error Handling Guide covers everything you need to know about handling AWS API responses consistently.","title":"Implement Read Handler"},{"location":"add-a-new-datasource/#write-passing-acceptance-tests","text":"In order to adequately test the data source we will need to write a complete set of Acceptance Tests. You will need an AWS account for this which allows the provider to read to state of the associated resource. See Writing Acceptance Tests for a detailed guide on how to approach these. You will need at minimum: Basic Test - Tests full lifecycle (CRUD + Import) of a minimal configuration (all required fields, no optional). Disappears Test - Tests what Terraform does if a resource it is tracking can no longer be found. Per Attribute Tests - For each attribute a test should exists which tests that particular attribute in isolation alongside any required fields.","title":"Write Passing Acceptance Tests"},{"location":"add-a-new-datasource/#create-documentation-for-the-data-source","text":"Add a file covering the use of the new data source in website/docs/d/<service>_<name>.md . You may want to also add examples of the data source in use particularly if its use is complex, or relies on resources in another service. This documentation will appear on the Terraform Registry when the data source is made available in a provider release. It is fine to link out to AWS Documentation where appropriate, particularly for values which are likely to change.","title":"Create Documentation for the Data Source"},{"location":"add-a-new-datasource/#ensure-format-and-lint-checks-are-passing-locally","text":"Run go fmt to format your code, and install and run all linters to detect and resolve any structural issues with the implementation or documentation. make fmt make tools # install linters and dependencies make lint # run provider linters make docs-lint # run documentation linters make website-lint # run website documentation linters","title":"Ensure Format and Lint Checks are Passing Locally"},{"location":"add-a-new-datasource/#raise-a-pull-request","text":"See Raising a Pull Request .","title":"Raise a Pull Request"},{"location":"add-a-new-datasource/#wait-for-prioritization","text":"In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our prioritization guide for full details of the process.","title":"Wait for Prioritization"},{"location":"add-a-new-region/","text":"Adding a Newly Released AWS Region # New regions can typically be used immediately with the provider, with two important caveats: Regions often need to be explicitly enabled via the AWS console. See ap-east-1 launch blog for an example of how to enable a new region for use. Until the provider is aware of the new region, automatic region validation will fail. In order to use the region before validation support is added to the provider you will need to disable region validation by doing the following: provider \"aws\" { # ... potentially other configuration ... region = \"me-south-1\" skip_region_validation = true } Enabling Region Validation # Support for region validation requires that the provider has an updated AWS Go SDK dependency that includes the new region. These are added to the AWS Go SDK aws/endpoints/defaults.go file and generally noted in the AWS Go SDK CHANGELOG as aws/endpoints: Updated Regions . This also needs to be done in the core Terraform binary itself to enable it for the S3 backend. The provider currently takes a dependency on both v1 AND v2 of the AWS Go SDK, as we start to base new (and migrate) resources on v2. Many of the authentication and provider level configuration interactions are also located in the aws-go-sdk-base library. As all of these things take direct dependencies and as a result there ends up being quite a few places these dependency updates need to be made. Update aws-go-sdk-base # aws-go-sdk-base Update aws-go-sdk Update aws-go-sdk-v2 Update Terraform AWS Provider # provider Update aws-go-sdk Update aws-go-sdk-v2 Update aws-go-sdk-base Update Terraform Core (S3 Backend) # core Update aws-go-sdk Update aws-go-sdk-v2 Update aws-go-sdk-base go get github.com/aws/aws-sdk-go@v#.#.# go mod tidy See the Changelog Process document for example changelog format. Update Region Specific values in static Data Sources # Some data sources include static values specific to regions that are not available via a standard AWS API call. These will need to be manually updated. AWS employees can code search previous region values to find new region values in internal packages like RIPStaticConfig if they are not documented yet. Check Elastic Load Balancing endpoints and quotas and add Route53 Hosted Zone ID if available to internal/service/elb/hosted_zone_id_data_source.go and internal/service/elbv2/hosted_zone_id_data_source.go Check Amazon Simple Storage Service endpoints and quotas and add Route53 Hosted Zone ID if available to internal/service/s3/hosted_zones.go Check CloudTrail Supported Regions docs and add AWS Account ID if available to internal/service/cloudtrail/service_account_data_source.go Check Elastic Load Balancing Access Logs docs and add Elastic Load Balancing Account ID if available to internal/service/elb/service_account_data_source.go Check Redshift Database Audit Logging docs and add AWS Account ID if available to internal/service/redshift/service_account_data_source.go Check AWS Elastic Beanstalk endpoints and quotas and add Route53 Hosted Zone ID if available to internal/service/elasticbeanstalk/hosted_zone_data_source.go Check SageMaker docs and add AWS Account IDs if available to internal/service/sagemaker/prebuilt_ecr_image_data_source.go","title":"AWS Region"},{"location":"add-a-new-region/#adding-a-newly-released-aws-region","text":"New regions can typically be used immediately with the provider, with two important caveats: Regions often need to be explicitly enabled via the AWS console. See ap-east-1 launch blog for an example of how to enable a new region for use. Until the provider is aware of the new region, automatic region validation will fail. In order to use the region before validation support is added to the provider you will need to disable region validation by doing the following: provider \"aws\" { # ... potentially other configuration ... region = \"me-south-1\" skip_region_validation = true }","title":"Adding a Newly Released AWS Region"},{"location":"add-a-new-region/#enabling-region-validation","text":"Support for region validation requires that the provider has an updated AWS Go SDK dependency that includes the new region. These are added to the AWS Go SDK aws/endpoints/defaults.go file and generally noted in the AWS Go SDK CHANGELOG as aws/endpoints: Updated Regions . This also needs to be done in the core Terraform binary itself to enable it for the S3 backend. The provider currently takes a dependency on both v1 AND v2 of the AWS Go SDK, as we start to base new (and migrate) resources on v2. Many of the authentication and provider level configuration interactions are also located in the aws-go-sdk-base library. As all of these things take direct dependencies and as a result there ends up being quite a few places these dependency updates need to be made.","title":"Enabling Region Validation"},{"location":"add-a-new-region/#update-aws-go-sdk-base","text":"aws-go-sdk-base Update aws-go-sdk Update aws-go-sdk-v2","title":"Update aws-go-sdk-base"},{"location":"add-a-new-region/#update-terraform-aws-provider","text":"provider Update aws-go-sdk Update aws-go-sdk-v2 Update aws-go-sdk-base","title":"Update Terraform AWS Provider"},{"location":"add-a-new-region/#update-terraform-core-s3-backend","text":"core Update aws-go-sdk Update aws-go-sdk-v2 Update aws-go-sdk-base go get github.com/aws/aws-sdk-go@v#.#.# go mod tidy See the Changelog Process document for example changelog format.","title":"Update Terraform Core (S3 Backend)"},{"location":"add-a-new-region/#update-region-specific-values-in-static-data-sources","text":"Some data sources include static values specific to regions that are not available via a standard AWS API call. These will need to be manually updated. AWS employees can code search previous region values to find new region values in internal packages like RIPStaticConfig if they are not documented yet. Check Elastic Load Balancing endpoints and quotas and add Route53 Hosted Zone ID if available to internal/service/elb/hosted_zone_id_data_source.go and internal/service/elbv2/hosted_zone_id_data_source.go Check Amazon Simple Storage Service endpoints and quotas and add Route53 Hosted Zone ID if available to internal/service/s3/hosted_zones.go Check CloudTrail Supported Regions docs and add AWS Account ID if available to internal/service/cloudtrail/service_account_data_source.go Check Elastic Load Balancing Access Logs docs and add Elastic Load Balancing Account ID if available to internal/service/elb/service_account_data_source.go Check Redshift Database Audit Logging docs and add AWS Account ID if available to internal/service/redshift/service_account_data_source.go Check AWS Elastic Beanstalk endpoints and quotas and add Route53 Hosted Zone ID if available to internal/service/elasticbeanstalk/hosted_zone_data_source.go Check SageMaker docs and add AWS Account IDs if available to internal/service/sagemaker/prebuilt_ecr_image_data_source.go","title":"Update Region Specific values in static Data Sources"},{"location":"add-a-new-resource/","text":"Adding a New Resource # New resources are required when AWS adds a new service, or adds new features within an existing service which would require a new resource to manage in Terraform. Typically anything with a new set of CRUD API endpoints is a great candidate for a new resource. Each resource should be submitted for review in isolation. Pull requests containing multiple resources are harder to review and the maintainers will normally ask for them to be broken apart. Prerequisites # If this is the first resource for a new service, please ensure the Service Client for the new service has been added and merged. See Adding a new Service for details. Determine which version of the AWS SDK for Go the resource will be built upon. For more information and instructions on how to determine this choice, please read AWS SDK for Go Versions Steps to Add a Resource # Fork the provider and create a feature branch # For a new resources use a branch named f-{resource name} for example: f-ec2-vpc . See Raising a Pull Request for more details. Create and Name the Resource # See the Naming Guide for details on how to name the new resource and the resource file. Not following the naming standards will cause extra delay as maintainers request that you make changes. Use the skaff provider scaffolding tool to generate new resource and test templates using your chosen name ensuring you provide the v1 flag if you are targeting version 1 of the aws-go-sdk . Doing so will ensure that any boilerplate code, structural best practices and repetitive naming is done for you and always represents our most current standards. Fill out the Resource Schema # In the internal/service/<service>/<service>.go file you will see a Schema property which exists as a map of Schema objects. This relates the AWS API data model with the Terraform resource itself. For each property you want to make available in Terraform, you will need to add it as an attribute, choose the correct data type and supply the correct Schema Behaviors in order to ensure Terraform knows how to correctly handle the value. Typically you will add arguments to represent the values that are under control by Terraform, and attributes in order to supply read-only values as references for Terraform. These are distinguished by Schema Behavior. Attribute names are to specified in camel_case as opposed to the AWS API which is CamelCase Implement CRUD handlers # These will map planned Terraform state to the AWS API call, or an AWS API response to an applied Terraform state. You will also need to handle different response types (including errors correctly). For complex attributes you will need to implement Flattener or Expander functions. The Data Handling and Conversion Guide covers everything you need to know for mapping AWS API responses to Terraform State and vice-versa. The Error Handling Guide covers everything you need to know about handling AWS API responses consistently. Write passing Acceptance Tests # In order to adequately test the resource we will need to write a complete set of Acceptance Tests. You will need an AWS account for this which allows the creation of that resource. See Writing Acceptance Tests for a detailed guide on how to approach these. You will need at minimum: Basic Test - Tests full lifecycle (CRUD + Import) of a minimal configuration (all required fields, no optional). Disappears Test - Tests what Terraform does if a resource it is tracking can no longer be found. Argument Tests - All arguments should be tested in a pragmatic way. Ensure that each argument can be initially set, updated, and cleared, as applicable. Depending on the logic and interaction of arguments, this may take one to several separate tests. Create documentation for the resource # Add a file covering the use of the new resource in website/docs/r/<service>_<name>.md . Add more examples if it is complex or relies on resources in another service. This documentation will appear on the Terraform Registry when the resource is made available in a provider release. Link to AWS Documentation where appropriate, particularly for values which are likely to change. Ensure format and lint checks are passing locally # Format your code and check linters to detect various issues. make fmt make tools # install linters and dependencies make lint # run provider linters make docs-lint # run documentation linters Raise a Pull Request # See Raising a Pull Request . Wait for Prioritization # In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our Prioritization Guide for full details of the process.","title":"Resource"},{"location":"add-a-new-resource/#adding-a-new-resource","text":"New resources are required when AWS adds a new service, or adds new features within an existing service which would require a new resource to manage in Terraform. Typically anything with a new set of CRUD API endpoints is a great candidate for a new resource. Each resource should be submitted for review in isolation. Pull requests containing multiple resources are harder to review and the maintainers will normally ask for them to be broken apart.","title":"Adding a New Resource"},{"location":"add-a-new-resource/#prerequisites","text":"If this is the first resource for a new service, please ensure the Service Client for the new service has been added and merged. See Adding a new Service for details. Determine which version of the AWS SDK for Go the resource will be built upon. For more information and instructions on how to determine this choice, please read AWS SDK for Go Versions","title":"Prerequisites"},{"location":"add-a-new-resource/#steps-to-add-a-resource","text":"","title":"Steps to Add a Resource"},{"location":"add-a-new-resource/#fork-the-provider-and-create-a-feature-branch","text":"For a new resources use a branch named f-{resource name} for example: f-ec2-vpc . See Raising a Pull Request for more details.","title":"Fork the provider and create a feature branch"},{"location":"add-a-new-resource/#create-and-name-the-resource","text":"See the Naming Guide for details on how to name the new resource and the resource file. Not following the naming standards will cause extra delay as maintainers request that you make changes. Use the skaff provider scaffolding tool to generate new resource and test templates using your chosen name ensuring you provide the v1 flag if you are targeting version 1 of the aws-go-sdk . Doing so will ensure that any boilerplate code, structural best practices and repetitive naming is done for you and always represents our most current standards.","title":"Create and Name the Resource"},{"location":"add-a-new-resource/#fill-out-the-resource-schema","text":"In the internal/service/<service>/<service>.go file you will see a Schema property which exists as a map of Schema objects. This relates the AWS API data model with the Terraform resource itself. For each property you want to make available in Terraform, you will need to add it as an attribute, choose the correct data type and supply the correct Schema Behaviors in order to ensure Terraform knows how to correctly handle the value. Typically you will add arguments to represent the values that are under control by Terraform, and attributes in order to supply read-only values as references for Terraform. These are distinguished by Schema Behavior. Attribute names are to specified in camel_case as opposed to the AWS API which is CamelCase","title":"Fill out the Resource Schema"},{"location":"add-a-new-resource/#implement-crud-handlers","text":"These will map planned Terraform state to the AWS API call, or an AWS API response to an applied Terraform state. You will also need to handle different response types (including errors correctly). For complex attributes you will need to implement Flattener or Expander functions. The Data Handling and Conversion Guide covers everything you need to know for mapping AWS API responses to Terraform State and vice-versa. The Error Handling Guide covers everything you need to know about handling AWS API responses consistently.","title":"Implement CRUD handlers"},{"location":"add-a-new-resource/#write-passing-acceptance-tests","text":"In order to adequately test the resource we will need to write a complete set of Acceptance Tests. You will need an AWS account for this which allows the creation of that resource. See Writing Acceptance Tests for a detailed guide on how to approach these. You will need at minimum: Basic Test - Tests full lifecycle (CRUD + Import) of a minimal configuration (all required fields, no optional). Disappears Test - Tests what Terraform does if a resource it is tracking can no longer be found. Argument Tests - All arguments should be tested in a pragmatic way. Ensure that each argument can be initially set, updated, and cleared, as applicable. Depending on the logic and interaction of arguments, this may take one to several separate tests.","title":"Write passing Acceptance Tests"},{"location":"add-a-new-resource/#create-documentation-for-the-resource","text":"Add a file covering the use of the new resource in website/docs/r/<service>_<name>.md . Add more examples if it is complex or relies on resources in another service. This documentation will appear on the Terraform Registry when the resource is made available in a provider release. Link to AWS Documentation where appropriate, particularly for values which are likely to change.","title":"Create documentation for the resource"},{"location":"add-a-new-resource/#ensure-format-and-lint-checks-are-passing-locally","text":"Format your code and check linters to detect various issues. make fmt make tools # install linters and dependencies make lint # run provider linters make docs-lint # run documentation linters","title":"Ensure format and lint checks are passing locally"},{"location":"add-a-new-resource/#raise-a-pull-request","text":"See Raising a Pull Request .","title":"Raise a Pull Request"},{"location":"add-a-new-resource/#wait-for-prioritization","text":"In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our Prioritization Guide for full details of the process.","title":"Wait for Prioritization"},{"location":"add-a-new-service/","text":"Adding a New AWS Service # AWS frequently launches new services, and Terraform support is frequently desired by the community shortly after launch. Depending on the API surface area of the new service, this could be a major undertaking. The following steps should be followed to prepare for adding the resources that allow for Terraform management of that service. Perform Service Design # Before adding a new service to the provider its a good idea to familiarize yourself with the primary workflows practitioners are likely to want to accomplish with the provider to ensure the provider design can solve for for this. Its not always necessary to cover 100% of the AWS service offering to unblock most workflows. You should have an idea of what resources and data sources should be added, their dependencies and relative importance in relation to the workflow. This should give you an idea of the order in which resources to be added. It's important to note that generally, we like to review and merge resources in isolation, and avoid combining multiple new resources in one Pull Request. Using the AWS API documentation as a reference, identify the various API's which correspond to the CRUD operations which consist of the management surface for that resource. These will be the set of API's called from the new resource. The API's model attributes will correspond to your resource schema. From there begin to map out the list of resources you would like to implement, and note your plan on the GitHub issue relating to the service (or create one if one does not exist) for the community and maintainers to feedback. Add a Service Client # Before new resources are submitted, please raise a separate pull request containing just the new AWS SDK for Go service client. To add an AWS SDK for Go service client: Check the file names/names_data.csv for the service. If it is already there, you are ready to implement the first resource or data source . Otherwise, determine the service identifier using the rule described in the Naming Guide . In names/names_data.csv , add a new line with all the requested information for the service following the guidance in the names README . Be very careful when adding or changing data in names_data.csv ! The Provider and generators depend on the file being correct. We strongly recommend using an editor with CSV support. Run the following then submit the pull request: make gen make test go mod tidy Once the service client has been added, implement the first resource or data source in a separate PR.","title":"Service"},{"location":"add-a-new-service/#adding-a-new-aws-service","text":"AWS frequently launches new services, and Terraform support is frequently desired by the community shortly after launch. Depending on the API surface area of the new service, this could be a major undertaking. The following steps should be followed to prepare for adding the resources that allow for Terraform management of that service.","title":"Adding a New AWS Service"},{"location":"add-a-new-service/#perform-service-design","text":"Before adding a new service to the provider its a good idea to familiarize yourself with the primary workflows practitioners are likely to want to accomplish with the provider to ensure the provider design can solve for for this. Its not always necessary to cover 100% of the AWS service offering to unblock most workflows. You should have an idea of what resources and data sources should be added, their dependencies and relative importance in relation to the workflow. This should give you an idea of the order in which resources to be added. It's important to note that generally, we like to review and merge resources in isolation, and avoid combining multiple new resources in one Pull Request. Using the AWS API documentation as a reference, identify the various API's which correspond to the CRUD operations which consist of the management surface for that resource. These will be the set of API's called from the new resource. The API's model attributes will correspond to your resource schema. From there begin to map out the list of resources you would like to implement, and note your plan on the GitHub issue relating to the service (or create one if one does not exist) for the community and maintainers to feedback.","title":"Perform Service Design"},{"location":"add-a-new-service/#add-a-service-client","text":"Before new resources are submitted, please raise a separate pull request containing just the new AWS SDK for Go service client. To add an AWS SDK for Go service client: Check the file names/names_data.csv for the service. If it is already there, you are ready to implement the first resource or data source . Otherwise, determine the service identifier using the rule described in the Naming Guide . In names/names_data.csv , add a new line with all the requested information for the service following the guidance in the names README . Be very careful when adding or changing data in names_data.csv ! The Provider and generators depend on the file being correct. We strongly recommend using an editor with CSV support. Run the following then submit the pull request: make gen make test go mod tidy Once the service client has been added, implement the first resource or data source in a separate PR.","title":"Add a Service Client"},{"location":"add-import-support/","text":"Adding Resource Import Support # Adding import support for Terraform resources will allow existing infrastructure to be managed within Terraform. This type of enhancement generally requires a small to moderate amount of code changes. Comprehensive code examples and information about resource import support can be found in the Terraform Plugin SDK v2 documentation . Resource Code Implementation : In the resource code (e.g., internal/service/{service}/{thing}.go ), implementation of Importer State function. When possible, prefer using schema.ImportStatePassthrough as the Importer State function Resource Acceptance Testing Implementation : In the resource acceptance testing (e.g., internal/service/{service}/{thing}_test.go ), implementation of TestStep s with ImportState: true Resource Documentation Implementation : In the resource documentation (e.g., website/docs/r/service_thing.html.markdown ), addition of Import documentation section at the bottom of the page","title":"Import Support"},{"location":"add-import-support/#adding-resource-import-support","text":"Adding import support for Terraform resources will allow existing infrastructure to be managed within Terraform. This type of enhancement generally requires a small to moderate amount of code changes. Comprehensive code examples and information about resource import support can be found in the Terraform Plugin SDK v2 documentation . Resource Code Implementation : In the resource code (e.g., internal/service/{service}/{thing}.go ), implementation of Importer State function. When possible, prefer using schema.ImportStatePassthrough as the Importer State function Resource Acceptance Testing Implementation : In the resource acceptance testing (e.g., internal/service/{service}/{thing}_test.go ), implementation of TestStep s with ImportState: true Resource Documentation Implementation : In the resource documentation (e.g., website/docs/r/service_thing.html.markdown ), addition of Import documentation section at the bottom of the page","title":"Adding Resource Import Support"},{"location":"adding-a-tag-resource/","text":"Adding a New Tag Resource # Adding a tag resource, similar to the aws_ecs_tag resource, has its own implementation procedure since the resource code and initial acceptance testing functions are automatically generated. The rest of the resource acceptance testing and resource documentation must still be manually created. In internal/generate : Ensure the service is supported by all generators. Run make gen after any modifications. In internal/service/{service}/generate.go : Add the new //go:generate call with the correct generator directives. Run make gen after any modifications. In internal/provider/provider.go : Add the new resource. Run make test and ensure there are no failures. Create internal/service/{service}/tag_gen_test.go with initial acceptance testing similar to the following (where the parent resource is simple to provision): import ( \"fmt\" \"testing\" \"github.com/aws/aws-sdk-go/service/{Service}\" \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/acctest\" \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/resource\" ) func TestAcc { Service } Tag_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_{service}_tag.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , { Service }. EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheck { Service } TagDestroy , Steps : [] resource . TestStep { { Config : testAcc { Service } TagConfig ( rName , \"key1\" , \"value1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheck { Service } TagExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"key\" , \"key1\" ), resource . TestCheckResourceAttr ( resourceName , \"value\" , \"value1\" ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func TestAcc { Service } Tag_disappears ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_{service}_tag.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , { Service }. EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheck { Service } TagDestroy , Steps : [] resource . TestStep { { Config : testAcc { Service } TagConfig ( rName , \"key1\" , \"value1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheck { Service } TagExists ( resourceName ), acctest . CheckResourceDisappears ( acctest . Provider , resourceAws { Service } Tag (), resourceName ), ), ExpectNonEmptyPlan : true , }, }, }) } func TestAcc { Service } Tag_Value ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_{service}_tag.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , { Service }. EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheck { Service } TagDestroy , Steps : [] resource . TestStep { { Config : testAcc { Service } TagConfig ( rName , \"key1\" , \"value1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheck { Service } TagExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"key\" , \"key1\" ), resource . TestCheckResourceAttr ( resourceName , \"value\" , \"value1\" ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, { Config : testAcc { Service } TagConfig ( rName , \"key1\" , \"value1updated\" ), Check : resource . ComposeTestCheckFunc ( testAccCheck { Service } TagExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"key\" , \"key1\" ), resource . TestCheckResourceAttr ( resourceName , \"value\" , \"value1updated\" ), ), }, }, }) } func testAcc { Service } TagConfig ( rName string , key string , value string ) string { return fmt . Sprintf ( ` resource \"aws_{service}_{thing}\" \"test\" { name = %[1]q lifecycle { ignore_changes = [tags] } } resource \"aws_{service}_tag\" \"test\" { resource_arn = aws_{service}_{thing}.test.arn key = %[2]q value = %[3]q } ` , rName , key , value ) } Run make testacc TESTS=TestAcc{Service}Tags_ PKG={Service} and ensure there are no failures. Create website/docs/r/{service}_tag.html.markdown with initial documentation similar to the following: --- subcategory: \"{SERVICE}\" layout: \"aws\" page_title: \"AWS: aws_{service}_tag\" description: |- Manages an individual {SERVICE} resource tag --- # Resource: aws_{service}_tag Manages an individual {SERVICE} resource tag. This resource should only be used in cases where {SERVICE} resources are created outside Terraform (e.g., {SERVICE} {THING}s implicitly created by {OTHER SERVICE THING}). ~> **NOTE:** This tagging resource should not be combined with the Terraform resource for managing the parent resource. For example, using `aws_{service}_{thing}` and `aws_{service}_tag` to manage tags of the same {SERVICE} {THING} will cause a perpetual difference where the `aws_{service}_{thing}` resource will try to remove the tag being added by the `aws_{service}_tag` resource. ~> **NOTE:** This tagging resource does not use the [ provider `ignore_tags` configuration ]( /docs/providers/aws/index.html#ignore_tags ). ## Example Usage ```terraform resource \"aws_{service}_tag\" \"example\" { resource_arn = \"...\" key = \"Name\" value = \"Hello World\" } ``` ## Argument Reference The following arguments are supported: * `resource_arn` - (Required) ARN of the {SERVICE} resource to tag. * `key` - (Required) Tag name. * `value` - (Required) Tag value. ## Attributes Reference In addition to all arguments above, the following attributes are exported: * `id` - {SERVICE} resource identifier and key, separated by a comma ( `,` ) ## Import `aws_{service}_tag` can be imported by using the {SERVICE} resource identifier and key, separated by a comma ( `,` ), e.g. ```console $ terraform import aws_ { service } _tag.example arn:aws: { service } :us-east-1:123456789012: { thing } /example,Name ```","title":"Tag Resource"},{"location":"adding-a-tag-resource/#adding-a-new-tag-resource","text":"Adding a tag resource, similar to the aws_ecs_tag resource, has its own implementation procedure since the resource code and initial acceptance testing functions are automatically generated. The rest of the resource acceptance testing and resource documentation must still be manually created. In internal/generate : Ensure the service is supported by all generators. Run make gen after any modifications. In internal/service/{service}/generate.go : Add the new //go:generate call with the correct generator directives. Run make gen after any modifications. In internal/provider/provider.go : Add the new resource. Run make test and ensure there are no failures. Create internal/service/{service}/tag_gen_test.go with initial acceptance testing similar to the following (where the parent resource is simple to provision): import ( \"fmt\" \"testing\" \"github.com/aws/aws-sdk-go/service/{Service}\" \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/acctest\" \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/resource\" ) func TestAcc { Service } Tag_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_{service}_tag.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , { Service }. EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheck { Service } TagDestroy , Steps : [] resource . TestStep { { Config : testAcc { Service } TagConfig ( rName , \"key1\" , \"value1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheck { Service } TagExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"key\" , \"key1\" ), resource . TestCheckResourceAttr ( resourceName , \"value\" , \"value1\" ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func TestAcc { Service } Tag_disappears ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_{service}_tag.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , { Service }. EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheck { Service } TagDestroy , Steps : [] resource . TestStep { { Config : testAcc { Service } TagConfig ( rName , \"key1\" , \"value1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheck { Service } TagExists ( resourceName ), acctest . CheckResourceDisappears ( acctest . Provider , resourceAws { Service } Tag (), resourceName ), ), ExpectNonEmptyPlan : true , }, }, }) } func TestAcc { Service } Tag_Value ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_{service}_tag.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , { Service }. EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheck { Service } TagDestroy , Steps : [] resource . TestStep { { Config : testAcc { Service } TagConfig ( rName , \"key1\" , \"value1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheck { Service } TagExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"key\" , \"key1\" ), resource . TestCheckResourceAttr ( resourceName , \"value\" , \"value1\" ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, { Config : testAcc { Service } TagConfig ( rName , \"key1\" , \"value1updated\" ), Check : resource . ComposeTestCheckFunc ( testAccCheck { Service } TagExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"key\" , \"key1\" ), resource . TestCheckResourceAttr ( resourceName , \"value\" , \"value1updated\" ), ), }, }, }) } func testAcc { Service } TagConfig ( rName string , key string , value string ) string { return fmt . Sprintf ( ` resource \"aws_{service}_{thing}\" \"test\" { name = %[1]q lifecycle { ignore_changes = [tags] } } resource \"aws_{service}_tag\" \"test\" { resource_arn = aws_{service}_{thing}.test.arn key = %[2]q value = %[3]q } ` , rName , key , value ) } Run make testacc TESTS=TestAcc{Service}Tags_ PKG={Service} and ensure there are no failures. Create website/docs/r/{service}_tag.html.markdown with initial documentation similar to the following: --- subcategory: \"{SERVICE}\" layout: \"aws\" page_title: \"AWS: aws_{service}_tag\" description: |- Manages an individual {SERVICE} resource tag --- # Resource: aws_{service}_tag Manages an individual {SERVICE} resource tag. This resource should only be used in cases where {SERVICE} resources are created outside Terraform (e.g., {SERVICE} {THING}s implicitly created by {OTHER SERVICE THING}). ~> **NOTE:** This tagging resource should not be combined with the Terraform resource for managing the parent resource. For example, using `aws_{service}_{thing}` and `aws_{service}_tag` to manage tags of the same {SERVICE} {THING} will cause a perpetual difference where the `aws_{service}_{thing}` resource will try to remove the tag being added by the `aws_{service}_tag` resource. ~> **NOTE:** This tagging resource does not use the [ provider `ignore_tags` configuration ]( /docs/providers/aws/index.html#ignore_tags ). ## Example Usage ```terraform resource \"aws_{service}_tag\" \"example\" { resource_arn = \"...\" key = \"Name\" value = \"Hello World\" } ``` ## Argument Reference The following arguments are supported: * `resource_arn` - (Required) ARN of the {SERVICE} resource to tag. * `key` - (Required) Tag name. * `value` - (Required) Tag value. ## Attributes Reference In addition to all arguments above, the following attributes are exported: * `id` - {SERVICE} resource identifier and key, separated by a comma ( `,` ) ## Import `aws_{service}_tag` can be imported by using the {SERVICE} resource identifier and key, separated by a comma ( `,` ), e.g. ```console $ terraform import aws_ { service } _tag.example arn:aws: { service } :us-east-1:123456789012: { thing } /example,Name ```","title":"Adding a New Tag Resource"},{"location":"aws-go-sdk-base/","text":"aws-go-sdk-base # https://github.com/hashicorp/aws-sdk-go-base This is a base library used by the AWS Provider , AWSCC Provider and the Terraform S3 Backend to allow them to handle authentication and other non-service level AWS interactions consistently. Typically this changes infrequently and changes are normally performed by HashiCorp maintainers. It should not be necessary to change this library for the majority of provider contributions.","title":"AWS Go SDK Base"},{"location":"aws-go-sdk-base/#aws-go-sdk-base","text":"https://github.com/hashicorp/aws-sdk-go-base This is a base library used by the AWS Provider , AWSCC Provider and the Terraform S3 Backend to allow them to handle authentication and other non-service level AWS interactions consistently. Typically this changes infrequently and changes are normally performed by HashiCorp maintainers. It should not be necessary to change this library for the majority of provider contributions.","title":"aws-go-sdk-base"},{"location":"aws-go-sdk-versions/","text":"AWS Go SDK Versions # The Terraform AWS Provider relies on the AWS SDK for Go which is maintained and published by AWS to allow us to safely and securely interact with AWS API's in a consistent fashion. There are two versions of this API, both of which are considered Generally Available and fully supported by AWS at present. AWS SDKs and Tools maintenance policy AWS SDKs and Tools version support matrix While the vast majority of the provider is based on the AWS SDK for Go v1 , the provider also allows the use of the AWS SDK for Go v2 . Which SDK Version should I use? # Each Terraform provider implementation for an AWS service relies on a service client which in turn is constructed based on a specific SDK version. At present, we are slowly increasing our footprint on SDK v2, but are not actively migrating existing code to use v2. The choice of SDK will be as follows: For new services, you should use AWS SDK for Go v2 . AWS has a migration guide that details the differences between the versions of the SDK. For existing services, use the version of the SDK that service currently uses. You can determine this by looking at the import section in the service's Go files. What does the SDK handle? # The AWS SDKs handle calling the various web service interfaces for AWS services. In addition to encoding and decoding the Go structures in the correct JSON or XML payloads, the SDKs handle authentication, request logging, and retrying requests. The various language SDKs and the AWS CLI share a consistent configuration interface, using environment variables and shared configuration and credentials files. The AWS SDKs also automatically retry several common failure cases, such as network errors. How do the SDK versions differ? # The AWS SDK for Go v1.0.0 was released in late 2015, when the current version of Go was v1.5. The Go language has evolved significantly since then. Many currently-recommended practices were not possible at that time, including the use of the context package, introduced in Go v1.7, and error wrapping, introduced in Go v1.13. The AWS SDK for Go v2 uses a modern Go style and has also been modularized, so that individual services are packaged and imported separately. For details on the specific changes to the AWS SDK for Go v2, see Migrating to the AWS SDK for Go v2 , especially the Service Clients section.","title":"AWS SDK for Go Versions"},{"location":"aws-go-sdk-versions/#aws-go-sdk-versions","text":"The Terraform AWS Provider relies on the AWS SDK for Go which is maintained and published by AWS to allow us to safely and securely interact with AWS API's in a consistent fashion. There are two versions of this API, both of which are considered Generally Available and fully supported by AWS at present. AWS SDKs and Tools maintenance policy AWS SDKs and Tools version support matrix While the vast majority of the provider is based on the AWS SDK for Go v1 , the provider also allows the use of the AWS SDK for Go v2 .","title":"AWS Go SDK Versions"},{"location":"aws-go-sdk-versions/#which-sdk-version-should-i-use","text":"Each Terraform provider implementation for an AWS service relies on a service client which in turn is constructed based on a specific SDK version. At present, we are slowly increasing our footprint on SDK v2, but are not actively migrating existing code to use v2. The choice of SDK will be as follows: For new services, you should use AWS SDK for Go v2 . AWS has a migration guide that details the differences between the versions of the SDK. For existing services, use the version of the SDK that service currently uses. You can determine this by looking at the import section in the service's Go files.","title":"Which SDK Version should I use?"},{"location":"aws-go-sdk-versions/#what-does-the-sdk-handle","text":"The AWS SDKs handle calling the various web service interfaces for AWS services. In addition to encoding and decoding the Go structures in the correct JSON or XML payloads, the SDKs handle authentication, request logging, and retrying requests. The various language SDKs and the AWS CLI share a consistent configuration interface, using environment variables and shared configuration and credentials files. The AWS SDKs also automatically retry several common failure cases, such as network errors.","title":"What does the SDK handle?"},{"location":"aws-go-sdk-versions/#how-do-the-sdk-versions-differ","text":"The AWS SDK for Go v1.0.0 was released in late 2015, when the current version of Go was v1.5. The Go language has evolved significantly since then. Many currently-recommended practices were not possible at that time, including the use of the context package, introduced in Go v1.7, and error wrapping, introduced in Go v1.13. The AWS SDK for Go v2 uses a modern Go style and has also been modularized, so that individual services are packaged and imported separately. For details on the specific changes to the AWS SDK for Go v2, see Migrating to the AWS SDK for Go v2 , especially the Service Clients section.","title":"How do the SDK versions differ?"},{"location":"bugs-and-enhancements/","text":"Making Small Changes to Existing Resources # Most contributions to the provider will take the form of small additions or bug-fixes on existing resources/data sources. In this case the existing resource will give you the best guidance on how the change should be structured, but we require the following to allow the change to be merged: Acceptance test coverage of new behavior : Existing resources each have a set of acceptance tests covering their functionality. These tests should exercise all the behavior of the resource. Whether you are adding something or fixing a bug, the idea is to have an acceptance test that fails if your code were to be removed. Sometimes it is sufficient to \"enhance\" an existing test by adding an assertion or tweaking the config that is used, but it's often better to add a new test. You can copy/paste an existing test and follow the conventions you see there, modifying the test to exercise the behavior of your code. Documentation updates : If your code makes any changes that need to be documented, you should include those documentation changes in the same PR. This includes things like new resource attributes or changes in default values. Well-formed Code : Do your best to follow existing conventions you see in the codebase, and ensure your code is formatted with go fmt . The PR reviewers can help out on this front, and may provide comments with suggestions on how to improve the code. Dependency updates : Create a separate PR if you are updating dependencies. This is to avoid conflicts as version updates tend to be fast- moving targets. We will plan to merge the PR with this change first. Changelog entry : Assuming the code change affects Terraform operators, the relevant PR ought to include a user-facing changelog entry describing the new behavior.","title":"Bugs and Enhancements"},{"location":"bugs-and-enhancements/#making-small-changes-to-existing-resources","text":"Most contributions to the provider will take the form of small additions or bug-fixes on existing resources/data sources. In this case the existing resource will give you the best guidance on how the change should be structured, but we require the following to allow the change to be merged: Acceptance test coverage of new behavior : Existing resources each have a set of acceptance tests covering their functionality. These tests should exercise all the behavior of the resource. Whether you are adding something or fixing a bug, the idea is to have an acceptance test that fails if your code were to be removed. Sometimes it is sufficient to \"enhance\" an existing test by adding an assertion or tweaking the config that is used, but it's often better to add a new test. You can copy/paste an existing test and follow the conventions you see there, modifying the test to exercise the behavior of your code. Documentation updates : If your code makes any changes that need to be documented, you should include those documentation changes in the same PR. This includes things like new resource attributes or changes in default values. Well-formed Code : Do your best to follow existing conventions you see in the codebase, and ensure your code is formatted with go fmt . The PR reviewers can help out on this front, and may provide comments with suggestions on how to improve the code. Dependency updates : Create a separate PR if you are updating dependencies. This is to avoid conflicts as version updates tend to be fast- moving targets. We will plan to merge the PR with this change first. Changelog entry : Assuming the code change affects Terraform operators, the relevant PR ought to include a user-facing changelog entry describing the new behavior.","title":"Making Small Changes to Existing Resources"},{"location":"changelog-process/","text":"Changelog Process # HashiCorp\u2019s open-source projects have always maintained user-friendly, readable CHANGELOG.md that allow users to tell at a glance whether a release should have any effect on them, and to gauge the risk of an upgrade. We use the go-changelog to generate and update the changelog from files created in the .changelog/ directory. It is important that when you raise your Pull Request, there is a changelog entry which describes the changes your contribution makes. Not all changes require an entry in the changelog, guidance follows on what changes do. Changelog format # The changelog format requires an entry in the following format, where HEADER corresponds to the changelog category, and the entry is the changelog entry itself. The entry should be included in a file in the .changelog directory with the naming convention {PR-NUMBER}.txt . For example, to create a changelog entry for pull request 1234, there should be a file named .changelog/1234.txt . ```release-note:{HEADER} {ENTRY} ``` If a pull request should contain multiple changelog entries, then multiple blocks can be added to the same changelog file. For example: ```release-note:note resource/aws_example_thing: The `broken` attribute has been deprecated. All configurations using `broken` should be updated to use the new `not_broken` attribute instead. ``` ```release-note:enhancement resource/aws_example_thing: Add `not_broken` attribute ``` Pull request types to CHANGELOG # The CHANGELOG is intended to show operator-impacting changes to the codebase for a particular version. If every change or commit to the code resulted in an entry, the CHANGELOG would become less useful for operators. The lists below are general guidelines and examples for when a decision needs to be made to decide whether a change should have an entry. Changes that should have a CHANGELOG entry # New resource # A new resource entry should only contain the name of the resource, and use the release-note:new-resource header. ```release-note:new-resource aws_secretsmanager_secret_policy ``` New data source # A new data source entry should only contain the name of the data source, and use the release-note:new-data-source header. ```release-note:new-data-source aws_workspaces_workspace ``` New full-length documentation guides (e.g., EKS Getting Started Guide, IAM Policy Documents with Terraform) # A new full length documentation entry gives the title of the documentation added, using the release-note:new-guide header. ```release-note:new-guide Custom Service Endpoint Configuration ``` Resource and provider bug fixes # A new bug entry should use the release-note:bug header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a provider prefix for provider level fixes. ```release-note:bug resource/aws_glue_classifier: Fix quote_symbol being optional ``` Resource and provider enhancements # A new enhancement entry should use the release-note:enhancement header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a provider prefix for provider level enhancements. ```release-note:enhancement resource/aws_eip: Add network_border_group argument ``` Deprecations # A deprecation entry should use the release-note:note header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a provider prefix for provider level changes. ```release-note:note resource/aws_dx_gateway_association: The vpn_gateway_id attribute is being deprecated in favor of the new associated_gateway_id attribute to support transit gateway associations ``` Breaking changes and removals # A breaking-change entry should use the release-note:breaking-change header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a provider prefix for provider level changes. ```release-note:breaking-change resource/aws_lambda_alias: Resource import no longer converts Lambda Function name to ARN ``` Region validation support # ```release-note:note provider: Region validation now automatically supports the new `XX-XXXXX-#` (Location) region. For AWS operations to work in the new region, the region must be explicitly enabled as outlined in the [ AWS Documentation ]( https://docs.aws.amazon.com/general/latest/gr/rande-manage.html#rande-manage-enable ). When the region is not enabled, the Terraform AWS Provider will return errors during credential validation (e.g., `error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid` ) or AWS operations will throw their own errors (e.g., `data.aws_availability_zones.available: Error fetching Availability Zones: AuthFailure: AWS was not able to validate the provided access credentials` ). [GH-####] ``` ```release-note:enhancement * provider: Support automatic region validation for `XX-XXXXX-#` [GH-####] ``` Changes that may have a CHANGELOG entry # Dependency updates: If the update contains relevant bug fixes or enhancements that affect operators, those should be called out. Any changes which do not fit into the above categories but warrant highlighting. Use resource/data source/provider prefixes where appropriate. ```release-note:note resource/aws_lambda_alias: Resource import no longer converts Lambda Function name to ARN ``` Changes that should not have a CHANGELOG entry # Resource and provider documentation updates Testing updates Code refactoring","title":"Changelog Process"},{"location":"changelog-process/#changelog-process","text":"HashiCorp\u2019s open-source projects have always maintained user-friendly, readable CHANGELOG.md that allow users to tell at a glance whether a release should have any effect on them, and to gauge the risk of an upgrade. We use the go-changelog to generate and update the changelog from files created in the .changelog/ directory. It is important that when you raise your Pull Request, there is a changelog entry which describes the changes your contribution makes. Not all changes require an entry in the changelog, guidance follows on what changes do.","title":"Changelog Process"},{"location":"changelog-process/#changelog-format","text":"The changelog format requires an entry in the following format, where HEADER corresponds to the changelog category, and the entry is the changelog entry itself. The entry should be included in a file in the .changelog directory with the naming convention {PR-NUMBER}.txt . For example, to create a changelog entry for pull request 1234, there should be a file named .changelog/1234.txt . ```release-note:{HEADER} {ENTRY} ``` If a pull request should contain multiple changelog entries, then multiple blocks can be added to the same changelog file. For example: ```release-note:note resource/aws_example_thing: The `broken` attribute has been deprecated. All configurations using `broken` should be updated to use the new `not_broken` attribute instead. ``` ```release-note:enhancement resource/aws_example_thing: Add `not_broken` attribute ```","title":"Changelog format"},{"location":"changelog-process/#pull-request-types-to-changelog","text":"The CHANGELOG is intended to show operator-impacting changes to the codebase for a particular version. If every change or commit to the code resulted in an entry, the CHANGELOG would become less useful for operators. The lists below are general guidelines and examples for when a decision needs to be made to decide whether a change should have an entry.","title":"Pull request types to CHANGELOG"},{"location":"changelog-process/#changes-that-should-have-a-changelog-entry","text":"","title":"Changes that should have a CHANGELOG entry"},{"location":"changelog-process/#new-resource","text":"A new resource entry should only contain the name of the resource, and use the release-note:new-resource header. ```release-note:new-resource aws_secretsmanager_secret_policy ```","title":"New resource"},{"location":"changelog-process/#new-data-source","text":"A new data source entry should only contain the name of the data source, and use the release-note:new-data-source header. ```release-note:new-data-source aws_workspaces_workspace ```","title":"New data source"},{"location":"changelog-process/#new-full-length-documentation-guides-eg-eks-getting-started-guide-iam-policy-documents-with-terraform","text":"A new full length documentation entry gives the title of the documentation added, using the release-note:new-guide header. ```release-note:new-guide Custom Service Endpoint Configuration ```","title":"New full-length documentation guides (e.g., EKS Getting Started Guide, IAM Policy Documents with Terraform)"},{"location":"changelog-process/#resource-and-provider-bug-fixes","text":"A new bug entry should use the release-note:bug header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a provider prefix for provider level fixes. ```release-note:bug resource/aws_glue_classifier: Fix quote_symbol being optional ```","title":"Resource and provider bug fixes"},{"location":"changelog-process/#resource-and-provider-enhancements","text":"A new enhancement entry should use the release-note:enhancement header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a provider prefix for provider level enhancements. ```release-note:enhancement resource/aws_eip: Add network_border_group argument ```","title":"Resource and provider enhancements"},{"location":"changelog-process/#deprecations","text":"A deprecation entry should use the release-note:note header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a provider prefix for provider level changes. ```release-note:note resource/aws_dx_gateway_association: The vpn_gateway_id attribute is being deprecated in favor of the new associated_gateway_id attribute to support transit gateway associations ```","title":"Deprecations"},{"location":"changelog-process/#breaking-changes-and-removals","text":"A breaking-change entry should use the release-note:breaking-change header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a provider prefix for provider level changes. ```release-note:breaking-change resource/aws_lambda_alias: Resource import no longer converts Lambda Function name to ARN ```","title":"Breaking changes and removals"},{"location":"changelog-process/#region-validation-support","text":"```release-note:note provider: Region validation now automatically supports the new `XX-XXXXX-#` (Location) region. For AWS operations to work in the new region, the region must be explicitly enabled as outlined in the [ AWS Documentation ]( https://docs.aws.amazon.com/general/latest/gr/rande-manage.html#rande-manage-enable ). When the region is not enabled, the Terraform AWS Provider will return errors during credential validation (e.g., `error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid` ) or AWS operations will throw their own errors (e.g., `data.aws_availability_zones.available: Error fetching Availability Zones: AuthFailure: AWS was not able to validate the provided access credentials` ). [GH-####] ``` ```release-note:enhancement * provider: Support automatic region validation for `XX-XXXXX-#` [GH-####] ```","title":"Region validation support"},{"location":"changelog-process/#changes-that-may-have-a-changelog-entry","text":"Dependency updates: If the update contains relevant bug fixes or enhancements that affect operators, those should be called out. Any changes which do not fit into the above categories but warrant highlighting. Use resource/data source/provider prefixes where appropriate. ```release-note:note resource/aws_lambda_alias: Resource import no longer converts Lambda Function name to ARN ```","title":"Changes that may have a CHANGELOG entry"},{"location":"changelog-process/#changes-that-should-not-have-a-changelog-entry","text":"Resource and provider documentation updates Testing updates Code refactoring","title":"Changes that should not have a CHANGELOG entry"},{"location":"core-services/","text":"Terraform AWS Provider Core Services # Core Services are AWS services we have identified as critical for a large majority of our users. Our goal is to continually increase the depth of coverage for these services. We will work to prioritize features and enhancements to these services in each weekly release, even if they are not necessarily highlighted in our quarterly roadmap. The core services we have identified are: EC2 Lambda EKS ECS VPC S3 RDS DynamoDB IAM Autoscaling (ASG) ElastiCache We'll continue to evaluate the selected services as our user base grows and changes.","title":"Terraform AWS Provider Core Services"},{"location":"core-services/#terraform-aws-provider-core-services","text":"Core Services are AWS services we have identified as critical for a large majority of our users. Our goal is to continually increase the depth of coverage for these services. We will work to prioritize features and enhancements to these services in each weekly release, even if they are not necessarily highlighted in our quarterly roadmap. The core services we have identified are: EC2 Lambda EKS ECS VPC S3 RDS DynamoDB IAM Autoscaling (ASG) ElastiCache We'll continue to evaluate the selected services as our user base grows and changes.","title":"Terraform AWS Provider Core Services"},{"location":"data-handling-and-conversion/","text":"Data Handling and Conversion # The Terraform AWS Provider codebase bridges the implementation of a Terraform Plugin and an AWS API client to support AWS operations and data types as Terraform Resources. Data handling and conversion is a large portion of resource implementation given the domain specific implementations of each side of the provider. The first where Terraform is a generic infrastructure as code tool with a generic data model and the other where the details are driven by AWS API data modeling concepts. This guide is intended to explain and show preferred Terraform AWS Provider code implementations required to successfully translate data between these two systems. At the bottom of this documentation is a Glossary section , which may be a helpful reference while reading the other sections. Data Conversions in Terraform Providers # Before getting into highly specific documentation about the Terraform AWS Provider handling of data, it may be helpful to briefly highlight how Terraform Plugins (Terraform Providers in this case) interact with Terraform CLI and the Terraform State in general and where this documentation fits into the whole process. There are two primary data flows that are typically handled by resources within a Terraform Provider. Data is either being converted from a planned new Terraform State into making a remote system request or a remote system response is being converted into a applied new Terraform State. The semantics of how the data of the planned new Terraform State is surfaced to the resource implementation is determined by where a resource is in its lifecycle and mainly handled by Terraform CLI. This concept can be explored further in the Terraform Resource Instance Change Lifecycle documentation , with the caveat that some additional behaviors occur within the Terraform Plugin SDK as well (if the Terraform Plugin uses that implementation detail). As a generic walkthrough, the following data handling occurs when creating a Terraform Resource: An operator creates a Terraform configuration with a new resource defined and runs terraform apply Terraform CLI merges an empty prior state for the resource, along with the given configuration state, to create a planned new state for the resource Terraform CLI sends a Terraform Plugin Protocol request to create the new resource with its planned new state data If the Terraform Plugin is using a higher level library, such as the Terraform Plugin SDK, that library receives the request and translates the Terraform Plugin Protocol data types into the expected library types Terraform Plugin invokes the resource creation function with the planned new state data The planned new state data is converted into an remote system request (e.g., API creation request) that is invoked The remote system response is received and the data is converted into an applied new state If the Terraform Plugin is using a higher level library, such as the Terraform Plugin SDK, that library translates the library types back into Terraform Plugin Protocol data types Terraform Plugin responds to Terraform Plugin Protocol request with the new state data Terraform CLI verifies and stores the new state The highlighted lines are the focus of this documentation today. In the future however, the Terraform AWS Provider may replace certain functionality in the items mentioning the Terraform Plugin SDK above to workaround certain limitations of that particular library. Implicit State Passthrough # An important behavior to note with Terraform State handling is if the value of a particular root attribute or block is not refreshed during plan or apply operations, then the prior Terraform State is implicitly deep copied to the new Terraform State for that attribute or block. Given a resource with a writeable root attribute named not_set_attr that never calls d.Set(\"not_set_attr\", /* ... nil or value */) , the following happens: If the Terraform configuration contains not_set_attr = \"anything\" on resource creation, the Terraform State contains not_set_attr equal to \"anything\" after apply. If the Terraform configuration is updated to not_set_attr = \"updated\" , the Terraform State contains not_set_attr equal to \"updated\" after apply. If the attribute was meant to be associated with a remote system value, it will never update the Terraform State on plan or apply with the remote value. Effectively, it cannot perform drift detection with the remote value. This however does not apply for nested attributes and blocks if the parent block is refreshed. Given a resource with a root block named parent , nested child attributes named set_attr and not_set_attr , and that calls d.Set(\"parent\", /* ... only refreshes nested set_attr ... */) , the Terraform State for the nested not_set_attr will not be copied. There are valid use cases for passthrough attribute values such as these (see the Virtual Attributes section ), however the behavior can be confusing or incorrect for operators if the drift detection is expected. Typically these types of drift detection issues can be discovered by implementing resource import testing with state verification. Data Conversions in the Terraform AWS Provider # To expand on the data handling that occurs specifically within the Terraform AWS Provider resource implementations, the above resource creation items become the below in practice given our current usage of the Terraform Plugin SDK: The Create / CreateContext function of a schema.Resource is invoked with *schema.ResourceData containing the planned new state data (conventionally named d ) and an AWS API client (conventionally named meta ). Note: Before reaching this point, the ResourceData was already translated from the Terraform Plugin Protocol data types by the Terraform Plugin SDK so values can be read by invoking d.Get() and d.GetOk() receiver methods with Attribute and Block names from the Schema of the schema.Resource . An AWS Go SDK operation input type (e.g., *ec2.CreateVpcInput ) is initialized For each necessary field to configure in the operation input type, the data is read from the ResourceData (e.g., d.Get() , d.GetOk() ) and converted into the AWS Go SDK type for the field (e.g., *string ) The AWS Go SDK operation is invoked and the output type (e.g., *ec2.CreateVpcOutput ) is initialized For each necessary Attribute, Block, or resource identifier to be saved in the state, the data is read from the AWS Go SDK type for the field ( *string ), if necessary converted into a ResourceData compatible type, and saved into a mutated ResourceData (e.g., d.Set() , d.SetId() ) Function is returned Type Mapping # To further understand the necessary data conversions used throughout the Terraform AWS Provider codebase between AWS Go SDK types and the Terraform Plugin SDK, the following table can be referenced for most scenarios: AWS API Model AWS Go SDK Terraform Plugin SDK Terraform Language/State boolean *bool TypeBool ( bool ) bool float *float64 TypeFloat ( float64 ) number integer *int64 TypeInt ( int ) number list []*T TypeList ( []interface{} of T ) TypeSet ( *schema.Set of T ) list(any) set(any) map map[T1]*T2 TypeMap ( map[string]interface{} ) map(any) string *string TypeString ( string ) string structure struct TypeList ( []interface{} of map[string]interface{} ) list(object(any)) timestamp *time.Time TypeString (typically RFC3339 formatted) string You may notice there are type encoding differences the AWS Go SDK and Terraform Plugin SDK: AWS Go SDK types are all Go pointer types, while Terraform Plugin SDK types are not. AWS Go SDK structures are the Go struct type, while there is no semantically equivalent Terraform Plugin SDK type. Instead they are represented as a slice of interfaces with an underlying map of interfaces. AWS Go SDK types are all Go concrete types, while the Terraform Plugin SDK types for collections and maps are interfaces. AWS Go SDK whole numeric type is always 64-bit, while the Terraform Plugin SDK type is implementation-specific. Conceptually, the first and second items above the most problematic in the Terraform AWS Provider codebase. The first item because non-pointer types in Go cannot implement the concept of no value ( nil ). The Zero Value Mapping section will go into more details about the implications of this limitation. The second item because it can be confusing to always handle a structure (\"object\") type as a list. There are efforts to replace the Terraform Plugin type system with one similar the underlying Terraform CLI type system. As these efforts materialize, this documentation will be updated. Zero Value Mapping # As mentioned in the Type Mapping section , there is a discrepancy with how the Terraform Plugin SDK represents values and the reality that a Terraform State may not configure an Attribute. These values will default to the matching underlying Go type \"zero value\" if not set: Terraform Plugin SDK Go Type Zero Value TypeBool bool false TypeFloat float64 0.0 TypeInt int 0 TypeString string \"\" For Terraform resource logic this means that these special values must always be accounted for in implementation. The semantics of the API and its meaning of the zero value will determine whether: If it is not used/needed, then generally the zero value can safely be used to store an \"unset\" value and should be ignored when sending to the API. If it is used/needed, whether: A value can always be set and it is safe to always send to the API. Generally, boolean values fall into this category. A different default/sentinel value must be used as the \"unset\" value so it can either match the default of the API or be ignored when sending to the API. A special type implementation is required within the schema to workaround the limitation. The maintainers can provide guidance on appropriate solutions for cases not mentioned in the Recommended Implementation section . Root Attributes Versus Block Attributes # All Attributes and Blocks at the top level of schema.Resource Schema are considered \"root\" attributes. These will always be handled with receiver methods on ResourceData , such as reading with d.Get() , d.GetOk() , etc. and writing with d.Set() . Any nested Attributes and Blocks inside those root Blocks will then be handled with standard Go types according to the table in the Type Mapping section . By convention in the codebase, each level of Block handling beyond root attributes should be separated into \"expand\" functions that convert Terraform Plugin SDK data into the equivalent AWS Go SDK type (typically named expand{Service}{Type} ) and \"flatten\" functions that convert an AWS Go SDK type into the equivalent Terraform Plugin SDK data (typically named flatten{Service}{Type} ). The Recommended Implementations section will go into those details. NOTE: While it is possible in certain type scenarios to deeply read and write ResourceData information for a Block Attribute, this practice is discouraged in preference of only handling root Attributes and Blocks. Recommended Implementations # Given the various complexities around the Terraform Plugin SDK type system, this section contains recommended implementations for Terraform AWS Provider resource code based on the Type Mapping section and the features of the Terraform Plugin SDK and AWS Go SDK. The eventual goal and styling for many of these recommendations is to ease static analysis of the codebase and future potential code generation efforts. Some of these coding patterns may not be well represented in the codebase, as refactoring the many older styles over years of community development is a large task. However this is meant to represent the preferred implementations today. These will continue to evolve as this codebase and the Terraform Plugin ecosystem changes. Where to Define Flex Functions # Define FLatten and EXpand (i.e., flex) functions at the most local level possible. This table provides guidance on the preferred place to define flex functions based on usage. Where Used Where to Define Include Service in Name One resource (e.g., aws_instance ) Resource file (e.g., internal/service/ec2/instance.go ) No Few resources in one service (e.g., EC2 ) Resource file or service flex file (e.g., internal/service/ec2/flex.go ) No Widely used in one service (e.g., EC2 ) Service flex file (e.g., internal/service/ec2/flex.go ) No Two services (e.g., EC2 and EKS ) Define a copy in each service If helpful 3+ services internal/flex/flex.go Yes Expand Functions for Blocks # func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { if tfMap == nil { return nil } apiObject := & service . Structure {} // ... nested attribute handling ... return apiObject } func expandStructures ( tfList [] interface {}) [] * service . Structure { if len ( tfList ) == 0 { return nil } var apiObjects [] * service . Structure for _ , tfMapRaw := range tfList { tfMap , ok := tfMapRaw .( map [ string ] interface {}) if ! ok { continue } apiObject := expandStructure ( tfMap ) if apiObject == nil { continue } apiObjects = append ( apiObjects , apiObject ) } return apiObjects } Flatten Functions for Blocks # func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { if apiObject == nil { return nil } tfMap := map [ string ] interface {}{} // ... nested attribute handling ... return tfMap } func flattenStructures ( apiObjects [] * service . Structure ) [] interface {} { if len ( apiObjects ) == 0 { return nil } var tfList [] interface {} for _ , apiObject := range apiObjects { if apiObject == nil { continue } tfList = append ( tfList , flattenStructure ( apiObject )) } return tfList } Root TypeBool and AWS Boolean # To read, if always sending the attribute value is correct: input := service . ExampleOperationInput { AttributeName : aws . String ( d . Get ( \"attribute_name\" ).( bool )) } Otherwise to read, if only sending the attribute value when true is preferred ( !ok for opposite): input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { input . AttributeName = aws . Bool ( v .( bool )) } To write: d . Set ( \"attribute_name\" , output . Thing . AttributeName ) Root TypeFloat and AWS Float # To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { input . AttributeName = aws . Float64 ( v .( float64 )) } To write: d . Set ( \"attribute_name\" , output . Thing . AttributeName ) Root TypeInt and AWS Integer # To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { input . AttributeName = aws . Int64 ( int64 ( v .( int ))) } To write: d . Set ( \"attribute_name\" , output . Thing . AttributeName ) Root TypeList of Resource and AWS List of Structure # To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && len ( v .([] interface {})) > 0 { input . AttributeName = expandStructures ( v .([] interface {})) } To write: if err := d . Set ( \"attribute_name\" , flattenStructures ( output . Thing . AttributeName )); err != nil { return fmt . Errorf ( \"error setting attribute_name: %w\" , err ) } Root TypeList of Resource and AWS Structure # To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && len ( v .([] interface {})) > 0 && v .([] interface {})[ 0 ] != nil { input . AttributeName = expandStructure ( v .([] interface {})[ 0 ].( map [ string ] interface {})) } To write ( likely to have helper function introduced soon ): if output . Thing . AttributeName != nil { if err := d . Set ( \"attribute_name\" , [] interface {}{ flattenStructure ( output . Thing . AttributeName )}); err != nil { return fmt . Errorf ( \"error setting attribute_name: %w\" , err ) } } else { d . Set ( \"attribute_name\" , nil ) } Root TypeList of TypeString and AWS List of String # To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && len ( v .([] interface {})) > 0 { input . AttributeName = flex . ExpandStringList ( v .([] interface {})) } To write: d . Set ( \"attribute_name\" , aws . StringValueSlice ( output . Thing . AttributeName )) Root TypeMap of TypeString and AWS Map of String # To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && len ( v .( map [ string ] interface {})) > 0 { input . AttributeName = flex . ExpandStringMap ( v .( map [ string ] interface {})) } To write: d . Set ( \"attribute_name\" , aws . StringValueMap ( output . Thing . AttributeName )) Root TypeSet of Resource and AWS List of Structure # To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && v .( * schema . Set ). Len () > 0 { input . AttributeName = expandStructures ( v .( * schema . Set ). List ()) } To write: if err := d . Set ( \"attribute_name\" , flattenStructures ( output . Thing . AttributeNames )); err != nil { return fmt . Errorf ( \"error setting attribute_name: %w\" , err ) } Root TypeSet of TypeString and AWS List of String # To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && v .( * schema . Set ). Len () > 0 { input . AttributeName = flex . ExpandStringSet ( v .( * schema . Set )) } To write: d . Set ( \"attribute_name\" , aws . StringValueSlice ( output . Thing . AttributeName )) Root TypeString and AWS String # To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { input . AttributeName = aws . String ( v .( string )) } To write: d . Set ( \"attribute_name\" , output . Thing . AttributeName ) Root TypeString and AWS Timestamp # To ensure that parsing the read string value does not fail, define attribute_name 's schema.Schema with an appropriate ValidateFunc : \"attribute_name\" : { Type : schema . TypeString , // ... ValidateFunc : validation . IsRFC3339Time , }, To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { v , _ := time . Parse ( time . RFC3339 , v .( string )) input . AttributeName = aws . Time ( v ) } To write: if output . Thing . AttributeName != nil { d . Set ( \"attribute_name\" , aws . TimeValue ( output . Thing . AttributeName ). Format ( time . RFC3339 )) } else { d . Set ( \"attribute_name\" , nil ) } Nested TypeBool and AWS Boolean # To read, if always sending the attribute value is correct: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( bool ); ok { apiObject . NestedAttributeName = aws . Bool ( v ) } // ... } To read, if only sending the attribute value when true is preferred ( !v for opposite): func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( bool ); ok && v { apiObject . NestedAttributeName = aws . Bool ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . BoolValue ( v ) } // ... } Nested TypeFloat and AWS Float # To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( float64 ); ok && v != 0.0 { apiObject . NestedAttributeName = aws . Float64 ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . Float64Value ( v ) } // ... } Nested TypeInt and AWS Integer # To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( int ); ok && v != 0 { apiObject . NestedAttributeName = aws . Int64 ( int64 ( v )) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . Int64Value ( v ) } // ... } Nested TypeList of Resource and AWS List of Structure # To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].([] interface {}); ok && len ( v ) > 0 { apiObject . NestedAttributeName = expandStructures ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = flattenNestedStructures ( v ) } // ... } Nested TypeList of Resource and AWS Structure # To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].([] interface {}); ok && len ( v ) > 0 && v [ 0 ] != nil { apiObject . NestedAttributeName = expandStructure ( v [ 0 ].( map [ string ] interface {})) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = [] interface {}{ flattenNestedStructure ( v )} } // ... } Nested TypeList of TypeString and AWS List of String # To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].([] interface {}); ok && len ( v ) > 0 { apiObject . NestedAttributeName = flex . ExpandStringList ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . StringValueSlice ( v ) } // ... } Nested TypeMap of TypeString and AWS Map of String # To read: input := service . ExampleOperationInput {} if v , ok := tfMap [ \"nested_attribute_name\" ].( map [ string ] interface {}); ok && len ( v ) > 0 { apiObject . NestedAttributeName = flex . ExpandStringMap ( v ) } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . StringValueMap ( v ) } // ... } Nested TypeSet of Resource and AWS List of Structure # To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( * schema . Set ); ok && v . Len () > 0 { apiObject . NestedAttributeName = expandStructures ( v . List ()) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = flattenNestedStructures ( v ) } // ... } Nested TypeSet of TypeString and AWS List of String # To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( * schema . Set ); ok && v . Len () > 0 { apiObject . NestedAttributeName = flex . ExpandStringSet ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . StringValueSlice ( v ) } // ... } Nested TypeString and AWS String # To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( string ); ok && v != \"\" { apiObject . NestedAttributeName = aws . String ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . StringValue ( v ) } // ... } Nested TypeString and AWS Timestamp # To ensure that parsing the read string value does not fail, define nested_attribute_name 's schema.Schema with an appropriate ValidateFunc : \"nested_attribute_name\" : { Type : schema . TypeString , // ... ValidateFunc : validation . IsRFC3339Time , }, To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( string ); ok && v != \"\" { v , _ := time . Parse ( time . RFC3339 , v ) apiObject . NestedAttributeName = aws . Time ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . TimeValue ( v ). Format ( time . RFC3339 ) } // ... } Further Guidelines # This section includes additional topics related to data design and decision making from the Terraform AWS Provider maintainers. Binary Values # Certain resources may need to interact with binary (non UTF-8) data while the Terraform State only supports UTF-8 data. Configurations attempting to pass binary data to an attribute will receive an error from Terraform CLI. These attributes should expect and store the value as a Base64 string while performing any necessary encoding or decoding in the resource logic. Destroy State Values # During resource destroy operations, only previously applied Terraform State values are available to resource logic. Even if the configuration is updated in a manner where both the resource destroy is triggered (e.g., setting the resource meta-argument count = 0 ) and an attribute value is updated, the resource logic will only have the previously applied data values. Any usage of attribute values during destroy should explicitly note in the resource documentation that the desired value must be applied into the Terraform State before any apply to destroy the resource. Hashed Values # Attribute values may be very lengthy or potentially contain Sensitive Values . A potential solution might be to use a hashing algorithm, such as MD5 or SHA256, to convert the value before saving in the Terraform State to reduce its relative size or attempt to obfuscate the value. However, there are a few reasons not to do so: Terraform expects any planned values to match applied values. Ensuring proper handling during the various Terraform operations such as difference planning and Terraform State storage can be a burden. Hashed values are generally unusable in downstream attribute references. If a value is hashed, it cannot be successfully used in another resource or provider configuration that expects the real value. Terraform plan differences are meant to be human readable. If a value is hashed, operators will only see the relatively unhelpful hash differences abc123 -> def456 in plans. Any value hashing implementation will not be accepted. An exception to this guidance is if the remote system explicitly provides a separate hash value in responses, in which a resource can provide a separate attribute with that hashed value. Sensitive Values # Marking an Attribute in the Terraform Plugin SDK Schema with Sensitive has the following real world implications: All occurrences of the Attribute will have the value hidden in plan difference output. In the context of an Attribute within a Block, all Blocks will hide all values of the Attribute. In Terraform CLI 0.14 (with the provider_sensitive_attrs experiment enabled) and later, any downstream references to the value in other configuration will hide the value in plan difference output. The value is either always hidden or not as the Terraform Plugin SDK does not currently implement conditional support for this functionality. Since Terraform Configurations have no control over the behavior, hiding values from the plan difference can incur a potentially undesirable user experience cost for operators. Given that and especially with the improvements in Terraform CLI 0.14, the Terraform AWS Provider maintainers guiding principles for determining whether an Attribute should be marked as Sensitive is if an Attribute value: Objectively will always contain a credential, password, or other secret material. Operators can have differing opinions on what constitutes secret material and the maintainers will make best effort determinations, if necessary consulting with the HashiCorp Security team. If the Attribute is within a Block, that all occurrences of the Attribute value will objectively contain secret material. Some APIs (and therefore the Terraform AWS Provider resources) implement generic \"setting\" and \"value\" structures which likely will contain a mixture of secret and non-secret material. These will generally not be accepted for marking as Sensitive . If you are unsatisfied with sensitive value handling, the maintainers can recommend ensuring there is a covering issue in the Terraform CLI and/or Terraform Plugin SDK projects explaining the use case. Ultimately, Terraform Plugins including the Terraform AWS Provider cannot implement their own sensitive value abilities if the upstream projects do not implement the appropriate functionality. Virtual Attributes # Attributes which only exist within Terraform and not the remote system are typically referred as virtual attributes. Especially in the case of Destroy State Values , these attributes rely on the Implicit State Passthrough behavior of values in Terraform to be available in resource logic. A fictitous example of one of these may be a resource attribute such as a skip_waiting flag, which is used only in the resource logic to skip the typical behavior of waiting for operations to complete. If a virtual attribute has a default value that does not match the Zero Value Mapping for the type, it is recommended to explicitly call d.Set() with the default value in the schema.Resource Importer State function, for example: & schema . Resource { // ... other fields ... Importer : & schema . ResourceImporter { State : func ( d * schema . ResourceData , meta interface {}) ([] * schema . ResourceData , error ) { d . Set ( \"skip_waiting\" , true ) return [] * schema . ResourceData { d }, nil }, }, } This helps prevent an immediate plan difference after resource import unless the configuration has a non-default value. Glossary # Below is a listing of relevant terms and descriptions for data handling and conversion in the Terraform AWS Provider to establish common conventions throughout this documentation. This list is not exhaustive of all concepts of Terraform Plugins, the Terraform AWS Provider, or the data handling that occurs during Terraform runs, but these should generally provide enough context about the topics discussed here. AWS Go SDK : Library that converts Go code into AWS Service API compatible operations and data types. Currently refers to version 1 (v1) available since 2015, however version 2 (v2) will reach general availability status soon. Project . AWS Go SDK Model : AWS Go SDK compatible format of AWS Service API Model. AWS Go SDK Service : AWS Service API Go code generated from the AWS Go SDK Model. Generated by the AWS Go SDK code. AWS Service API : Logical boundary of an AWS service by API endpoint. Some large AWS services may be marketed with many different product names under the same service API (e.g., VPC functionality is part of the EC2 API) and vice-versa where some services may be marketed with one product name but are split into multiple service APIs (e.g., Single Sign-On functionality is split into the Identity Store and SSO Admin APIs). AWS Service API Model : Declarative description of the AWS Service API operations and data types. Generated by the AWS service teams. Used to operate the API and generate API clients such as the various AWS Software Development Kits (SDKs). Terraform Language (\"Configuration\"): Configuration syntax interpreted by the Terraform CLI. An implementation of HCL . Full Documentation . Terraform Plugin Protocol : Description of Terraform Plugin operations and data types. Currently based on the Remote Procedure Call (RPC) library gRPC . Terraform Plugin Go : Low-level library that converts Go code into Terraform Plugin Protocol compatible operations and data types. Not currently implemented in the Terraform AWS Provider. Project . Terraform Plugin SDK : High-level library that converts Go code into Terraform Plugin Protocol compatible operations and data types. Project . Terraform Plugin SDK Schema : Declarative description of types and domain specific behaviors for a Terraform provider, including resources and attributes. Full Documentation . Terraform State : Bindings between objects in a remote system (e.g., an EC2 VPC) and a Terraform configuration (e.g., an aws_vpc resource configuration). Full Documentation . AWS Service API Models use specific terminology to describe data and types: Enumeration : Collection of valid values for a Shape. Operation : An API call. Includes information about input, output, and error Shapes. Shape : Type description. boolean : Boolean value. float : Fractional numeric value. May contain value validation such as maximum or minimum. integer : Whole numeric value. May contain value validation such as maximum or minimum. list : Collection that contains member Shapes. May contain value validation such as maximum or minimum keys. map : Grouping of key Shape to value Shape. May contain value validation such as maximum or minimum keys. string : Sequence of characters. May contain value validation such as an enumeration, regular expression pattern, maximum length, or minimum length. structure : Object that contains member Shapes. May represent an error. timestamp : Date and time value. The Terraform Language uses the following terminology to describe data and types: Attribute (\"Argument\"): Assigns a name to a data value. Block (\"Configuration Block\"): Container type for Attributes or Blocks. null : Virtual value equivalent to the Attribute not being set. Types : Full Documentation . any : Virtual type representing any concrete type in type declarations. bool : Boolean value. list (\"tuple\"): Ordered collection of values. map (\"object\"): Grouping of string keys to values. number : Numeric value. Can be either whole or fractional numbers. set : Unordered collection of values. string : Sequence of characters. Terraform Plugin SDK Schemas use the following terminology to describe data and types: Behaviors : Full Documentation . Sensitive : Whether the value should be hidden from user interface output. StateFunc : Conversion function between the value set by the Terraform Plugin and the value seen by Terraform Plugin SDK (and ultimately the Terraform State). Element : Underylying value type for a collection or grouping Schema. Resource Data : Data representation of a Resource Schema. Translation layer between the Schema and Go code of a Terraform Plugin. In the Terraform Plugin SDK, the ResourceData Go type. Resource Schema : Grouping of Schema that represents a Terraform Resource. Schema : Represents an Attribute or Block. Has a Type and Behavior(s). Types : Full Documentation . TypeBool : Boolean value. TypeFloat : Fractional numeric value. TypeInt : Whole numeric value. TypeList : Ordered collection of values or Blocks. TypeMap : Grouping of key Type to value Type. TypeSet : Unordered collection of values or Blocks. TypeString : Sequence of characters value. Some other terms that may be used: Block Attribute (\"Child Attribute\", \"Nested Attribute\"): Block level Attribute. Expand Function : Function that converts Terraform Plugin SDK data into the equivalent AWS Go SDK type. Flatten Function : Function that converts an AWS Go SDK type into the equivalent Terraform Plugin SDK data. NullableTypeBool : Workaround \"schema type\" created to accept a boolean value that is not configured in addition to true and false. Not implemented in the Terraform Plugin SDK, but uses TypeString (where \"\" represents not configured) and additional validation. NullableTypeFloat : Workaround \"schema type\" created to accept a fractional numeric value that is not configured in addition to 0.0 . Not implemented in the Terraform Plugin SDK, but uses TypeString (where \"\" represents not configured) and additional validation. NullableTypeInt : Workaround \"schema type\" created to accept a whole numeric value that is not configured in addition to 0 . Not implemented in the Terraform Plugin SDK, but uses TypeString (where \"\" represents not configured) and additional validation. Root Attribute : Resource top level Attribute or Block. For additional reference, the Terraform documentation also includes a full glossary of terminology .","title":"Data Handling and Conversion"},{"location":"data-handling-and-conversion/#data-handling-and-conversion","text":"The Terraform AWS Provider codebase bridges the implementation of a Terraform Plugin and an AWS API client to support AWS operations and data types as Terraform Resources. Data handling and conversion is a large portion of resource implementation given the domain specific implementations of each side of the provider. The first where Terraform is a generic infrastructure as code tool with a generic data model and the other where the details are driven by AWS API data modeling concepts. This guide is intended to explain and show preferred Terraform AWS Provider code implementations required to successfully translate data between these two systems. At the bottom of this documentation is a Glossary section , which may be a helpful reference while reading the other sections.","title":"Data Handling and Conversion"},{"location":"data-handling-and-conversion/#data-conversions-in-terraform-providers","text":"Before getting into highly specific documentation about the Terraform AWS Provider handling of data, it may be helpful to briefly highlight how Terraform Plugins (Terraform Providers in this case) interact with Terraform CLI and the Terraform State in general and where this documentation fits into the whole process. There are two primary data flows that are typically handled by resources within a Terraform Provider. Data is either being converted from a planned new Terraform State into making a remote system request or a remote system response is being converted into a applied new Terraform State. The semantics of how the data of the planned new Terraform State is surfaced to the resource implementation is determined by where a resource is in its lifecycle and mainly handled by Terraform CLI. This concept can be explored further in the Terraform Resource Instance Change Lifecycle documentation , with the caveat that some additional behaviors occur within the Terraform Plugin SDK as well (if the Terraform Plugin uses that implementation detail). As a generic walkthrough, the following data handling occurs when creating a Terraform Resource: An operator creates a Terraform configuration with a new resource defined and runs terraform apply Terraform CLI merges an empty prior state for the resource, along with the given configuration state, to create a planned new state for the resource Terraform CLI sends a Terraform Plugin Protocol request to create the new resource with its planned new state data If the Terraform Plugin is using a higher level library, such as the Terraform Plugin SDK, that library receives the request and translates the Terraform Plugin Protocol data types into the expected library types Terraform Plugin invokes the resource creation function with the planned new state data The planned new state data is converted into an remote system request (e.g., API creation request) that is invoked The remote system response is received and the data is converted into an applied new state If the Terraform Plugin is using a higher level library, such as the Terraform Plugin SDK, that library translates the library types back into Terraform Plugin Protocol data types Terraform Plugin responds to Terraform Plugin Protocol request with the new state data Terraform CLI verifies and stores the new state The highlighted lines are the focus of this documentation today. In the future however, the Terraform AWS Provider may replace certain functionality in the items mentioning the Terraform Plugin SDK above to workaround certain limitations of that particular library.","title":"Data Conversions in Terraform Providers"},{"location":"data-handling-and-conversion/#implicit-state-passthrough","text":"An important behavior to note with Terraform State handling is if the value of a particular root attribute or block is not refreshed during plan or apply operations, then the prior Terraform State is implicitly deep copied to the new Terraform State for that attribute or block. Given a resource with a writeable root attribute named not_set_attr that never calls d.Set(\"not_set_attr\", /* ... nil or value */) , the following happens: If the Terraform configuration contains not_set_attr = \"anything\" on resource creation, the Terraform State contains not_set_attr equal to \"anything\" after apply. If the Terraform configuration is updated to not_set_attr = \"updated\" , the Terraform State contains not_set_attr equal to \"updated\" after apply. If the attribute was meant to be associated with a remote system value, it will never update the Terraform State on plan or apply with the remote value. Effectively, it cannot perform drift detection with the remote value. This however does not apply for nested attributes and blocks if the parent block is refreshed. Given a resource with a root block named parent , nested child attributes named set_attr and not_set_attr , and that calls d.Set(\"parent\", /* ... only refreshes nested set_attr ... */) , the Terraform State for the nested not_set_attr will not be copied. There are valid use cases for passthrough attribute values such as these (see the Virtual Attributes section ), however the behavior can be confusing or incorrect for operators if the drift detection is expected. Typically these types of drift detection issues can be discovered by implementing resource import testing with state verification.","title":"Implicit State Passthrough"},{"location":"data-handling-and-conversion/#data-conversions-in-the-terraform-aws-provider","text":"To expand on the data handling that occurs specifically within the Terraform AWS Provider resource implementations, the above resource creation items become the below in practice given our current usage of the Terraform Plugin SDK: The Create / CreateContext function of a schema.Resource is invoked with *schema.ResourceData containing the planned new state data (conventionally named d ) and an AWS API client (conventionally named meta ). Note: Before reaching this point, the ResourceData was already translated from the Terraform Plugin Protocol data types by the Terraform Plugin SDK so values can be read by invoking d.Get() and d.GetOk() receiver methods with Attribute and Block names from the Schema of the schema.Resource . An AWS Go SDK operation input type (e.g., *ec2.CreateVpcInput ) is initialized For each necessary field to configure in the operation input type, the data is read from the ResourceData (e.g., d.Get() , d.GetOk() ) and converted into the AWS Go SDK type for the field (e.g., *string ) The AWS Go SDK operation is invoked and the output type (e.g., *ec2.CreateVpcOutput ) is initialized For each necessary Attribute, Block, or resource identifier to be saved in the state, the data is read from the AWS Go SDK type for the field ( *string ), if necessary converted into a ResourceData compatible type, and saved into a mutated ResourceData (e.g., d.Set() , d.SetId() ) Function is returned","title":"Data Conversions in the Terraform AWS Provider"},{"location":"data-handling-and-conversion/#type-mapping","text":"To further understand the necessary data conversions used throughout the Terraform AWS Provider codebase between AWS Go SDK types and the Terraform Plugin SDK, the following table can be referenced for most scenarios: AWS API Model AWS Go SDK Terraform Plugin SDK Terraform Language/State boolean *bool TypeBool ( bool ) bool float *float64 TypeFloat ( float64 ) number integer *int64 TypeInt ( int ) number list []*T TypeList ( []interface{} of T ) TypeSet ( *schema.Set of T ) list(any) set(any) map map[T1]*T2 TypeMap ( map[string]interface{} ) map(any) string *string TypeString ( string ) string structure struct TypeList ( []interface{} of map[string]interface{} ) list(object(any)) timestamp *time.Time TypeString (typically RFC3339 formatted) string You may notice there are type encoding differences the AWS Go SDK and Terraform Plugin SDK: AWS Go SDK types are all Go pointer types, while Terraform Plugin SDK types are not. AWS Go SDK structures are the Go struct type, while there is no semantically equivalent Terraform Plugin SDK type. Instead they are represented as a slice of interfaces with an underlying map of interfaces. AWS Go SDK types are all Go concrete types, while the Terraform Plugin SDK types for collections and maps are interfaces. AWS Go SDK whole numeric type is always 64-bit, while the Terraform Plugin SDK type is implementation-specific. Conceptually, the first and second items above the most problematic in the Terraform AWS Provider codebase. The first item because non-pointer types in Go cannot implement the concept of no value ( nil ). The Zero Value Mapping section will go into more details about the implications of this limitation. The second item because it can be confusing to always handle a structure (\"object\") type as a list. There are efforts to replace the Terraform Plugin type system with one similar the underlying Terraform CLI type system. As these efforts materialize, this documentation will be updated.","title":"Type Mapping"},{"location":"data-handling-and-conversion/#zero-value-mapping","text":"As mentioned in the Type Mapping section , there is a discrepancy with how the Terraform Plugin SDK represents values and the reality that a Terraform State may not configure an Attribute. These values will default to the matching underlying Go type \"zero value\" if not set: Terraform Plugin SDK Go Type Zero Value TypeBool bool false TypeFloat float64 0.0 TypeInt int 0 TypeString string \"\" For Terraform resource logic this means that these special values must always be accounted for in implementation. The semantics of the API and its meaning of the zero value will determine whether: If it is not used/needed, then generally the zero value can safely be used to store an \"unset\" value and should be ignored when sending to the API. If it is used/needed, whether: A value can always be set and it is safe to always send to the API. Generally, boolean values fall into this category. A different default/sentinel value must be used as the \"unset\" value so it can either match the default of the API or be ignored when sending to the API. A special type implementation is required within the schema to workaround the limitation. The maintainers can provide guidance on appropriate solutions for cases not mentioned in the Recommended Implementation section .","title":"Zero Value Mapping"},{"location":"data-handling-and-conversion/#root-attributes-versus-block-attributes","text":"All Attributes and Blocks at the top level of schema.Resource Schema are considered \"root\" attributes. These will always be handled with receiver methods on ResourceData , such as reading with d.Get() , d.GetOk() , etc. and writing with d.Set() . Any nested Attributes and Blocks inside those root Blocks will then be handled with standard Go types according to the table in the Type Mapping section . By convention in the codebase, each level of Block handling beyond root attributes should be separated into \"expand\" functions that convert Terraform Plugin SDK data into the equivalent AWS Go SDK type (typically named expand{Service}{Type} ) and \"flatten\" functions that convert an AWS Go SDK type into the equivalent Terraform Plugin SDK data (typically named flatten{Service}{Type} ). The Recommended Implementations section will go into those details. NOTE: While it is possible in certain type scenarios to deeply read and write ResourceData information for a Block Attribute, this practice is discouraged in preference of only handling root Attributes and Blocks.","title":"Root Attributes Versus Block Attributes"},{"location":"data-handling-and-conversion/#recommended-implementations","text":"Given the various complexities around the Terraform Plugin SDK type system, this section contains recommended implementations for Terraform AWS Provider resource code based on the Type Mapping section and the features of the Terraform Plugin SDK and AWS Go SDK. The eventual goal and styling for many of these recommendations is to ease static analysis of the codebase and future potential code generation efforts. Some of these coding patterns may not be well represented in the codebase, as refactoring the many older styles over years of community development is a large task. However this is meant to represent the preferred implementations today. These will continue to evolve as this codebase and the Terraform Plugin ecosystem changes.","title":"Recommended Implementations"},{"location":"data-handling-and-conversion/#where-to-define-flex-functions","text":"Define FLatten and EXpand (i.e., flex) functions at the most local level possible. This table provides guidance on the preferred place to define flex functions based on usage. Where Used Where to Define Include Service in Name One resource (e.g., aws_instance ) Resource file (e.g., internal/service/ec2/instance.go ) No Few resources in one service (e.g., EC2 ) Resource file or service flex file (e.g., internal/service/ec2/flex.go ) No Widely used in one service (e.g., EC2 ) Service flex file (e.g., internal/service/ec2/flex.go ) No Two services (e.g., EC2 and EKS ) Define a copy in each service If helpful 3+ services internal/flex/flex.go Yes","title":"Where to Define Flex Functions"},{"location":"data-handling-and-conversion/#expand-functions-for-blocks","text":"func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { if tfMap == nil { return nil } apiObject := & service . Structure {} // ... nested attribute handling ... return apiObject } func expandStructures ( tfList [] interface {}) [] * service . Structure { if len ( tfList ) == 0 { return nil } var apiObjects [] * service . Structure for _ , tfMapRaw := range tfList { tfMap , ok := tfMapRaw .( map [ string ] interface {}) if ! ok { continue } apiObject := expandStructure ( tfMap ) if apiObject == nil { continue } apiObjects = append ( apiObjects , apiObject ) } return apiObjects }","title":"Expand Functions for Blocks"},{"location":"data-handling-and-conversion/#flatten-functions-for-blocks","text":"func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { if apiObject == nil { return nil } tfMap := map [ string ] interface {}{} // ... nested attribute handling ... return tfMap } func flattenStructures ( apiObjects [] * service . Structure ) [] interface {} { if len ( apiObjects ) == 0 { return nil } var tfList [] interface {} for _ , apiObject := range apiObjects { if apiObject == nil { continue } tfList = append ( tfList , flattenStructure ( apiObject )) } return tfList }","title":"Flatten Functions for Blocks"},{"location":"data-handling-and-conversion/#root-typebool-and-aws-boolean","text":"To read, if always sending the attribute value is correct: input := service . ExampleOperationInput { AttributeName : aws . String ( d . Get ( \"attribute_name\" ).( bool )) } Otherwise to read, if only sending the attribute value when true is preferred ( !ok for opposite): input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { input . AttributeName = aws . Bool ( v .( bool )) } To write: d . Set ( \"attribute_name\" , output . Thing . AttributeName )","title":"Root TypeBool and AWS Boolean"},{"location":"data-handling-and-conversion/#root-typefloat-and-aws-float","text":"To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { input . AttributeName = aws . Float64 ( v .( float64 )) } To write: d . Set ( \"attribute_name\" , output . Thing . AttributeName )","title":"Root TypeFloat and AWS Float"},{"location":"data-handling-and-conversion/#root-typeint-and-aws-integer","text":"To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { input . AttributeName = aws . Int64 ( int64 ( v .( int ))) } To write: d . Set ( \"attribute_name\" , output . Thing . AttributeName )","title":"Root TypeInt and AWS Integer"},{"location":"data-handling-and-conversion/#root-typelist-of-resource-and-aws-list-of-structure","text":"To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && len ( v .([] interface {})) > 0 { input . AttributeName = expandStructures ( v .([] interface {})) } To write: if err := d . Set ( \"attribute_name\" , flattenStructures ( output . Thing . AttributeName )); err != nil { return fmt . Errorf ( \"error setting attribute_name: %w\" , err ) }","title":"Root TypeList of Resource and AWS List of Structure"},{"location":"data-handling-and-conversion/#root-typelist-of-resource-and-aws-structure","text":"To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && len ( v .([] interface {})) > 0 && v .([] interface {})[ 0 ] != nil { input . AttributeName = expandStructure ( v .([] interface {})[ 0 ].( map [ string ] interface {})) } To write ( likely to have helper function introduced soon ): if output . Thing . AttributeName != nil { if err := d . Set ( \"attribute_name\" , [] interface {}{ flattenStructure ( output . Thing . AttributeName )}); err != nil { return fmt . Errorf ( \"error setting attribute_name: %w\" , err ) } } else { d . Set ( \"attribute_name\" , nil ) }","title":"Root TypeList of Resource and AWS Structure"},{"location":"data-handling-and-conversion/#root-typelist-of-typestring-and-aws-list-of-string","text":"To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && len ( v .([] interface {})) > 0 { input . AttributeName = flex . ExpandStringList ( v .([] interface {})) } To write: d . Set ( \"attribute_name\" , aws . StringValueSlice ( output . Thing . AttributeName ))","title":"Root TypeList of TypeString and AWS List of String"},{"location":"data-handling-and-conversion/#root-typemap-of-typestring-and-aws-map-of-string","text":"To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && len ( v .( map [ string ] interface {})) > 0 { input . AttributeName = flex . ExpandStringMap ( v .( map [ string ] interface {})) } To write: d . Set ( \"attribute_name\" , aws . StringValueMap ( output . Thing . AttributeName ))","title":"Root TypeMap of TypeString and AWS Map of String"},{"location":"data-handling-and-conversion/#root-typeset-of-resource-and-aws-list-of-structure","text":"To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && v .( * schema . Set ). Len () > 0 { input . AttributeName = expandStructures ( v .( * schema . Set ). List ()) } To write: if err := d . Set ( \"attribute_name\" , flattenStructures ( output . Thing . AttributeNames )); err != nil { return fmt . Errorf ( \"error setting attribute_name: %w\" , err ) }","title":"Root TypeSet of Resource and AWS List of Structure"},{"location":"data-handling-and-conversion/#root-typeset-of-typestring-and-aws-list-of-string","text":"To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok && v .( * schema . Set ). Len () > 0 { input . AttributeName = flex . ExpandStringSet ( v .( * schema . Set )) } To write: d . Set ( \"attribute_name\" , aws . StringValueSlice ( output . Thing . AttributeName ))","title":"Root TypeSet of TypeString and AWS List of String"},{"location":"data-handling-and-conversion/#root-typestring-and-aws-string","text":"To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { input . AttributeName = aws . String ( v .( string )) } To write: d . Set ( \"attribute_name\" , output . Thing . AttributeName )","title":"Root TypeString and AWS String"},{"location":"data-handling-and-conversion/#root-typestring-and-aws-timestamp","text":"To ensure that parsing the read string value does not fail, define attribute_name 's schema.Schema with an appropriate ValidateFunc : \"attribute_name\" : { Type : schema . TypeString , // ... ValidateFunc : validation . IsRFC3339Time , }, To read: input := service . ExampleOperationInput {} if v , ok := d . GetOk ( \"attribute_name\" ); ok { v , _ := time . Parse ( time . RFC3339 , v .( string )) input . AttributeName = aws . Time ( v ) } To write: if output . Thing . AttributeName != nil { d . Set ( \"attribute_name\" , aws . TimeValue ( output . Thing . AttributeName ). Format ( time . RFC3339 )) } else { d . Set ( \"attribute_name\" , nil ) }","title":"Root TypeString and AWS Timestamp"},{"location":"data-handling-and-conversion/#nested-typebool-and-aws-boolean","text":"To read, if always sending the attribute value is correct: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( bool ); ok { apiObject . NestedAttributeName = aws . Bool ( v ) } // ... } To read, if only sending the attribute value when true is preferred ( !v for opposite): func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( bool ); ok && v { apiObject . NestedAttributeName = aws . Bool ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . BoolValue ( v ) } // ... }","title":"Nested TypeBool and AWS Boolean"},{"location":"data-handling-and-conversion/#nested-typefloat-and-aws-float","text":"To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( float64 ); ok && v != 0.0 { apiObject . NestedAttributeName = aws . Float64 ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . Float64Value ( v ) } // ... }","title":"Nested TypeFloat and AWS Float"},{"location":"data-handling-and-conversion/#nested-typeint-and-aws-integer","text":"To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( int ); ok && v != 0 { apiObject . NestedAttributeName = aws . Int64 ( int64 ( v )) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . Int64Value ( v ) } // ... }","title":"Nested TypeInt and AWS Integer"},{"location":"data-handling-and-conversion/#nested-typelist-of-resource-and-aws-list-of-structure","text":"To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].([] interface {}); ok && len ( v ) > 0 { apiObject . NestedAttributeName = expandStructures ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = flattenNestedStructures ( v ) } // ... }","title":"Nested TypeList of Resource and AWS List of Structure"},{"location":"data-handling-and-conversion/#nested-typelist-of-resource-and-aws-structure","text":"To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].([] interface {}); ok && len ( v ) > 0 && v [ 0 ] != nil { apiObject . NestedAttributeName = expandStructure ( v [ 0 ].( map [ string ] interface {})) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = [] interface {}{ flattenNestedStructure ( v )} } // ... }","title":"Nested TypeList of Resource and AWS Structure"},{"location":"data-handling-and-conversion/#nested-typelist-of-typestring-and-aws-list-of-string","text":"To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].([] interface {}); ok && len ( v ) > 0 { apiObject . NestedAttributeName = flex . ExpandStringList ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . StringValueSlice ( v ) } // ... }","title":"Nested TypeList of TypeString and AWS List of String"},{"location":"data-handling-and-conversion/#nested-typemap-of-typestring-and-aws-map-of-string","text":"To read: input := service . ExampleOperationInput {} if v , ok := tfMap [ \"nested_attribute_name\" ].( map [ string ] interface {}); ok && len ( v ) > 0 { apiObject . NestedAttributeName = flex . ExpandStringMap ( v ) } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . StringValueMap ( v ) } // ... }","title":"Nested TypeMap of TypeString and AWS Map of String"},{"location":"data-handling-and-conversion/#nested-typeset-of-resource-and-aws-list-of-structure","text":"To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( * schema . Set ); ok && v . Len () > 0 { apiObject . NestedAttributeName = expandStructures ( v . List ()) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = flattenNestedStructures ( v ) } // ... }","title":"Nested TypeSet of Resource and AWS List of Structure"},{"location":"data-handling-and-conversion/#nested-typeset-of-typestring-and-aws-list-of-string","text":"To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( * schema . Set ); ok && v . Len () > 0 { apiObject . NestedAttributeName = flex . ExpandStringSet ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . StringValueSlice ( v ) } // ... }","title":"Nested TypeSet of TypeString and AWS List of String"},{"location":"data-handling-and-conversion/#nested-typestring-and-aws-string","text":"To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( string ); ok && v != \"\" { apiObject . NestedAttributeName = aws . String ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . StringValue ( v ) } // ... }","title":"Nested TypeString and AWS String"},{"location":"data-handling-and-conversion/#nested-typestring-and-aws-timestamp","text":"To ensure that parsing the read string value does not fail, define nested_attribute_name 's schema.Schema with an appropriate ValidateFunc : \"nested_attribute_name\" : { Type : schema . TypeString , // ... ValidateFunc : validation . IsRFC3339Time , }, To read: func expandStructure ( tfMap map [ string ] interface {}) * service . Structure { // ... if v , ok := tfMap [ \"nested_attribute_name\" ].( string ); ok && v != \"\" { v , _ := time . Parse ( time . RFC3339 , v ) apiObject . NestedAttributeName = aws . Time ( v ) } // ... } To write: func flattenStructure ( apiObject * service . Structure ) map [ string ] interface {} { // ... if v := apiObject . NestedAttributeName ; v != nil { tfMap [ \"nested_attribute_name\" ] = aws . TimeValue ( v ). Format ( time . RFC3339 ) } // ... }","title":"Nested TypeString and AWS Timestamp"},{"location":"data-handling-and-conversion/#further-guidelines","text":"This section includes additional topics related to data design and decision making from the Terraform AWS Provider maintainers.","title":"Further Guidelines"},{"location":"data-handling-and-conversion/#binary-values","text":"Certain resources may need to interact with binary (non UTF-8) data while the Terraform State only supports UTF-8 data. Configurations attempting to pass binary data to an attribute will receive an error from Terraform CLI. These attributes should expect and store the value as a Base64 string while performing any necessary encoding or decoding in the resource logic.","title":"Binary Values"},{"location":"data-handling-and-conversion/#destroy-state-values","text":"During resource destroy operations, only previously applied Terraform State values are available to resource logic. Even if the configuration is updated in a manner where both the resource destroy is triggered (e.g., setting the resource meta-argument count = 0 ) and an attribute value is updated, the resource logic will only have the previously applied data values. Any usage of attribute values during destroy should explicitly note in the resource documentation that the desired value must be applied into the Terraform State before any apply to destroy the resource.","title":"Destroy State Values"},{"location":"data-handling-and-conversion/#hashed-values","text":"Attribute values may be very lengthy or potentially contain Sensitive Values . A potential solution might be to use a hashing algorithm, such as MD5 or SHA256, to convert the value before saving in the Terraform State to reduce its relative size or attempt to obfuscate the value. However, there are a few reasons not to do so: Terraform expects any planned values to match applied values. Ensuring proper handling during the various Terraform operations such as difference planning and Terraform State storage can be a burden. Hashed values are generally unusable in downstream attribute references. If a value is hashed, it cannot be successfully used in another resource or provider configuration that expects the real value. Terraform plan differences are meant to be human readable. If a value is hashed, operators will only see the relatively unhelpful hash differences abc123 -> def456 in plans. Any value hashing implementation will not be accepted. An exception to this guidance is if the remote system explicitly provides a separate hash value in responses, in which a resource can provide a separate attribute with that hashed value.","title":"Hashed Values"},{"location":"data-handling-and-conversion/#sensitive-values","text":"Marking an Attribute in the Terraform Plugin SDK Schema with Sensitive has the following real world implications: All occurrences of the Attribute will have the value hidden in plan difference output. In the context of an Attribute within a Block, all Blocks will hide all values of the Attribute. In Terraform CLI 0.14 (with the provider_sensitive_attrs experiment enabled) and later, any downstream references to the value in other configuration will hide the value in plan difference output. The value is either always hidden or not as the Terraform Plugin SDK does not currently implement conditional support for this functionality. Since Terraform Configurations have no control over the behavior, hiding values from the plan difference can incur a potentially undesirable user experience cost for operators. Given that and especially with the improvements in Terraform CLI 0.14, the Terraform AWS Provider maintainers guiding principles for determining whether an Attribute should be marked as Sensitive is if an Attribute value: Objectively will always contain a credential, password, or other secret material. Operators can have differing opinions on what constitutes secret material and the maintainers will make best effort determinations, if necessary consulting with the HashiCorp Security team. If the Attribute is within a Block, that all occurrences of the Attribute value will objectively contain secret material. Some APIs (and therefore the Terraform AWS Provider resources) implement generic \"setting\" and \"value\" structures which likely will contain a mixture of secret and non-secret material. These will generally not be accepted for marking as Sensitive . If you are unsatisfied with sensitive value handling, the maintainers can recommend ensuring there is a covering issue in the Terraform CLI and/or Terraform Plugin SDK projects explaining the use case. Ultimately, Terraform Plugins including the Terraform AWS Provider cannot implement their own sensitive value abilities if the upstream projects do not implement the appropriate functionality.","title":"Sensitive Values"},{"location":"data-handling-and-conversion/#virtual-attributes","text":"Attributes which only exist within Terraform and not the remote system are typically referred as virtual attributes. Especially in the case of Destroy State Values , these attributes rely on the Implicit State Passthrough behavior of values in Terraform to be available in resource logic. A fictitous example of one of these may be a resource attribute such as a skip_waiting flag, which is used only in the resource logic to skip the typical behavior of waiting for operations to complete. If a virtual attribute has a default value that does not match the Zero Value Mapping for the type, it is recommended to explicitly call d.Set() with the default value in the schema.Resource Importer State function, for example: & schema . Resource { // ... other fields ... Importer : & schema . ResourceImporter { State : func ( d * schema . ResourceData , meta interface {}) ([] * schema . ResourceData , error ) { d . Set ( \"skip_waiting\" , true ) return [] * schema . ResourceData { d }, nil }, }, } This helps prevent an immediate plan difference after resource import unless the configuration has a non-default value.","title":"Virtual Attributes"},{"location":"data-handling-and-conversion/#glossary","text":"Below is a listing of relevant terms and descriptions for data handling and conversion in the Terraform AWS Provider to establish common conventions throughout this documentation. This list is not exhaustive of all concepts of Terraform Plugins, the Terraform AWS Provider, or the data handling that occurs during Terraform runs, but these should generally provide enough context about the topics discussed here. AWS Go SDK : Library that converts Go code into AWS Service API compatible operations and data types. Currently refers to version 1 (v1) available since 2015, however version 2 (v2) will reach general availability status soon. Project . AWS Go SDK Model : AWS Go SDK compatible format of AWS Service API Model. AWS Go SDK Service : AWS Service API Go code generated from the AWS Go SDK Model. Generated by the AWS Go SDK code. AWS Service API : Logical boundary of an AWS service by API endpoint. Some large AWS services may be marketed with many different product names under the same service API (e.g., VPC functionality is part of the EC2 API) and vice-versa where some services may be marketed with one product name but are split into multiple service APIs (e.g., Single Sign-On functionality is split into the Identity Store and SSO Admin APIs). AWS Service API Model : Declarative description of the AWS Service API operations and data types. Generated by the AWS service teams. Used to operate the API and generate API clients such as the various AWS Software Development Kits (SDKs). Terraform Language (\"Configuration\"): Configuration syntax interpreted by the Terraform CLI. An implementation of HCL . Full Documentation . Terraform Plugin Protocol : Description of Terraform Plugin operations and data types. Currently based on the Remote Procedure Call (RPC) library gRPC . Terraform Plugin Go : Low-level library that converts Go code into Terraform Plugin Protocol compatible operations and data types. Not currently implemented in the Terraform AWS Provider. Project . Terraform Plugin SDK : High-level library that converts Go code into Terraform Plugin Protocol compatible operations and data types. Project . Terraform Plugin SDK Schema : Declarative description of types and domain specific behaviors for a Terraform provider, including resources and attributes. Full Documentation . Terraform State : Bindings between objects in a remote system (e.g., an EC2 VPC) and a Terraform configuration (e.g., an aws_vpc resource configuration). Full Documentation . AWS Service API Models use specific terminology to describe data and types: Enumeration : Collection of valid values for a Shape. Operation : An API call. Includes information about input, output, and error Shapes. Shape : Type description. boolean : Boolean value. float : Fractional numeric value. May contain value validation such as maximum or minimum. integer : Whole numeric value. May contain value validation such as maximum or minimum. list : Collection that contains member Shapes. May contain value validation such as maximum or minimum keys. map : Grouping of key Shape to value Shape. May contain value validation such as maximum or minimum keys. string : Sequence of characters. May contain value validation such as an enumeration, regular expression pattern, maximum length, or minimum length. structure : Object that contains member Shapes. May represent an error. timestamp : Date and time value. The Terraform Language uses the following terminology to describe data and types: Attribute (\"Argument\"): Assigns a name to a data value. Block (\"Configuration Block\"): Container type for Attributes or Blocks. null : Virtual value equivalent to the Attribute not being set. Types : Full Documentation . any : Virtual type representing any concrete type in type declarations. bool : Boolean value. list (\"tuple\"): Ordered collection of values. map (\"object\"): Grouping of string keys to values. number : Numeric value. Can be either whole or fractional numbers. set : Unordered collection of values. string : Sequence of characters. Terraform Plugin SDK Schemas use the following terminology to describe data and types: Behaviors : Full Documentation . Sensitive : Whether the value should be hidden from user interface output. StateFunc : Conversion function between the value set by the Terraform Plugin and the value seen by Terraform Plugin SDK (and ultimately the Terraform State). Element : Underylying value type for a collection or grouping Schema. Resource Data : Data representation of a Resource Schema. Translation layer between the Schema and Go code of a Terraform Plugin. In the Terraform Plugin SDK, the ResourceData Go type. Resource Schema : Grouping of Schema that represents a Terraform Resource. Schema : Represents an Attribute or Block. Has a Type and Behavior(s). Types : Full Documentation . TypeBool : Boolean value. TypeFloat : Fractional numeric value. TypeInt : Whole numeric value. TypeList : Ordered collection of values or Blocks. TypeMap : Grouping of key Type to value Type. TypeSet : Unordered collection of values or Blocks. TypeString : Sequence of characters value. Some other terms that may be used: Block Attribute (\"Child Attribute\", \"Nested Attribute\"): Block level Attribute. Expand Function : Function that converts Terraform Plugin SDK data into the equivalent AWS Go SDK type. Flatten Function : Function that converts an AWS Go SDK type into the equivalent Terraform Plugin SDK data. NullableTypeBool : Workaround \"schema type\" created to accept a boolean value that is not configured in addition to true and false. Not implemented in the Terraform Plugin SDK, but uses TypeString (where \"\" represents not configured) and additional validation. NullableTypeFloat : Workaround \"schema type\" created to accept a fractional numeric value that is not configured in addition to 0.0 . Not implemented in the Terraform Plugin SDK, but uses TypeString (where \"\" represents not configured) and additional validation. NullableTypeInt : Workaround \"schema type\" created to accept a whole numeric value that is not configured in addition to 0 . Not implemented in the Terraform Plugin SDK, but uses TypeString (where \"\" represents not configured) and additional validation. Root Attribute : Resource top level Attribute or Block. For additional reference, the Terraform documentation also includes a full glossary of terminology .","title":"Glossary"},{"location":"dependency-updates/","text":"Dependency Updates # Generally dependency updates are handled by maintainers. Go Default Version Update # This project typically upgrades its Go version for development and testing shortly after release to get the latest and greatest Go functionality. Before beginning the update process, ensure that you review the new version release notes to look for any areas of possible friction when updating. Create an issue to cover the update noting down any areas of particular interest or friction. Ensure that the following steps are tracked within the issue and completed within the resulting pull request. Update go version in go.mod Verify make test lint works as expected Verify goreleaser build --snapshot succeeds for all currently supported architectures Verify goenv support for the new version Update docs/development-environment.md Update .go-version Update CHANGELOG.md detailing the update and mention any notes practitioners need to be aware of. See #9992 / #10206 for a recent example. AWS Go SDK Updates # Almost exclusively, github.com/aws/aws-sdk-go updates are additive in nature. It is generally safe to only scan through them before approving and merging. If you have any concerns about any of the service client updates such as suspicious code removals in the update, or deprecations introduced, run the acceptance testing for potentially affected resources before merging. Authentication changes # Occasionally, there will be changes listed in the authentication pieces of the AWS Go SDK codebase, e.g., changes to aws/session . The AWS Go SDK CHANGELOG should include a relevant description of these changes under a heading such as SDK Enhancements or SDK Bug Fixes . If they seem worthy of a callout in the Terraform AWS Provider CHANGELOG , then upon merging we should include a similar message prefixed with the provider subsystem, e.g., * provider: ... . Additionally, if a CHANGELOG addition seemed appropriate, this dependency and version should also be updated in the Terraform S3 Backend, which currently lives in Terraform Core. An example of this can be found with https://github.com/hashicorp/terraform-provider-aws/pull/9305 and https://github.com/hashicorp/terraform/pull/22055. CloudFront changes # CloudFront service client updates have previously caused an issue when a new field introduced in the SDK was not included with Terraform and caused all requests to error (https://github.com/hashicorp/terraform-provider-aws/issues/4091). As a precaution, if you see CloudFront updates, run all the CloudFront resource acceptance testing before merging ( TestAccCloudFront ). golangci-lint Updates # Merge if CI passes. Terraform Plugin SDK Updates # Except for trivial changes, run the full acceptance testing suite against the pull request and verify there are no new or unexpected failures. tfproviderdocs Updates # Merge if CI passes. tfproviderlint Updates # Merge if CI passes. yaml.v2 Updates # Run the acceptance testing pattern, TestAccCloudFormationStack(_dataSource)?_yaml , and merge if passing.","title":"Dependency Updates"},{"location":"dependency-updates/#dependency-updates","text":"Generally dependency updates are handled by maintainers.","title":"Dependency Updates"},{"location":"dependency-updates/#go-default-version-update","text":"This project typically upgrades its Go version for development and testing shortly after release to get the latest and greatest Go functionality. Before beginning the update process, ensure that you review the new version release notes to look for any areas of possible friction when updating. Create an issue to cover the update noting down any areas of particular interest or friction. Ensure that the following steps are tracked within the issue and completed within the resulting pull request. Update go version in go.mod Verify make test lint works as expected Verify goreleaser build --snapshot succeeds for all currently supported architectures Verify goenv support for the new version Update docs/development-environment.md Update .go-version Update CHANGELOG.md detailing the update and mention any notes practitioners need to be aware of. See #9992 / #10206 for a recent example.","title":"Go Default Version Update"},{"location":"dependency-updates/#aws-go-sdk-updates","text":"Almost exclusively, github.com/aws/aws-sdk-go updates are additive in nature. It is generally safe to only scan through them before approving and merging. If you have any concerns about any of the service client updates such as suspicious code removals in the update, or deprecations introduced, run the acceptance testing for potentially affected resources before merging.","title":"AWS Go SDK Updates"},{"location":"dependency-updates/#authentication-changes","text":"Occasionally, there will be changes listed in the authentication pieces of the AWS Go SDK codebase, e.g., changes to aws/session . The AWS Go SDK CHANGELOG should include a relevant description of these changes under a heading such as SDK Enhancements or SDK Bug Fixes . If they seem worthy of a callout in the Terraform AWS Provider CHANGELOG , then upon merging we should include a similar message prefixed with the provider subsystem, e.g., * provider: ... . Additionally, if a CHANGELOG addition seemed appropriate, this dependency and version should also be updated in the Terraform S3 Backend, which currently lives in Terraform Core. An example of this can be found with https://github.com/hashicorp/terraform-provider-aws/pull/9305 and https://github.com/hashicorp/terraform/pull/22055.","title":"Authentication changes"},{"location":"dependency-updates/#cloudfront-changes","text":"CloudFront service client updates have previously caused an issue when a new field introduced in the SDK was not included with Terraform and caused all requests to error (https://github.com/hashicorp/terraform-provider-aws/issues/4091). As a precaution, if you see CloudFront updates, run all the CloudFront resource acceptance testing before merging ( TestAccCloudFront ).","title":"CloudFront changes"},{"location":"dependency-updates/#golangci-lint-updates","text":"Merge if CI passes.","title":"golangci-lint Updates"},{"location":"dependency-updates/#terraform-plugin-sdk-updates","text":"Except for trivial changes, run the full acceptance testing suite against the pull request and verify there are no new or unexpected failures.","title":"Terraform Plugin SDK Updates"},{"location":"dependency-updates/#tfproviderdocs-updates","text":"Merge if CI passes.","title":"tfproviderdocs Updates"},{"location":"dependency-updates/#tfproviderlint-updates","text":"Merge if CI passes.","title":"tfproviderlint Updates"},{"location":"dependency-updates/#yamlv2-updates","text":"Run the acceptance testing pattern, TestAccCloudFormationStack(_dataSource)?_yaml , and merge if passing.","title":"yaml.v2 Updates"},{"location":"development-environment/","text":"Development Environment Setup # Requirements # Terraform 0.12.26+ (to run acceptance tests) Go 1.18.4+ (to build the provider plugin) Quick Start # If you wish to work on the provider, you'll first need Go installed on your machine (please check the requirements before proceeding). Note: This project uses Go Modules making it safe to work with it outside of your existing GOPATH . The instructions that follow assume a directory in your home directory outside of the standard GOPATH (i.e $HOME/development/hashicorp/ ). Clone repository to: $HOME/development/hashicorp/ $ mkdir -p $HOME /development/hashicorp/ ; cd $HOME /development/hashicorp/ $ git clone git@github.com:hashicorp/terraform-provider-aws ... Enter the provider directory and run make tools . This will install the needed tools for the provider. $ make tools To compile the provider, run make build . This will build the provider and put the provider binary in the $GOPATH/bin directory. $ make build ... $ $GOPATH /bin/terraform-provider-aws ... Testing the Provider # In order to test the provider, you can run make test . Note: Make sure no AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY variables are set, and there's no [default] section in the AWS credentials file ~/.aws/credentials . $ make test In order to run the full suite of Acceptance tests, run make testacc . Note: Acceptance tests create real resources, and often cost money to run. Please read Running and Writing Acceptance Tests in the contribution guidelines for more information on usage. $ make testacc Using the Provider # With Terraform v0.14 and later, development overrides for provider developers can be leveraged in order to use the provider built from source. To do this, populate a Terraform CLI configuration file ( ~/.terraformrc for all platforms other than Windows; terraform.rc in the %APPDATA% directory when using Windows) with at least the following options: provider_installation { dev_overrides { \"hashicorp/aws\" = \"[REPLACE WITH GOPATH]/bin\" } direct {} }","title":"Development Environment"},{"location":"development-environment/#development-environment-setup","text":"","title":"Development Environment Setup"},{"location":"development-environment/#requirements","text":"Terraform 0.12.26+ (to run acceptance tests) Go 1.18.4+ (to build the provider plugin)","title":"Requirements"},{"location":"development-environment/#quick-start","text":"If you wish to work on the provider, you'll first need Go installed on your machine (please check the requirements before proceeding). Note: This project uses Go Modules making it safe to work with it outside of your existing GOPATH . The instructions that follow assume a directory in your home directory outside of the standard GOPATH (i.e $HOME/development/hashicorp/ ). Clone repository to: $HOME/development/hashicorp/ $ mkdir -p $HOME /development/hashicorp/ ; cd $HOME /development/hashicorp/ $ git clone git@github.com:hashicorp/terraform-provider-aws ... Enter the provider directory and run make tools . This will install the needed tools for the provider. $ make tools To compile the provider, run make build . This will build the provider and put the provider binary in the $GOPATH/bin directory. $ make build ... $ $GOPATH /bin/terraform-provider-aws ...","title":"Quick Start"},{"location":"development-environment/#testing-the-provider","text":"In order to test the provider, you can run make test . Note: Make sure no AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY variables are set, and there's no [default] section in the AWS credentials file ~/.aws/credentials . $ make test In order to run the full suite of Acceptance tests, run make testacc . Note: Acceptance tests create real resources, and often cost money to run. Please read Running and Writing Acceptance Tests in the contribution guidelines for more information on usage. $ make testacc","title":"Testing the Provider"},{"location":"development-environment/#using-the-provider","text":"With Terraform v0.14 and later, development overrides for provider developers can be leveraged in order to use the provider built from source. To do this, populate a Terraform CLI configuration file ( ~/.terraformrc for all platforms other than Windows; terraform.rc in the %APPDATA% directory when using Windows) with at least the following options: provider_installation { dev_overrides { \"hashicorp/aws\" = \"[REPLACE WITH GOPATH]/bin\" } direct {} }","title":"Using the Provider"},{"location":"documentation-changes/","text":"End User Documentation Changes # All practitioner focused documentation is found in the /website folder of the repository. \u251c\u2500\u2500 website/ \u251c\u2500\u2500 r/ # Documentation for resources \u251c\u2500\u2500 d/ # Documentation for data sources \u251c\u2500\u2500 guides/ # Long format guides for provider level configuration or provider upgrades. \u2514\u2500\u2500 index.html.markdown # Home page and all provider level documentation. \u2514\u2500\u2500 examples/ # Large example configurations For any documentation change please raise a pull request including and adhering to the following: Reasoning for Change : Documentation updates should include an explanation for why the update is needed. If the change is a correction which is an alignment to AWS behavior, please include a link to the AWS Documentation in the PR. Prefer AWS Documentation : Documentation about AWS service features and valid argument values that are likely to update over time should link to AWS service user guides and API references where possible. Large Example Configurations : Example Terraform configuration that includes multiple resource definitions should be added to the repository examples directory instead of an individual resource documentation page. Each directory under examples should be self-contained to call terraform apply without special configuration. Avoid Terraform Configuration Language Features : Individual resource documentation pages and examples should refrain from highlighting particular Terraform configuration language syntax workarounds or features such as variable , local , count , and built-in functions.","title":"Documentation Changes"},{"location":"documentation-changes/#end-user-documentation-changes","text":"All practitioner focused documentation is found in the /website folder of the repository. \u251c\u2500\u2500 website/ \u251c\u2500\u2500 r/ # Documentation for resources \u251c\u2500\u2500 d/ # Documentation for data sources \u251c\u2500\u2500 guides/ # Long format guides for provider level configuration or provider upgrades. \u2514\u2500\u2500 index.html.markdown # Home page and all provider level documentation. \u2514\u2500\u2500 examples/ # Large example configurations For any documentation change please raise a pull request including and adhering to the following: Reasoning for Change : Documentation updates should include an explanation for why the update is needed. If the change is a correction which is an alignment to AWS behavior, please include a link to the AWS Documentation in the PR. Prefer AWS Documentation : Documentation about AWS service features and valid argument values that are likely to update over time should link to AWS service user guides and API references where possible. Large Example Configurations : Example Terraform configuration that includes multiple resource definitions should be added to the repository examples directory instead of an individual resource documentation page. Each directory under examples should be self-contained to call terraform apply without special configuration. Avoid Terraform Configuration Language Features : Individual resource documentation pages and examples should refrain from highlighting particular Terraform configuration language syntax workarounds or features such as variable , local , count , and built-in functions.","title":"End User Documentation Changes"},{"location":"error-handling/","text":"Error Handling # The Terraform AWS Provider codebase bridges the implementation of a Terraform Plugin and an AWS API client to support AWS operations and data types as Terraform Resources. An important aspect of performing resource and remote actions is properly handling those operations, but those operations are not guaranteed to succeed every time. Some common examples include where network connections are unreliable, necessary permissions are not properly setup, incorrect Terraform configurations, or the remote system responds unexpectedly. All these situations lead to an unexpected workflow action that must be surfaced to the Terraform user interface for operators to troubleshoot. This guide is intended to explain and show various Terraform AWS Provider code implementations that are considered best practice for surfacing these issues properly to operators and code maintainers. For further details about how the AWS SDK for Go v1 and the Terraform AWS Provider resource logic handle retryable errors, see the Retries and Waiters documentation . General Guidelines and Helpers # Naming and Check Style # Following typical Go conventions, error variables in the Terraform AWS Provider codebase should be named err , e.g. result , err := strconv . Itoa ( \"oh no!\" ) The code that then checks these errors should prefer if conditionals that usually return (or in the case of looping constructs, break / continue ) early, especially in the case of multiple error checks, e.g. if /* ... something checking err first ... */ { // ... return, break, continue, etc. ... } if err != nil { // ... return, break, continue, etc. ... } // all good! This is in preference of some other styles of error checking, such as switch conditionals without a condition. Wrap Errors # Go implements error wrapping, which means that a deeply nested function call can return a particular error type, while each function up the stack can provide additional error message context without losing the ability to determine the original error. Additional information about this concept can be found on the Go blog entry titled Working with Errors in Go 1.13 . For most use cases in this codebase, this means if code is receiving an error and needs to return it, it should implement fmt.Errorf() and the %w verb, e.g. return fmt . Errorf ( \"adding some additional message: %w\" , err ) This type of error wrapping should be applied to all Terraform resource logic. It should also be applied to any nested functions that contains two or more error conditions (e.g., a function that calls an update API and waits for the update to finish) so practitioners and code maintainers have a clear idea which generated the error. When returning errors in those situations, it is important to only include necessary additional context. Resource logic will typically include the information such as the type of operation and resource identifier (e.g., updating Service Thing (%s): %w ), so these messages can be more terse such as waiting for completion: %w . AWS SDK for Go v1 Errors # The AWS SDK for Go v1 documentation includes a section on handling errors , which is recommended reading. For the purposes of this documentation, the most important concepts with handling these errors are: Each response error (which eventually implements awserr.Error ) has a string error code ( Code ) and string error message ( Message ). When printed as a string, they format as: Code: Message , e.g., InvalidParameterValueException: IAM Role arn:aws:iam::123456789012:role/XXX cannot be assumed by AWS Backup . Error handling is almost exclusively done via those string fields and not other response information, such as HTTP Status Codes. When the error code is non-specific, the error message should also be checked. Unfortunately, AWS APIs generally do not provide documentation or API modeling with the contents of these messages and often the Terraform AWS Provider code must rely on substring matching. Not all errors are returned in the response error from an AWS API operation. This is service- and sometimes API-call-specific. For example, the EC2 DeleteVpcEndpoints API call can return a \"successful\" response (in terms of no response error) but include information in an Unsuccessful field in the response body. When working with AWS SDK for Go v1 errors, it is preferred to use the helpers outlined below and use the %w format verb. Code should generally avoid type assertions with the underlying awserr.Error type or calling its Code() , Error() , Message() , or String() receiver methods. Using the %v , %#v , or %+v format verbs generally provides extraneous information that is not helpful to operators or code maintainers. AWS SDK for Go Error Helpers # To simplify operations with AWS SDK for Go error types, the following helpers are available via the github.com/hashicorp/aws-sdk-go-base/v2/awsv1shim/v2/tfawserr Go package: tfawserr.ErrCodeEquals(err, \"Code\") : Preferred when the error code is specific enough for the check condition. For example, a ResourceNotFoundError code provides enough information that the requested API resource identifier/Amazon Resource Name does not exist. tfawserr.ErrMessageContains(err, \"Code\", \"MessageContains\") : Does simple substring matching for the error message. The recommendation for error message checking is to be just specific enough to capture the anticipated issue, but not include too much matching as the AWS API can change over time without notice. The maintainers have observed changes in wording and capitalization cause unexpected issues in the past. For example, given this error code and message: InvalidParameterValueException: IAM Role arn:aws:iam::123456789012:role/XXX cannot be assumed by AWS Backup An error check for this might be: if tfawserr . ErrMessageContains ( err , backup . ErrCodeInvalidParameterValueException , \"cannot be assumed\" ) { /* ... */ } The Amazon Resource Name in the error message will be different for every environment and does not add value to the check. The AWS Backup suffix is also extraneous and could change should the service ever rename. Use AWS SDK for Go v1 Error Code Constants # Each AWS SDK for Go v1 service API typically implements common error codes, which get exported as public constants in the SDK. In the AWS SDK for Go v1 API Reference , these can be found in each of the service packages under the Constants section (typically named ErrCode{ExceptionName} ). If an AWS SDK for Go service API is missing an error code constant, an AWS Support case should be submitted and a new constant can be added to internal/service/{SERVICE}/errors.go file (created if not present), e.g. const ( ErrCodeInvalidParameterException = \"InvalidParameterException\" ) Then referencing code can use it via: // imports tf { SERVICE } \"github.com/hashicorp/terraform-provider-aws/internal/service/{SERVICE}\" // logic tfawserr . ErrCodeEquals ( err , tf { SERVICE }. ErrCodeInvalidParameterException ) e.g. // imports tfec2 \"github.com/hashicorp/terraform-provider-aws/internal/service/ec2\" // logic tfawserr . ErrCodeEquals ( err , tfec2 . ErrCodeInvalidParameterException ) Terraform Plugin SDK Types and Helpers # The Terraform Plugin SDK includes some error types which are used in certain operations and typically preferred over implementing new types: resource.NotFoundError resource.TimeoutError : Returned from resource.Retry() , resource.RetryContext() , (resource.StateChangeConf).WaitForState() , and (resource.StateChangeConf).WaitForStateContext() The Terraform AWS Provider codebase implements some additional helpers for working with these in the github.com/hashicorp/terraform-provider-aws/internal/tfresource package: tfresource.NotFound(err) : Returns true if the error is a resource.NotFoundError . tfresource.TimedOut(err) : Returns true if the error is a resource.TimeoutError and contains no LastError . This typically signifies that the retry logic was never signaled for a retry, which can happen when AWS API operations are automatically retrying before returning. Resource Lifecycle Guidelines # Terraform CLI and the Terraform Plugin SDK have certain expectations and automatic behaviors depending on the lifecycle operation of a resource. This section highlights some common issues that can occur and their expected resolution. Resource Creation # Invoked in the resource via the schema.Resource type Create / CreateContext function. d.IsNewResource() Checks # During resource creation, Terraform CLI expects either a properly applied state for the new resource or an error. To signal proper resource existence, the Terraform Plugin SDK uses an underlying resource identifier (set via d.SetId(/* some value */) ). If for some reason the resource creation is returned without an error, but also without the resource identifier being set, Terraform CLI will return an error such as: Error: Provider produced inconsistent result after apply When applying changes to aws_sns_topic_subscription.sqs, provider \"registry.terraform.io/hashicorp/aws\" produced an unexpected new value: Root resource was present, but now absent. This is a bug in the provider, which should be reported in the provider ' s own issue tracker. A typical pattern in resource implementations in the Create / CreateContext function is to return the Read / ReadContext function at the end to fill in the Terraform State for all attributes. Another typical pattern in resource implementations in the Read / ReadContext function is to remove the resource from the Terraform State if the remote system returns an error or status that indicates the remote resource no longer exists by explicitly calling d.SetId(\"\") and returning no error. If the remote system is not strongly read-after-write consistent (eventually consistent), this means the resource creation can return no error and also return no resource state. To prevent this type of Terraform CLI error, the resource implementation should also check against d.IsNewResource() before removing from the Terraform State and returning no error. If that check is true , then remote operation error (or one synthesized from the non-existent status) should be returned instead. While adding this check will not fix the resource implementation to handle the eventually consistent nature of the remote system, the error being returned will be less opaque for operators and code maintainers to troubleshoot. In the Terraform AWS Provider, an initial fix for the Terraform CLI error will typically look like: func resourceServiceThingCreate ( d * schema . ResourceData , meta interface {}) error { /* ... */ return resourceServiceThingRead ( d , meta ) } func resourceServiceThingRead ( d * schema . ResourceData , meta interface {}) error { /* ... */ output , err := conn . DescribeServiceThing ( input ) if ! d . IsNewResource () && tfawserr . ErrCodeEquals ( err , \"ResourceNotFoundException\" ) { log . Printf ( \"[WARN] {Service} {Thing} (%s) not found, removing from state\" , d . Id ()) d . SetId ( \"\" ) return nil } if err != nil { return fmt . Errorf ( \"reading {Service} {Thing} (%s): %w\" , d . Id (), err ) } /* ... */ } If the remote system is not strongly read-after-write consistent, see the Retries and Waiters documentation on Resource Lifecycle Retries for how to prevent consistency-type errors. Creation Error Message Context # Returning errors during creation should include additional messaging about the location or cause of the error for operators and code maintainers by wrapping with fmt.Errorf() : if err != nil { return fmt . Errorf ( \"creating {SERVICE} {THING}: %w\" , err ) } e.g. if err != nil { return fmt . Errorf ( \"creating EC2 VPC: %w\" , err ) } Code that also uses waiters or other operations that return errors should follow a similar pattern, including the resource identifier since it has typically been set before this execution: if _ , err := VpcAvailable ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for EC2 VPC (%s) availability: %w\" , d . Id (), err ) } Resource Deletion # Invoked in the resource via the schema.Resource type Delete / DeleteContext function. Resource Already Deleted # A typical pattern for resource deletion is to immediately perform the remote system deletion operation without checking existence. This is generally acceptable as operators are encouraged to always refresh their Terraform State prior to performing changes. However in certain scenarios, such as external systems modifying the remote system prior to the Terraform execution, it is certainly still possible that the remote system will return an error signifying that remote resource does not exist. In these cases, resources should implement logic that catches the error and returns no error. NOTE: The Terraform Plugin SDK automatically handles the equivalent of d.SetId(\"\") on deletion, so it is not necessary to include it. For example in the Terraform AWS Provider: func resourceServiceThingDelete ( d * schema . ResourceData , meta interface {}) error { /* ... */ output , err := conn . DeleteServiceThing ( input ) if tfawserr . ErrCodeEquals ( err , \"ResourceNotFoundException\" ) { return nil } if err != nil { return fmt . Errorf ( \"deleting {Service} {Thing} (%s): %w\" , d . Id (), err ) } /* ... */ } Deletion Error Message Context # Returning errors during deletion should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers by wrapping with fmt.Errorf() : if err != nil { return fmt . Errorf ( \"deleting {SERVICE} {THING} (%s): %w\" , d . Id (), err ) } e.g. if err != nil { return fmt . Errorf ( \"deleting EC2 VPC (%s): %w\" , d . Id (), err ) } Code that also uses waiters or other operations that return errors should follow a similar pattern: if _ , err := VpcDeleted ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for EC2 VPC (%s) deletion: %w\" , d . Id (), err ) } Resource Read # Invoked in the resource via the schema.Resource type Read / ReadContext function. Singular Data Source Errors # A data source which is expected to return Terraform State about a single remote resource is commonly referred to as a \"singular\" data source. Implementation-wise, it may use any available describe or listing functionality from the remote system to retrieve the information. In addition to any remote operation and other data handling errors that should be returned, these two additional cases should be covered: Returning an error when zero results are found. Returning an error when multiple results are found. For remote operations that are designed to return an error when the remote resource is not found, this error is typically just passed through similar to other remote operation errors. For remote operations that are designed to return a successful result whether there is zero, one, or multiple multiple results the error must be generated. For example in pseudo-code: output , err := conn . ListServiceThings ( input ) if err != nil { return fmt . Errorf ( \"listing {Service} {Thing}s: %w\" , err ) } if output == nil || len ( output . Results ) == 0 { return fmt . Errorf ( \"no {Service} {Thing} found matching criteria; try different search\" ) } if len ( output . Results ) > 1 { return fmt . Errorf ( \"multiple {Service} {Thing} found matching criteria; try different search\" ) } Plural Data Source Errors # An emergent concept is a data source that returns multiple results, acting similar to any available listing functionality available from the remote system. These types of data sources should return no error if zero results are returned and no error if multiple results are found. Remote operation and other data handling errors should still be returned. Read Error Message Context # Returning errors during read should include the resource identifier (for managed resources) and additional messaging about the location or cause of the error for operators and code maintainers by wrapping with fmt.Errorf() : if err != nil { return fmt . Errorf ( \"reading {SERVICE} {THING} (%s): %w\" , d . Id (), err ) } e.g. if err != nil { return fmt . Errorf ( \"reading EC2 VPC (%s): %w\" , d . Id (), err ) } Resource Update # Invoked in the resource via the schema.Resource type Update / UpdateContext function. Update Error Message Context # Returning errors during update should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers by wrapping with fmt.Errorf() : if err != nil { return fmt . Errorf ( \"updating {SERVICE} {THING} (%s): %w\" , d . Id (), err ) } e.g. if err != nil { return fmt . Errorf ( \"updating EC2 VPC (%s): %w\" , d . Id (), err ) } Code that also uses waiters or other operations that return errors should follow a similar pattern: if _ , err := VpcAvailable ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for EC2 VPC (%s) update: %w\" , d . Id (), err ) }","title":"Error Handling"},{"location":"error-handling/#error-handling","text":"The Terraform AWS Provider codebase bridges the implementation of a Terraform Plugin and an AWS API client to support AWS operations and data types as Terraform Resources. An important aspect of performing resource and remote actions is properly handling those operations, but those operations are not guaranteed to succeed every time. Some common examples include where network connections are unreliable, necessary permissions are not properly setup, incorrect Terraform configurations, or the remote system responds unexpectedly. All these situations lead to an unexpected workflow action that must be surfaced to the Terraform user interface for operators to troubleshoot. This guide is intended to explain and show various Terraform AWS Provider code implementations that are considered best practice for surfacing these issues properly to operators and code maintainers. For further details about how the AWS SDK for Go v1 and the Terraform AWS Provider resource logic handle retryable errors, see the Retries and Waiters documentation .","title":"Error Handling"},{"location":"error-handling/#general-guidelines-and-helpers","text":"","title":"General Guidelines and Helpers"},{"location":"error-handling/#naming-and-check-style","text":"Following typical Go conventions, error variables in the Terraform AWS Provider codebase should be named err , e.g. result , err := strconv . Itoa ( \"oh no!\" ) The code that then checks these errors should prefer if conditionals that usually return (or in the case of looping constructs, break / continue ) early, especially in the case of multiple error checks, e.g. if /* ... something checking err first ... */ { // ... return, break, continue, etc. ... } if err != nil { // ... return, break, continue, etc. ... } // all good! This is in preference of some other styles of error checking, such as switch conditionals without a condition.","title":"Naming and Check Style"},{"location":"error-handling/#wrap-errors","text":"Go implements error wrapping, which means that a deeply nested function call can return a particular error type, while each function up the stack can provide additional error message context without losing the ability to determine the original error. Additional information about this concept can be found on the Go blog entry titled Working with Errors in Go 1.13 . For most use cases in this codebase, this means if code is receiving an error and needs to return it, it should implement fmt.Errorf() and the %w verb, e.g. return fmt . Errorf ( \"adding some additional message: %w\" , err ) This type of error wrapping should be applied to all Terraform resource logic. It should also be applied to any nested functions that contains two or more error conditions (e.g., a function that calls an update API and waits for the update to finish) so practitioners and code maintainers have a clear idea which generated the error. When returning errors in those situations, it is important to only include necessary additional context. Resource logic will typically include the information such as the type of operation and resource identifier (e.g., updating Service Thing (%s): %w ), so these messages can be more terse such as waiting for completion: %w .","title":"Wrap Errors"},{"location":"error-handling/#aws-sdk-for-go-v1-errors","text":"The AWS SDK for Go v1 documentation includes a section on handling errors , which is recommended reading. For the purposes of this documentation, the most important concepts with handling these errors are: Each response error (which eventually implements awserr.Error ) has a string error code ( Code ) and string error message ( Message ). When printed as a string, they format as: Code: Message , e.g., InvalidParameterValueException: IAM Role arn:aws:iam::123456789012:role/XXX cannot be assumed by AWS Backup . Error handling is almost exclusively done via those string fields and not other response information, such as HTTP Status Codes. When the error code is non-specific, the error message should also be checked. Unfortunately, AWS APIs generally do not provide documentation or API modeling with the contents of these messages and often the Terraform AWS Provider code must rely on substring matching. Not all errors are returned in the response error from an AWS API operation. This is service- and sometimes API-call-specific. For example, the EC2 DeleteVpcEndpoints API call can return a \"successful\" response (in terms of no response error) but include information in an Unsuccessful field in the response body. When working with AWS SDK for Go v1 errors, it is preferred to use the helpers outlined below and use the %w format verb. Code should generally avoid type assertions with the underlying awserr.Error type or calling its Code() , Error() , Message() , or String() receiver methods. Using the %v , %#v , or %+v format verbs generally provides extraneous information that is not helpful to operators or code maintainers.","title":"AWS SDK for Go v1 Errors"},{"location":"error-handling/#aws-sdk-for-go-error-helpers","text":"To simplify operations with AWS SDK for Go error types, the following helpers are available via the github.com/hashicorp/aws-sdk-go-base/v2/awsv1shim/v2/tfawserr Go package: tfawserr.ErrCodeEquals(err, \"Code\") : Preferred when the error code is specific enough for the check condition. For example, a ResourceNotFoundError code provides enough information that the requested API resource identifier/Amazon Resource Name does not exist. tfawserr.ErrMessageContains(err, \"Code\", \"MessageContains\") : Does simple substring matching for the error message. The recommendation for error message checking is to be just specific enough to capture the anticipated issue, but not include too much matching as the AWS API can change over time without notice. The maintainers have observed changes in wording and capitalization cause unexpected issues in the past. For example, given this error code and message: InvalidParameterValueException: IAM Role arn:aws:iam::123456789012:role/XXX cannot be assumed by AWS Backup An error check for this might be: if tfawserr . ErrMessageContains ( err , backup . ErrCodeInvalidParameterValueException , \"cannot be assumed\" ) { /* ... */ } The Amazon Resource Name in the error message will be different for every environment and does not add value to the check. The AWS Backup suffix is also extraneous and could change should the service ever rename.","title":"AWS SDK for Go Error Helpers"},{"location":"error-handling/#use-aws-sdk-for-go-v1-error-code-constants","text":"Each AWS SDK for Go v1 service API typically implements common error codes, which get exported as public constants in the SDK. In the AWS SDK for Go v1 API Reference , these can be found in each of the service packages under the Constants section (typically named ErrCode{ExceptionName} ). If an AWS SDK for Go service API is missing an error code constant, an AWS Support case should be submitted and a new constant can be added to internal/service/{SERVICE}/errors.go file (created if not present), e.g. const ( ErrCodeInvalidParameterException = \"InvalidParameterException\" ) Then referencing code can use it via: // imports tf { SERVICE } \"github.com/hashicorp/terraform-provider-aws/internal/service/{SERVICE}\" // logic tfawserr . ErrCodeEquals ( err , tf { SERVICE }. ErrCodeInvalidParameterException ) e.g. // imports tfec2 \"github.com/hashicorp/terraform-provider-aws/internal/service/ec2\" // logic tfawserr . ErrCodeEquals ( err , tfec2 . ErrCodeInvalidParameterException )","title":"Use AWS SDK for Go v1 Error Code Constants"},{"location":"error-handling/#terraform-plugin-sdk-types-and-helpers","text":"The Terraform Plugin SDK includes some error types which are used in certain operations and typically preferred over implementing new types: resource.NotFoundError resource.TimeoutError : Returned from resource.Retry() , resource.RetryContext() , (resource.StateChangeConf).WaitForState() , and (resource.StateChangeConf).WaitForStateContext() The Terraform AWS Provider codebase implements some additional helpers for working with these in the github.com/hashicorp/terraform-provider-aws/internal/tfresource package: tfresource.NotFound(err) : Returns true if the error is a resource.NotFoundError . tfresource.TimedOut(err) : Returns true if the error is a resource.TimeoutError and contains no LastError . This typically signifies that the retry logic was never signaled for a retry, which can happen when AWS API operations are automatically retrying before returning.","title":"Terraform Plugin SDK Types and Helpers"},{"location":"error-handling/#resource-lifecycle-guidelines","text":"Terraform CLI and the Terraform Plugin SDK have certain expectations and automatic behaviors depending on the lifecycle operation of a resource. This section highlights some common issues that can occur and their expected resolution.","title":"Resource Lifecycle Guidelines"},{"location":"error-handling/#resource-creation","text":"Invoked in the resource via the schema.Resource type Create / CreateContext function.","title":"Resource Creation"},{"location":"error-handling/#disnewresource-checks","text":"During resource creation, Terraform CLI expects either a properly applied state for the new resource or an error. To signal proper resource existence, the Terraform Plugin SDK uses an underlying resource identifier (set via d.SetId(/* some value */) ). If for some reason the resource creation is returned without an error, but also without the resource identifier being set, Terraform CLI will return an error such as: Error: Provider produced inconsistent result after apply When applying changes to aws_sns_topic_subscription.sqs, provider \"registry.terraform.io/hashicorp/aws\" produced an unexpected new value: Root resource was present, but now absent. This is a bug in the provider, which should be reported in the provider ' s own issue tracker. A typical pattern in resource implementations in the Create / CreateContext function is to return the Read / ReadContext function at the end to fill in the Terraform State for all attributes. Another typical pattern in resource implementations in the Read / ReadContext function is to remove the resource from the Terraform State if the remote system returns an error or status that indicates the remote resource no longer exists by explicitly calling d.SetId(\"\") and returning no error. If the remote system is not strongly read-after-write consistent (eventually consistent), this means the resource creation can return no error and also return no resource state. To prevent this type of Terraform CLI error, the resource implementation should also check against d.IsNewResource() before removing from the Terraform State and returning no error. If that check is true , then remote operation error (or one synthesized from the non-existent status) should be returned instead. While adding this check will not fix the resource implementation to handle the eventually consistent nature of the remote system, the error being returned will be less opaque for operators and code maintainers to troubleshoot. In the Terraform AWS Provider, an initial fix for the Terraform CLI error will typically look like: func resourceServiceThingCreate ( d * schema . ResourceData , meta interface {}) error { /* ... */ return resourceServiceThingRead ( d , meta ) } func resourceServiceThingRead ( d * schema . ResourceData , meta interface {}) error { /* ... */ output , err := conn . DescribeServiceThing ( input ) if ! d . IsNewResource () && tfawserr . ErrCodeEquals ( err , \"ResourceNotFoundException\" ) { log . Printf ( \"[WARN] {Service} {Thing} (%s) not found, removing from state\" , d . Id ()) d . SetId ( \"\" ) return nil } if err != nil { return fmt . Errorf ( \"reading {Service} {Thing} (%s): %w\" , d . Id (), err ) } /* ... */ } If the remote system is not strongly read-after-write consistent, see the Retries and Waiters documentation on Resource Lifecycle Retries for how to prevent consistency-type errors.","title":"d.IsNewResource() Checks"},{"location":"error-handling/#creation-error-message-context","text":"Returning errors during creation should include additional messaging about the location or cause of the error for operators and code maintainers by wrapping with fmt.Errorf() : if err != nil { return fmt . Errorf ( \"creating {SERVICE} {THING}: %w\" , err ) } e.g. if err != nil { return fmt . Errorf ( \"creating EC2 VPC: %w\" , err ) } Code that also uses waiters or other operations that return errors should follow a similar pattern, including the resource identifier since it has typically been set before this execution: if _ , err := VpcAvailable ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for EC2 VPC (%s) availability: %w\" , d . Id (), err ) }","title":"Creation Error Message Context"},{"location":"error-handling/#resource-deletion","text":"Invoked in the resource via the schema.Resource type Delete / DeleteContext function.","title":"Resource Deletion"},{"location":"error-handling/#resource-already-deleted","text":"A typical pattern for resource deletion is to immediately perform the remote system deletion operation without checking existence. This is generally acceptable as operators are encouraged to always refresh their Terraform State prior to performing changes. However in certain scenarios, such as external systems modifying the remote system prior to the Terraform execution, it is certainly still possible that the remote system will return an error signifying that remote resource does not exist. In these cases, resources should implement logic that catches the error and returns no error. NOTE: The Terraform Plugin SDK automatically handles the equivalent of d.SetId(\"\") on deletion, so it is not necessary to include it. For example in the Terraform AWS Provider: func resourceServiceThingDelete ( d * schema . ResourceData , meta interface {}) error { /* ... */ output , err := conn . DeleteServiceThing ( input ) if tfawserr . ErrCodeEquals ( err , \"ResourceNotFoundException\" ) { return nil } if err != nil { return fmt . Errorf ( \"deleting {Service} {Thing} (%s): %w\" , d . Id (), err ) } /* ... */ }","title":"Resource Already Deleted"},{"location":"error-handling/#deletion-error-message-context","text":"Returning errors during deletion should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers by wrapping with fmt.Errorf() : if err != nil { return fmt . Errorf ( \"deleting {SERVICE} {THING} (%s): %w\" , d . Id (), err ) } e.g. if err != nil { return fmt . Errorf ( \"deleting EC2 VPC (%s): %w\" , d . Id (), err ) } Code that also uses waiters or other operations that return errors should follow a similar pattern: if _ , err := VpcDeleted ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for EC2 VPC (%s) deletion: %w\" , d . Id (), err ) }","title":"Deletion Error Message Context"},{"location":"error-handling/#resource-read","text":"Invoked in the resource via the schema.Resource type Read / ReadContext function.","title":"Resource Read"},{"location":"error-handling/#singular-data-source-errors","text":"A data source which is expected to return Terraform State about a single remote resource is commonly referred to as a \"singular\" data source. Implementation-wise, it may use any available describe or listing functionality from the remote system to retrieve the information. In addition to any remote operation and other data handling errors that should be returned, these two additional cases should be covered: Returning an error when zero results are found. Returning an error when multiple results are found. For remote operations that are designed to return an error when the remote resource is not found, this error is typically just passed through similar to other remote operation errors. For remote operations that are designed to return a successful result whether there is zero, one, or multiple multiple results the error must be generated. For example in pseudo-code: output , err := conn . ListServiceThings ( input ) if err != nil { return fmt . Errorf ( \"listing {Service} {Thing}s: %w\" , err ) } if output == nil || len ( output . Results ) == 0 { return fmt . Errorf ( \"no {Service} {Thing} found matching criteria; try different search\" ) } if len ( output . Results ) > 1 { return fmt . Errorf ( \"multiple {Service} {Thing} found matching criteria; try different search\" ) }","title":"Singular Data Source Errors"},{"location":"error-handling/#plural-data-source-errors","text":"An emergent concept is a data source that returns multiple results, acting similar to any available listing functionality available from the remote system. These types of data sources should return no error if zero results are returned and no error if multiple results are found. Remote operation and other data handling errors should still be returned.","title":"Plural Data Source Errors"},{"location":"error-handling/#read-error-message-context","text":"Returning errors during read should include the resource identifier (for managed resources) and additional messaging about the location or cause of the error for operators and code maintainers by wrapping with fmt.Errorf() : if err != nil { return fmt . Errorf ( \"reading {SERVICE} {THING} (%s): %w\" , d . Id (), err ) } e.g. if err != nil { return fmt . Errorf ( \"reading EC2 VPC (%s): %w\" , d . Id (), err ) }","title":"Read Error Message Context"},{"location":"error-handling/#resource-update","text":"Invoked in the resource via the schema.Resource type Update / UpdateContext function.","title":"Resource Update"},{"location":"error-handling/#update-error-message-context","text":"Returning errors during update should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers by wrapping with fmt.Errorf() : if err != nil { return fmt . Errorf ( \"updating {SERVICE} {THING} (%s): %w\" , d . Id (), err ) } e.g. if err != nil { return fmt . Errorf ( \"updating EC2 VPC (%s): %w\" , d . Id (), err ) } Code that also uses waiters or other operations that return errors should follow a similar pattern: if _ , err := VpcAvailable ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for EC2 VPC (%s) update: %w\" , d . Id (), err ) }","title":"Update Error Message Context"},{"location":"faq/","text":"Frequently Asked Questions # Who are the maintainers? # The HashiCorp Terraform AWS provider team is : Marc Cosentino, Product Manager - GitHub @marcosentino Simon Davis, Engineering Manager - GitHub @breathingdust Justin Retzolk, Technical Community Manager - GitHub @justinretzolk Adrian Johnson, Engineer - GitHub @johnsonaj Dirk Avery, Engineer - GitHub @YakDriver Graham Davison, Engineer - GitHub @gdavison Jared Baker, Engineer - GitHub @jar-b Kerim Satirli, Developer Advocate - GitHub @ksatirli Kit Ewbank, Engineer - GitHub @ewbankkit Why isn\u2019t my PR merged yet? # Unfortunately, due to the volume of issues and new pull requests we receive, we are unable to give each one the full attention that we would like. We always focus on the contributions that provide the greatest value to the most community members. For more information on how we prioritize pull requests, see the prioritization guide . How do you decide what gets merged for each release? # We have a large backlog of pull requests to get through and the team are moving through them as quick as we can. All pull requests must be reviewed by a HashiCorp engineer before inclusion. This is to ensure that the design of the addition fits with what provider users have come to expect, and to ensure that testing and best practices are adhered to. This is particularly important for such a large codebase, to ensure that we sustain its maintainability as its grows. The number one factor we look at when deciding what issues to look at are your \ud83d\udc4d reactions to the original issue/PR description as these can be easily discovered . Comments that further explain desired use cases or poor user experience are also heavily factored. The items with the most support are always on our radar, and we commit to keep the community updated on their status and potential timelines. We publish a roadmap every quarter which describes major themes or specific product areas of focus. What is excluded from the public roadmap is work performed under NDA with AWS on new services, and any ad-hoc work we pick up during the quarter. This ad-hoc work can be responding to bugs, gardening day activity, customer prioritization, and technical debt items. We also are investing time to improve the contributing experience by improving documentation, adding more linter coverage to ensure that incoming PR's can be in as good shape as possible. This will allow us to get through them quicker. My PR hasn't been merged and it now has merge conflicts/failed checks, should I keep it up to date? # We realize that sometimes pull requests sit for a considerable amount of time without being addressed. During this time period they may accumulate merge conflicts and failed linter checks as the provider codebase moves forward. As maintainers we have no expectation that you keep your PR up to date, these issues will be addressed at review time most often by the maintainers themselves. Obviously we would hope that your PR is mergeable when first raised! The mergeability of the PR does not affect its prioritization for review. How often do you release? # We release weekly on Thursday. We release often to ensure we can bring value to the community at a frequent cadence and to ensure we are in a good place to react to AWS region launches and service announcements. Backward Compatibility Promise # Our policy is described on the Terraform website here . While we do our best to prevent breaking changes until major version releases of the provider, it is generally recommended to pin the provider version in your configuration . Due to the constant release pace of AWS and the relatively infrequent major version releases of the provider, there can be cases where a minor version update may contain unexpected changes depending on your configuration or environment. These may include items such as a resource requiring additional IAM permissions to support newer functionality. We typically base these decisions on a pragmatic compromise between introducing a relatively minor one-time inconvenience for a subset of the community versus better overall user experience for the entire community. Once a major release is published, will new features and fixes be backported to previous versions? # Generally new features and fixes will only be added to the most recent major version. Due to the high touch nature of provider development and the extensive regression testing required to ensure stability, maintaining multiple versions of the provider is not sustainable at this time. An exception to this could be a discovered security vulnerability for which backporting may be the most reasonable course of action. These would be reviewed on a case by case basis. AWS just announced a new region, when will I see it in the provider. # Normally pretty quickly. We usually see the region appear within the aws-go-sdk within a couple days of the announcement. Depending on when it lands, we can often get it out within the current or following weekly release. Comparatively, adding support for a new region in the S3 backend can take a little longer, as it is shipped as part of Terraform Core and not via the AWS Provider. Please note that this new region requires a manual process to enable in your account. Once enabled in the console, it takes a few minutes for everything to work properly. If the region is not enabled properly, or the enablement process is still in progress, you may receive errors like these: $ terraform apply Error: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid. status code: 403, request id: 142f947b-b2c3-11e9-9959-c11ab17bcc63 on main.tf line 1, in provider \"aws\": 1: provider \"aws\" { To use this new region before support has been added to the Terraform AWS Provider, you can disable the provider's automatic region validation via: provider \"aws\" { # ... potentially other configuration ... region = \"af-south-1\" skip_region_validation = true } How can I help? # Great question, if you have contributed before check out issues with the help-wanted label. These are normally enhancement issues that will have a great impact, but the maintainers are unable to develop them in the near future. If you are just getting started, take a look at issues with the good-first-issue label. Items with these labels will always be given priority for response. Check out the Contributing Guide for additional information. How can I become a maintainer? # This is an area under active research. Stay tuned!","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#who-are-the-maintainers","text":"The HashiCorp Terraform AWS provider team is : Marc Cosentino, Product Manager - GitHub @marcosentino Simon Davis, Engineering Manager - GitHub @breathingdust Justin Retzolk, Technical Community Manager - GitHub @justinretzolk Adrian Johnson, Engineer - GitHub @johnsonaj Dirk Avery, Engineer - GitHub @YakDriver Graham Davison, Engineer - GitHub @gdavison Jared Baker, Engineer - GitHub @jar-b Kerim Satirli, Developer Advocate - GitHub @ksatirli Kit Ewbank, Engineer - GitHub @ewbankkit","title":"Who are the maintainers?"},{"location":"faq/#why-isnt-my-pr-merged-yet","text":"Unfortunately, due to the volume of issues and new pull requests we receive, we are unable to give each one the full attention that we would like. We always focus on the contributions that provide the greatest value to the most community members. For more information on how we prioritize pull requests, see the prioritization guide .","title":"Why isn\u2019t my PR merged yet?"},{"location":"faq/#how-do-you-decide-what-gets-merged-for-each-release","text":"We have a large backlog of pull requests to get through and the team are moving through them as quick as we can. All pull requests must be reviewed by a HashiCorp engineer before inclusion. This is to ensure that the design of the addition fits with what provider users have come to expect, and to ensure that testing and best practices are adhered to. This is particularly important for such a large codebase, to ensure that we sustain its maintainability as its grows. The number one factor we look at when deciding what issues to look at are your \ud83d\udc4d reactions to the original issue/PR description as these can be easily discovered . Comments that further explain desired use cases or poor user experience are also heavily factored. The items with the most support are always on our radar, and we commit to keep the community updated on their status and potential timelines. We publish a roadmap every quarter which describes major themes or specific product areas of focus. What is excluded from the public roadmap is work performed under NDA with AWS on new services, and any ad-hoc work we pick up during the quarter. This ad-hoc work can be responding to bugs, gardening day activity, customer prioritization, and technical debt items. We also are investing time to improve the contributing experience by improving documentation, adding more linter coverage to ensure that incoming PR's can be in as good shape as possible. This will allow us to get through them quicker.","title":"How do you decide what gets merged for each release?"},{"location":"faq/#my-pr-hasnt-been-merged-and-it-now-has-merge-conflictsfailed-checks-should-i-keep-it-up-to-date","text":"We realize that sometimes pull requests sit for a considerable amount of time without being addressed. During this time period they may accumulate merge conflicts and failed linter checks as the provider codebase moves forward. As maintainers we have no expectation that you keep your PR up to date, these issues will be addressed at review time most often by the maintainers themselves. Obviously we would hope that your PR is mergeable when first raised! The mergeability of the PR does not affect its prioritization for review.","title":"My PR hasn't been merged and it now has merge conflicts/failed checks, should I keep it up to date?"},{"location":"faq/#how-often-do-you-release","text":"We release weekly on Thursday. We release often to ensure we can bring value to the community at a frequent cadence and to ensure we are in a good place to react to AWS region launches and service announcements.","title":"How often do you release?"},{"location":"faq/#backward-compatibility-promise","text":"Our policy is described on the Terraform website here . While we do our best to prevent breaking changes until major version releases of the provider, it is generally recommended to pin the provider version in your configuration . Due to the constant release pace of AWS and the relatively infrequent major version releases of the provider, there can be cases where a minor version update may contain unexpected changes depending on your configuration or environment. These may include items such as a resource requiring additional IAM permissions to support newer functionality. We typically base these decisions on a pragmatic compromise between introducing a relatively minor one-time inconvenience for a subset of the community versus better overall user experience for the entire community.","title":"Backward Compatibility Promise"},{"location":"faq/#once-a-major-release-is-published-will-new-features-and-fixes-be-backported-to-previous-versions","text":"Generally new features and fixes will only be added to the most recent major version. Due to the high touch nature of provider development and the extensive regression testing required to ensure stability, maintaining multiple versions of the provider is not sustainable at this time. An exception to this could be a discovered security vulnerability for which backporting may be the most reasonable course of action. These would be reviewed on a case by case basis.","title":"Once a major release is published, will new features and fixes be backported to previous versions?"},{"location":"faq/#aws-just-announced-a-new-region-when-will-i-see-it-in-the-provider","text":"Normally pretty quickly. We usually see the region appear within the aws-go-sdk within a couple days of the announcement. Depending on when it lands, we can often get it out within the current or following weekly release. Comparatively, adding support for a new region in the S3 backend can take a little longer, as it is shipped as part of Terraform Core and not via the AWS Provider. Please note that this new region requires a manual process to enable in your account. Once enabled in the console, it takes a few minutes for everything to work properly. If the region is not enabled properly, or the enablement process is still in progress, you may receive errors like these: $ terraform apply Error: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid. status code: 403, request id: 142f947b-b2c3-11e9-9959-c11ab17bcc63 on main.tf line 1, in provider \"aws\": 1: provider \"aws\" { To use this new region before support has been added to the Terraform AWS Provider, you can disable the provider's automatic region validation via: provider \"aws\" { # ... potentially other configuration ... region = \"af-south-1\" skip_region_validation = true }","title":"AWS just announced a new region, when will I see it in the provider."},{"location":"faq/#how-can-i-help","text":"Great question, if you have contributed before check out issues with the help-wanted label. These are normally enhancement issues that will have a great impact, but the maintainers are unable to develop them in the near future. If you are just getting started, take a look at issues with the good-first-issue label. Items with these labels will always be given priority for response. Check out the Contributing Guide for additional information.","title":"How can I help?"},{"location":"faq/#how-can-i-become-a-maintainer","text":"This is an area under active research. Stay tuned!","title":"How can I become a maintainer?"},{"location":"issue-reporting-and-lifecycle/","text":"Issue Reporting and Lifecycle # Issue Reporting Checklists # We welcome issues of all kinds including feature requests, bug reports, and general questions. Below you'll find checklists with guidelines for well-formed issues of each type. We encourage opening new issues rather than commenting on closed issues if a problem has not been completely solved or causes a regression. This ensures we are able to triage it effectively. Bug Reports # Test against latest release : Make sure you test against the latest released version. It is possible we already fixed the bug you're experiencing. Search for possible duplicate reports : It's helpful to keep bug reports consolidated to one thread, so do a quick search on existing bug reports to check if anybody else has reported the same thing. You can scope searches by the label \"bug\" to help narrow things down. Include steps to reproduce : Provide steps to reproduce the issue, along with your .tf files, with secrets removed, so we can try to reproduce it. Without this, it makes it much harder to fix the issue. For panics, include crash.log : If you experienced a panic, please create a gist of the entire generated crash log for us to look at. Double check no sensitive items were in the log. Feature Requests # Search for possible duplicate requests : It's helpful to keep requests consolidated to one thread, so do a quick search on existing requests to check if anybody else has reported the same thing. You can scope searches by the label \"enhancement\" to help narrow things down. Include a use case description : In addition to describing the behavior of the feature you'd like to see added, it's helpful to also lay out the reason why the feature would be important and how it would benefit Terraform users. Questions # Search for answers in Terraform documentation : We're happy to answer questions in GitHub Issues, but it helps reduce issue churn and maintainer workload if you work to find answers to common questions in the documentation . Oftentimes Question issues result in documentation updates to help future users, so if you don't find an answer, you can give us pointers for where you'd expect to see it in the docs. Issue Lifecycle # Note: For detailed information on how issues are prioritized, see the prioritization guide . The issue is reported. The issue is verified and categorized by a Terraform collaborator. Categorization is done via GitHub labels. We generally use a two-label system of (1) issue/PR type, and (2) section of the codebase. Type is one of \"bug\", \"enhancement\", \"documentation\", or \"question\", and section is usually the AWS service name. An initial triage process determines whether the issue is critical and must be addressed immediately, or can be left open for community discussion. The issue is addressed in a pull request or commit. The issue number will be referenced in the commit message so that the code that fixes it is clearly linked. The issue is closed. Sometimes, valid issues will be closed because they are tracked elsewhere or non-actionable. The issue is still indexed and available for future viewers, or can be re-opened if necessary. 30 days after the issue has been closed it is locked, preventing further comments.","title":"Submit an Issue"},{"location":"issue-reporting-and-lifecycle/#issue-reporting-and-lifecycle","text":"","title":"Issue Reporting and Lifecycle"},{"location":"issue-reporting-and-lifecycle/#issue-reporting-checklists","text":"We welcome issues of all kinds including feature requests, bug reports, and general questions. Below you'll find checklists with guidelines for well-formed issues of each type. We encourage opening new issues rather than commenting on closed issues if a problem has not been completely solved or causes a regression. This ensures we are able to triage it effectively.","title":"Issue Reporting Checklists"},{"location":"issue-reporting-and-lifecycle/#bug-reports","text":"Test against latest release : Make sure you test against the latest released version. It is possible we already fixed the bug you're experiencing. Search for possible duplicate reports : It's helpful to keep bug reports consolidated to one thread, so do a quick search on existing bug reports to check if anybody else has reported the same thing. You can scope searches by the label \"bug\" to help narrow things down. Include steps to reproduce : Provide steps to reproduce the issue, along with your .tf files, with secrets removed, so we can try to reproduce it. Without this, it makes it much harder to fix the issue. For panics, include crash.log : If you experienced a panic, please create a gist of the entire generated crash log for us to look at. Double check no sensitive items were in the log.","title":"Bug Reports"},{"location":"issue-reporting-and-lifecycle/#feature-requests","text":"Search for possible duplicate requests : It's helpful to keep requests consolidated to one thread, so do a quick search on existing requests to check if anybody else has reported the same thing. You can scope searches by the label \"enhancement\" to help narrow things down. Include a use case description : In addition to describing the behavior of the feature you'd like to see added, it's helpful to also lay out the reason why the feature would be important and how it would benefit Terraform users.","title":"Feature Requests"},{"location":"issue-reporting-and-lifecycle/#questions","text":"Search for answers in Terraform documentation : We're happy to answer questions in GitHub Issues, but it helps reduce issue churn and maintainer workload if you work to find answers to common questions in the documentation . Oftentimes Question issues result in documentation updates to help future users, so if you don't find an answer, you can give us pointers for where you'd expect to see it in the docs.","title":"Questions"},{"location":"issue-reporting-and-lifecycle/#issue-lifecycle","text":"Note: For detailed information on how issues are prioritized, see the prioritization guide . The issue is reported. The issue is verified and categorized by a Terraform collaborator. Categorization is done via GitHub labels. We generally use a two-label system of (1) issue/PR type, and (2) section of the codebase. Type is one of \"bug\", \"enhancement\", \"documentation\", or \"question\", and section is usually the AWS service name. An initial triage process determines whether the issue is critical and must be addressed immediately, or can be left open for community discussion. The issue is addressed in a pull request or commit. The issue number will be referenced in the commit message so that the code that fixes it is clearly linked. The issue is closed. Sometimes, valid issues will be closed because they are tracked elsewhere or non-actionable. The issue is still indexed and available for future viewers, or can be re-opened if necessary. 30 days after the issue has been closed it is locked, preventing further comments.","title":"Issue Lifecycle"},{"location":"naming/","text":"Naming Conventions for the AWS Provider # Service Identifier # In the AWS Provider, a service identifier should consistently identify an AWS service from code to documentation to provider use by a practitioner. Prominent places you will see service identifiers: The package name (e.g., internal/service/<serviceidentifier> ) In resource and data source names (e.g., aws_<serviceidentifier>_thing ) Documentation file names (e.g., website/docs/r/<serviceidentifier>_thing ) Typically, choosing the AWS Provider identifier for a service is simple. AWS consistently uses one name and we use that name as the identifier. However, some services are not simple. To provide consistency, and to help contributors and practitioners know what to expect, we provide this rule for defining a service identifier: Rule # Determine the service package name for AWS Go SDK v2 . Determine the AWS CLI v2 command corresponding to the service (i.e., the word following aws in CLI commands; e.g., for aws sts get-caller-identity , sts is the command , get-caller-identity is the subcommand ). If the SDK and CLI agree, use that. If the service only exists in one, use that. If they differ, use the shorter of the two. Use lowercase letters and do not include any underscores ( _ ). How Well Is It Followed? # With 156+ services having some level of implementation, the following is a summary of how well this rule is currently followed. For AWS provider service package names, only five packages violate this rule: appautoscaling should be applicationautoscaling , codedeploy should be deploy , elasticsearch should be es , cloudwatchlogs should be logs , and simpledb should be sdb . For the service identifiers used in resource and data source configuration names (e.g., aws_acmpca_certificate_authority ), 32 wholly or partially violate the rule. EC2, ELB, ELBv2, and RDS have legacy but heavily used resources and data sources that do not or inconsistently use service identifiers. The remaining 28 services violate the rule in a consistent way: appautoscaling should be applicationautoscaling , codedeploy should be deploy , elasticsearch should be es , cloudwatch_log should be logs , simpledb should be sdb , prometheus should be amp , api_gateway should be apigateway , cloudcontrolapi should be cloudcontrol , cognito_identity should be cognitoidentity , cognito should be cognitoidp , config should be configservice , dx should be directconnect , directory_service should be ds , elastic_beanstalk should be elasticbeanstalk , cloudwatch_event should be events , kinesis_firehose should be firehose , msk should be kafka , mskconnect should be kafkaconnect , kinesis_analytics should be kinesisanalytics , kinesis_video should be kinesisvideo , lex should be lexmodels , media_convert should be mediaconvert , media_package should be mediapackage , media_store should be mediastore , route53_resolver should be route53resolver , relevant s3 should be s3control , serverlessapplicationrepository should be serverlessrepo , and service_discovery should be servicediscovery . Packages # Package names are not seen or used by practitioners. However, they should still be carefully considered. Rule # For service packages (i.e., packages under internal/service ), use the AWS Provider service identifier as the package name. For other packages, use a short name for the package. Common Go lengths are 3-9 characters. Use a descriptive name. The name should capture the key purpose of the package. Use lowercase letters and do not include any underscores ( _ ). Avoid useless names like helper . These names convey zero information. Everything in the AWS Provider is helping something or someone do something so the name helper doesn't narrow down the purpose of the package within the codebase. Use a name that is not too narrow or too broad as Go packages should not be too big or too small. Tiny packages can be combined using a broader name encompassing both. For example, verify is a good name because it tells you what the package does and allows a broad set of validation, comparison, and checking functionality. Resources and Data Sources # When creating a new resource or data source, it is important to get names right. Once practitioners rely on names, we can only change them through breaking changes. If you are unsure about what to call a resource or data source, discuss it with the community and maintainers. Rule # Follow the AWS SDK for Go v2 . Almost always, the API operations make determining the name simple. For example, the Amazon CloudWatch Evidently service includes CreateExperiment , GetExperiment , UpdateExperiment , and DeleteExperiment . Thus, the resource (or data source) name is \"Experiment.\" Give a resource its Terraform configuration (i.e., HCL) name (e.g., aws_imagebuilder_image_pipeline ) by joining these three parts with underscores: aws prefix Service identifier (service identifiers do not include underscores), all lower case (e.g., imagebuilder ) Resource (or data source) name in snake case (spaces replaced with underscores, if any), all lower case (e.g., image_pipeline ) Name the main resource function Resource<ResourceName>() , with the resource name in MixedCaps . Do not include the service name or identifier. For example, define ResourceImagePipeline() in a file called internal/service/imagebuilder/image_pipeline.go . Similarly, name the main data source function DataSource<ResourceName>() , with the data source name in MixedCaps . Do not include the service name or identifier. For example, define DataSourceImagePipeline() in a file called internal/service/imagebuilder/image_pipeline_data_source.go . Files # File names should follow Go and Markdown conventions with these additional points. Resource and Data Source Documentation Rule # Resource markdown goes in the website/docs/r directory. Data source markdown goes in the website/docs/d directory. Use the service identifier and resource or data source name, separated by an underscore ( _ ). All letters are lowercase. Use .html.markdown as the extension. Do not include \"aws\" in the name. A correct example is accessanalyzer_analyzer.html.markdown . An incorrect example is service_discovery_instance.html.markdown because the service identifier should not include an underscore. Go File Rule # Resource and data source files are in the internal/service/<service> directory. Do not include the service as part of the file name. Data sources should include _data_source after the data source name (e.g., application_data_source.go ). Put unit and acceptance tests in a file ending with _test.go (e.g., custom_domain_association_test.go ). Use snake case for multiword names (i.e., all letters are lowercase, words separated by underscores). Use the .go extension. Idiomatic names for common non-resource, non-data-source files include consts.go (service-wide constants), find.go (finders), flex.go (FLatteners and EXpanders), generate.go (directives for code generation), id.go (ID creators and parsers), status.go (status functions), sweep.go (sweepers), tags_gen.go (generated tag code), validate.go (validators), and wait.go (waiters). MixedCaps # Write multiword names in Go using MixedCaps (or mixedCaps ) rather than underscores. For more details on capitalizations we enforce with CI Semgrep tests, see the Caps List . Initialisms and other abbreviations are a key difference between many camel/Pascal case interpretations and mixedCaps. Abbreviations in mixedCaps should be the correct, human-readable case. After all, names in code are for humans . (The mixedCaps convention aligns with HashiCorp's emphasis on pragmatism and beauty.) For example, an initialism such as \"VPC\" should either be all capitalized (\"VPC\") or all lower case (\"vpc\"), never \"Vpc\" or \"vPC.\" Similarly, in mixedCaps, \"DynamoDB\" should either be \"DynamoDB\" or \"dynamoDB\", depending on whether an initial cap is needed or not, and never \"dynamoDb\" or \"DynamoDb.\" Rule # Use mixedCaps for function, type, method, variable, and constant names in the Terraform AWS Provider Go code. Functions # In general, follow Go best practices for good function naming. This rule is for functions defined outside of the test context (i.e., not in a file ending with _test.go ). For test functions, see Test Support Functions or Acceptance Test Configurations below. Rule # Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package, including in the _test ( .test ) package. Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords. Do not include the service name in the function name. (If functions are used outside the current package, the import package clarifies a function's origin. For example, the EC2 function FindVPCEndpointByID() is used outside the internal/service/ec2 package but where it is used, the call is tfec2.FindVPCEndpointByID() .) For CRUD functions for resources, use this format: resource<ResourceName><CRUDFunction> . For example, resourceImageRecipeUpdate() , resourceBaiduChannelRead() . For data sources, for Read functions, use this format: dataSource<DataSourceName>Read . For example, dataSourceBrokerRead() , dataSourceEngineVersionRead() . To improve readability, consider including the resource name in helper function names that pertain only to that resource. For example, for an expander function for an \"App\" resource and a \"Campaign Hook\" expander, use expandAppCampaignHook() . Do not include \"AWS\" or \"Aws\" in the name. Variables and Constants # In general, follow Go best practices for good variable and constant naming. Rule # Only export variables and constants (capitalize) when necessary, i.e., the variable or constant is used outside the current package, including in the _test ( .test ) package. Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords. Do not include the service name in variable or constant names. (If variables or constants are used outside the current package, the import package clarifies its origin. For example, IAM's PropagationTimeout is widely used outside of IAM but each instance is through the package import alias, tfiam.PropagationTimeout . \"IAM\" is unnecessary in the constant name.) To improve readability, consider including the resource name in variable and constant names that pertain only to that resource. For example, for a string constant for a \"Role\" resource and a \"not found\" status, use roleStatusNotFound or RoleStatusNotFound , if used outside the service's package. Do not include \"AWS\" or \"Aws\" in the name. NOTE: Give priority to using constants from the AWS SDK for Go rather than defining new constants for the same values. Acceptance and Unit Tests # With about 6000 acceptance and unit tests, following these naming conventions is essential to organization and (human) context switching between services. There are three types of tests in the AWS Provider: (regular) acceptance tests, serialized acceptance tests, and unit tests. All are functions that take a variable of type *testing.T . Acceptance tests and unit tests have exported (i.e., capitalized) names while serialized tests do not. Serialized tests are called by another exported acceptance test, often ending with _serial . The majority of tests in the AWS provider are acceptance tests. Acceptance Test Rule # Acceptance test names have a minimum of two (e.g., TestAccBackupPlan_tags ) or a maximum of three (e.g., TestAccDynamoDBTable_Replica_multiple ) parts, joined with underscores: First part: All have a prefix (i.e., TestAcc ), service name (e.g., Backup , DynamoDB ), and resource name (e.g., Plan , Table ), MixedCaps without underscores between. Do not include \"AWS\" or \"Aws\" in the name. Middle part (Optional): Test group (e.g., Replica ), uppercase, MixedCaps . Consider a metaphor where tests are chapters in a book. If it is helpful, tests can be grouped together like chapters in a book that are sometimes grouped into parts or sections of the book. Last part: Test identifier (e.g., basic , tags , or multiple ), lowercase, mixedCaps ). The identifier should make the test's purpose clear but be concise. For example, the identifier conflictsWithCloudFrontDefaultCertificate (41 characters) conveys no more information than conflictDefaultCertificate (26 characters), since \"CloudFront\" is implied and \"with\" is always implicit. Avoid words that convey no meaning or whose meaning is implied. For example, \"with\" (e.g., _withTags ) is not needed because we imply the name is telling us what the test is with . withTags can be simplified to tags . Serialized Acceptance Test Rule # The names of serialized acceptance tests follow the regular acceptance test name rule except serialized acceptance test names: Start with testAcc instead of TestAcc Do not include the name of the service (e.g., a serialized acceptance test would be called testAccApp_basic not testAccAmplifyApp_basic ). Unit Test Rule # Unit test names follow the same rule as acceptance test names except unit test names: Start with Test , not TestAcc Do not include the name of the service Usually do not have any underscores If they test a function, should include the function name (e.g., a unit test of ExpandListener() should be called TestExpandListener() ) Test Support Functions # This rule is for functions defined in the test context (i.e., in a file ending with _test.go ) that do not return a string with Terraform configuration. For non-test functions, see Functions above. Or, see Acceptance Test Configurations below. Rule # Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package. This is very rare. Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords. Do not include the service name in the function name. For example, testAccCheckAMPWorkspaceExists() should be named testAccCheckWorkspaceExists() instead, dropping the service name. Several types of support functions occur commonly and should follow these patterns: Destroy: testAccCheck<Resource>Destroy Disappears: testAccCheck<Resource>Disappears Exists: testAccCheck<Resource>Exists Not Recreated: testAccCheck<Resource>NotRecreated PreCheck: testAccPreCheck (often, only one PreCheck is needed per service so no resource name is needed) Recreated: testAccCheck<Resource>Recreated Do not include \"AWS\" or \"Aws\" in the name. Acceptance Test Configurations # This rule is for functions defined in the test context (i.e., in a file ending with _test.go ) that return a string with Terraform configuration. For test support functions, see Test Support Functions above. Or, for non-test functions, see Functions above. NOTE: This rule is not widely used currently. However, new functions and functions you change should follow it. Rule # Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package. This is very rare. Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords. Do not include the service name in the function name. Follow this pattern: testAccConfig<Resource>_<TestGroup>_<configDescription> _<TestGroup> is optional. Refer to the Acceptance Test Rule test group discussion. Especially when an acceptance test only uses one configuration, the <configDescription> should be the same as the test identifier discussed in the Acceptance Test Rule . Do not include \"AWS\" or \"Aws\" in the name.","title":"Naming Standards"},{"location":"naming/#naming-conventions-for-the-aws-provider","text":"","title":"Naming Conventions for the AWS Provider"},{"location":"naming/#service-identifier","text":"In the AWS Provider, a service identifier should consistently identify an AWS service from code to documentation to provider use by a practitioner. Prominent places you will see service identifiers: The package name (e.g., internal/service/<serviceidentifier> ) In resource and data source names (e.g., aws_<serviceidentifier>_thing ) Documentation file names (e.g., website/docs/r/<serviceidentifier>_thing ) Typically, choosing the AWS Provider identifier for a service is simple. AWS consistently uses one name and we use that name as the identifier. However, some services are not simple. To provide consistency, and to help contributors and practitioners know what to expect, we provide this rule for defining a service identifier:","title":"Service Identifier"},{"location":"naming/#rule","text":"Determine the service package name for AWS Go SDK v2 . Determine the AWS CLI v2 command corresponding to the service (i.e., the word following aws in CLI commands; e.g., for aws sts get-caller-identity , sts is the command , get-caller-identity is the subcommand ). If the SDK and CLI agree, use that. If the service only exists in one, use that. If they differ, use the shorter of the two. Use lowercase letters and do not include any underscores ( _ ).","title":"Rule"},{"location":"naming/#how-well-is-it-followed","text":"With 156+ services having some level of implementation, the following is a summary of how well this rule is currently followed. For AWS provider service package names, only five packages violate this rule: appautoscaling should be applicationautoscaling , codedeploy should be deploy , elasticsearch should be es , cloudwatchlogs should be logs , and simpledb should be sdb . For the service identifiers used in resource and data source configuration names (e.g., aws_acmpca_certificate_authority ), 32 wholly or partially violate the rule. EC2, ELB, ELBv2, and RDS have legacy but heavily used resources and data sources that do not or inconsistently use service identifiers. The remaining 28 services violate the rule in a consistent way: appautoscaling should be applicationautoscaling , codedeploy should be deploy , elasticsearch should be es , cloudwatch_log should be logs , simpledb should be sdb , prometheus should be amp , api_gateway should be apigateway , cloudcontrolapi should be cloudcontrol , cognito_identity should be cognitoidentity , cognito should be cognitoidp , config should be configservice , dx should be directconnect , directory_service should be ds , elastic_beanstalk should be elasticbeanstalk , cloudwatch_event should be events , kinesis_firehose should be firehose , msk should be kafka , mskconnect should be kafkaconnect , kinesis_analytics should be kinesisanalytics , kinesis_video should be kinesisvideo , lex should be lexmodels , media_convert should be mediaconvert , media_package should be mediapackage , media_store should be mediastore , route53_resolver should be route53resolver , relevant s3 should be s3control , serverlessapplicationrepository should be serverlessrepo , and service_discovery should be servicediscovery .","title":"How Well Is It Followed?"},{"location":"naming/#packages","text":"Package names are not seen or used by practitioners. However, they should still be carefully considered.","title":"Packages"},{"location":"naming/#rule_1","text":"For service packages (i.e., packages under internal/service ), use the AWS Provider service identifier as the package name. For other packages, use a short name for the package. Common Go lengths are 3-9 characters. Use a descriptive name. The name should capture the key purpose of the package. Use lowercase letters and do not include any underscores ( _ ). Avoid useless names like helper . These names convey zero information. Everything in the AWS Provider is helping something or someone do something so the name helper doesn't narrow down the purpose of the package within the codebase. Use a name that is not too narrow or too broad as Go packages should not be too big or too small. Tiny packages can be combined using a broader name encompassing both. For example, verify is a good name because it tells you what the package does and allows a broad set of validation, comparison, and checking functionality.","title":"Rule"},{"location":"naming/#resources-and-data-sources","text":"When creating a new resource or data source, it is important to get names right. Once practitioners rely on names, we can only change them through breaking changes. If you are unsure about what to call a resource or data source, discuss it with the community and maintainers.","title":"Resources and Data Sources"},{"location":"naming/#rule_2","text":"Follow the AWS SDK for Go v2 . Almost always, the API operations make determining the name simple. For example, the Amazon CloudWatch Evidently service includes CreateExperiment , GetExperiment , UpdateExperiment , and DeleteExperiment . Thus, the resource (or data source) name is \"Experiment.\" Give a resource its Terraform configuration (i.e., HCL) name (e.g., aws_imagebuilder_image_pipeline ) by joining these three parts with underscores: aws prefix Service identifier (service identifiers do not include underscores), all lower case (e.g., imagebuilder ) Resource (or data source) name in snake case (spaces replaced with underscores, if any), all lower case (e.g., image_pipeline ) Name the main resource function Resource<ResourceName>() , with the resource name in MixedCaps . Do not include the service name or identifier. For example, define ResourceImagePipeline() in a file called internal/service/imagebuilder/image_pipeline.go . Similarly, name the main data source function DataSource<ResourceName>() , with the data source name in MixedCaps . Do not include the service name or identifier. For example, define DataSourceImagePipeline() in a file called internal/service/imagebuilder/image_pipeline_data_source.go .","title":"Rule"},{"location":"naming/#files","text":"File names should follow Go and Markdown conventions with these additional points.","title":"Files"},{"location":"naming/#resource-and-data-source-documentation-rule","text":"Resource markdown goes in the website/docs/r directory. Data source markdown goes in the website/docs/d directory. Use the service identifier and resource or data source name, separated by an underscore ( _ ). All letters are lowercase. Use .html.markdown as the extension. Do not include \"aws\" in the name. A correct example is accessanalyzer_analyzer.html.markdown . An incorrect example is service_discovery_instance.html.markdown because the service identifier should not include an underscore.","title":"Resource and Data Source Documentation Rule"},{"location":"naming/#go-file-rule","text":"Resource and data source files are in the internal/service/<service> directory. Do not include the service as part of the file name. Data sources should include _data_source after the data source name (e.g., application_data_source.go ). Put unit and acceptance tests in a file ending with _test.go (e.g., custom_domain_association_test.go ). Use snake case for multiword names (i.e., all letters are lowercase, words separated by underscores). Use the .go extension. Idiomatic names for common non-resource, non-data-source files include consts.go (service-wide constants), find.go (finders), flex.go (FLatteners and EXpanders), generate.go (directives for code generation), id.go (ID creators and parsers), status.go (status functions), sweep.go (sweepers), tags_gen.go (generated tag code), validate.go (validators), and wait.go (waiters).","title":"Go File Rule"},{"location":"naming/#mixedcaps","text":"Write multiword names in Go using MixedCaps (or mixedCaps ) rather than underscores. For more details on capitalizations we enforce with CI Semgrep tests, see the Caps List . Initialisms and other abbreviations are a key difference between many camel/Pascal case interpretations and mixedCaps. Abbreviations in mixedCaps should be the correct, human-readable case. After all, names in code are for humans . (The mixedCaps convention aligns with HashiCorp's emphasis on pragmatism and beauty.) For example, an initialism such as \"VPC\" should either be all capitalized (\"VPC\") or all lower case (\"vpc\"), never \"Vpc\" or \"vPC.\" Similarly, in mixedCaps, \"DynamoDB\" should either be \"DynamoDB\" or \"dynamoDB\", depending on whether an initial cap is needed or not, and never \"dynamoDb\" or \"DynamoDb.\"","title":"MixedCaps"},{"location":"naming/#rule_3","text":"Use mixedCaps for function, type, method, variable, and constant names in the Terraform AWS Provider Go code.","title":"Rule"},{"location":"naming/#functions","text":"In general, follow Go best practices for good function naming. This rule is for functions defined outside of the test context (i.e., not in a file ending with _test.go ). For test functions, see Test Support Functions or Acceptance Test Configurations below.","title":"Functions"},{"location":"naming/#rule_4","text":"Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package, including in the _test ( .test ) package. Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords. Do not include the service name in the function name. (If functions are used outside the current package, the import package clarifies a function's origin. For example, the EC2 function FindVPCEndpointByID() is used outside the internal/service/ec2 package but where it is used, the call is tfec2.FindVPCEndpointByID() .) For CRUD functions for resources, use this format: resource<ResourceName><CRUDFunction> . For example, resourceImageRecipeUpdate() , resourceBaiduChannelRead() . For data sources, for Read functions, use this format: dataSource<DataSourceName>Read . For example, dataSourceBrokerRead() , dataSourceEngineVersionRead() . To improve readability, consider including the resource name in helper function names that pertain only to that resource. For example, for an expander function for an \"App\" resource and a \"Campaign Hook\" expander, use expandAppCampaignHook() . Do not include \"AWS\" or \"Aws\" in the name.","title":"Rule"},{"location":"naming/#variables-and-constants","text":"In general, follow Go best practices for good variable and constant naming.","title":"Variables and Constants"},{"location":"naming/#rule_5","text":"Only export variables and constants (capitalize) when necessary, i.e., the variable or constant is used outside the current package, including in the _test ( .test ) package. Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords. Do not include the service name in variable or constant names. (If variables or constants are used outside the current package, the import package clarifies its origin. For example, IAM's PropagationTimeout is widely used outside of IAM but each instance is through the package import alias, tfiam.PropagationTimeout . \"IAM\" is unnecessary in the constant name.) To improve readability, consider including the resource name in variable and constant names that pertain only to that resource. For example, for a string constant for a \"Role\" resource and a \"not found\" status, use roleStatusNotFound or RoleStatusNotFound , if used outside the service's package. Do not include \"AWS\" or \"Aws\" in the name. NOTE: Give priority to using constants from the AWS SDK for Go rather than defining new constants for the same values.","title":"Rule"},{"location":"naming/#acceptance-and-unit-tests","text":"With about 6000 acceptance and unit tests, following these naming conventions is essential to organization and (human) context switching between services. There are three types of tests in the AWS Provider: (regular) acceptance tests, serialized acceptance tests, and unit tests. All are functions that take a variable of type *testing.T . Acceptance tests and unit tests have exported (i.e., capitalized) names while serialized tests do not. Serialized tests are called by another exported acceptance test, often ending with _serial . The majority of tests in the AWS provider are acceptance tests.","title":"Acceptance and Unit Tests"},{"location":"naming/#acceptance-test-rule","text":"Acceptance test names have a minimum of two (e.g., TestAccBackupPlan_tags ) or a maximum of three (e.g., TestAccDynamoDBTable_Replica_multiple ) parts, joined with underscores: First part: All have a prefix (i.e., TestAcc ), service name (e.g., Backup , DynamoDB ), and resource name (e.g., Plan , Table ), MixedCaps without underscores between. Do not include \"AWS\" or \"Aws\" in the name. Middle part (Optional): Test group (e.g., Replica ), uppercase, MixedCaps . Consider a metaphor where tests are chapters in a book. If it is helpful, tests can be grouped together like chapters in a book that are sometimes grouped into parts or sections of the book. Last part: Test identifier (e.g., basic , tags , or multiple ), lowercase, mixedCaps ). The identifier should make the test's purpose clear but be concise. For example, the identifier conflictsWithCloudFrontDefaultCertificate (41 characters) conveys no more information than conflictDefaultCertificate (26 characters), since \"CloudFront\" is implied and \"with\" is always implicit. Avoid words that convey no meaning or whose meaning is implied. For example, \"with\" (e.g., _withTags ) is not needed because we imply the name is telling us what the test is with . withTags can be simplified to tags .","title":"Acceptance Test Rule"},{"location":"naming/#serialized-acceptance-test-rule","text":"The names of serialized acceptance tests follow the regular acceptance test name rule except serialized acceptance test names: Start with testAcc instead of TestAcc Do not include the name of the service (e.g., a serialized acceptance test would be called testAccApp_basic not testAccAmplifyApp_basic ).","title":"Serialized Acceptance Test Rule"},{"location":"naming/#unit-test-rule","text":"Unit test names follow the same rule as acceptance test names except unit test names: Start with Test , not TestAcc Do not include the name of the service Usually do not have any underscores If they test a function, should include the function name (e.g., a unit test of ExpandListener() should be called TestExpandListener() )","title":"Unit Test Rule"},{"location":"naming/#test-support-functions","text":"This rule is for functions defined in the test context (i.e., in a file ending with _test.go ) that do not return a string with Terraform configuration. For non-test functions, see Functions above. Or, see Acceptance Test Configurations below.","title":"Test Support Functions"},{"location":"naming/#rule_6","text":"Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package. This is very rare. Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords. Do not include the service name in the function name. For example, testAccCheckAMPWorkspaceExists() should be named testAccCheckWorkspaceExists() instead, dropping the service name. Several types of support functions occur commonly and should follow these patterns: Destroy: testAccCheck<Resource>Destroy Disappears: testAccCheck<Resource>Disappears Exists: testAccCheck<Resource>Exists Not Recreated: testAccCheck<Resource>NotRecreated PreCheck: testAccPreCheck (often, only one PreCheck is needed per service so no resource name is needed) Recreated: testAccCheck<Resource>Recreated Do not include \"AWS\" or \"Aws\" in the name.","title":"Rule"},{"location":"naming/#acceptance-test-configurations","text":"This rule is for functions defined in the test context (i.e., in a file ending with _test.go ) that return a string with Terraform configuration. For test support functions, see Test Support Functions above. Or, for non-test functions, see Functions above. NOTE: This rule is not widely used currently. However, new functions and functions you change should follow it.","title":"Acceptance Test Configurations"},{"location":"naming/#rule_7","text":"Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package. This is very rare. Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords. Do not include the service name in the function name. Follow this pattern: testAccConfig<Resource>_<TestGroup>_<configDescription> _<TestGroup> is optional. Refer to the Acceptance Test Rule test group discussion. Especially when an acceptance test only uses one configuration, the <configDescription> should be the same as the test identifier discussed in the Acceptance Test Rule . Do not include \"AWS\" or \"Aws\" in the name.","title":"Rule"},{"location":"prioritization/","text":"How We Prioritize # Intro # What this document is # This document describes how we handle prioritization of work from a variety of input sources. Our focus is always to deliver tangible value to the practitioner on a predictable and frequent schedule, and we feel it is important to be transparent in how we weigh input in order to deliver on this goal. What this document is not # Due to the variety of input sources, the scale of the provider, and resource constraints, it is impossible to give a hard number on how each of the factors outlined in this document are weighted. Instead, the goal of the document is to give a transparent, but generalized assessment of each of the sources of input so that the community has a better idea of why things are prioritized the way they are. Additional information may be found in the FAQ . Prioritization # We prioritze work based on a number of factors, including community feedback, issue/PR reactions, as well as the source of the request. While community feedback is heavily weighted, there are times where other factors take precedence. By their nature, some factors are less visible to the community, and so are outlined here as a way to be as transparent as possible. Each of the sources of input are detailed below. Community # Our large community of practitioners are vocal and immensely productive in contributing to the provider codebase. Unfortunately our current team capacity means that we are unable to give every issue or pull request the same level of attention. This means we need to prioritize the issues that provide the most value to the greatest number of practitioners. We will always focus on the issues which have the most attention. The main rubric we have for assessing community wants is GitHub reactions. In addition to reactions, we look at comments, reactions to comments, and links to additional issues and PRs to help get a more holistic view of where the community stands. We try to ensure that for the issues where we have the most community support, we are responsive to that support and attempt to give timelines where-ever possible. Customer # Another source of work that must be weighted are escalations around particular feature requests and bugs from HashiCorp and AWS customers. Escalations typically come via several routes: Customer Support Sales Engineering AWS Solutions Architects contacting us on behalf of their clients. These reports flow into an internal board and are triaged on a weekly basis to determine whether the escalation request should be prioritized for an upcoming release or added to the backlog to monitor for additional community support. During triage, we verify whether a GitHub issue or PR exists for the request and will create one if it does not exist. In this way, these requests are visible to the community to some degree. An escalation coming from a customer does not necessarily guarantee that it will be prioritized over requests made by the community. Instead, we assess them based on the following rubric: Does the issue have considerable community support? Does the issue pertain to one of our Core Services ? By weighing these factors, we can make a call to determine whether, and how it is to be prioritized. Partner # AWS Service Teams and Partner representatives regularly contact us to discuss upcoming features or new services. This work is often done under an NDA, so usually needs to be done in private. Often the ask is to enable Terraform support or an upcoming feature or service. As with customer escalations, a request from a partner does not necessarily mean that it will be prioritized over other efforts; capacity restraints require us to prioritize major releases or prefer offerings in line with our core services . Internal # SDK/Core Updates # We endeavor to keep in step with all minor SDK releases, so these are automatically pulled in by our GitHub automation. Major releases normally include breaking changes and usually require us to bump the provider itself to a major version. We plan to make one major version change a year and try to avoid any more than that. Technical Debt # We always include capacity for technical debt work in every iteration, but engineers are free to include minor tech debt work on their own recognizance. For larger items, these are discussed and prioritized in an internal meeting aimed at reviewing technical debt. Adverse User Experience or Security Vulnerabilities # Issues with the provider that provide a poor user experience (bugs, crashes), or involve a threat to security are always prioritized for inclusion. The severity of these will determine how soon they are included for release.","title":"How We Prioritize"},{"location":"prioritization/#how-we-prioritize","text":"","title":"How We Prioritize"},{"location":"prioritization/#intro","text":"","title":"Intro"},{"location":"prioritization/#what-this-document-is","text":"This document describes how we handle prioritization of work from a variety of input sources. Our focus is always to deliver tangible value to the practitioner on a predictable and frequent schedule, and we feel it is important to be transparent in how we weigh input in order to deliver on this goal.","title":"What this document is"},{"location":"prioritization/#what-this-document-is-not","text":"Due to the variety of input sources, the scale of the provider, and resource constraints, it is impossible to give a hard number on how each of the factors outlined in this document are weighted. Instead, the goal of the document is to give a transparent, but generalized assessment of each of the sources of input so that the community has a better idea of why things are prioritized the way they are. Additional information may be found in the FAQ .","title":"What this document is not"},{"location":"prioritization/#prioritization","text":"We prioritze work based on a number of factors, including community feedback, issue/PR reactions, as well as the source of the request. While community feedback is heavily weighted, there are times where other factors take precedence. By their nature, some factors are less visible to the community, and so are outlined here as a way to be as transparent as possible. Each of the sources of input are detailed below.","title":"Prioritization"},{"location":"prioritization/#community","text":"Our large community of practitioners are vocal and immensely productive in contributing to the provider codebase. Unfortunately our current team capacity means that we are unable to give every issue or pull request the same level of attention. This means we need to prioritize the issues that provide the most value to the greatest number of practitioners. We will always focus on the issues which have the most attention. The main rubric we have for assessing community wants is GitHub reactions. In addition to reactions, we look at comments, reactions to comments, and links to additional issues and PRs to help get a more holistic view of where the community stands. We try to ensure that for the issues where we have the most community support, we are responsive to that support and attempt to give timelines where-ever possible.","title":"Community"},{"location":"prioritization/#customer","text":"Another source of work that must be weighted are escalations around particular feature requests and bugs from HashiCorp and AWS customers. Escalations typically come via several routes: Customer Support Sales Engineering AWS Solutions Architects contacting us on behalf of their clients. These reports flow into an internal board and are triaged on a weekly basis to determine whether the escalation request should be prioritized for an upcoming release or added to the backlog to monitor for additional community support. During triage, we verify whether a GitHub issue or PR exists for the request and will create one if it does not exist. In this way, these requests are visible to the community to some degree. An escalation coming from a customer does not necessarily guarantee that it will be prioritized over requests made by the community. Instead, we assess them based on the following rubric: Does the issue have considerable community support? Does the issue pertain to one of our Core Services ? By weighing these factors, we can make a call to determine whether, and how it is to be prioritized.","title":"Customer"},{"location":"prioritization/#partner","text":"AWS Service Teams and Partner representatives regularly contact us to discuss upcoming features or new services. This work is often done under an NDA, so usually needs to be done in private. Often the ask is to enable Terraform support or an upcoming feature or service. As with customer escalations, a request from a partner does not necessarily mean that it will be prioritized over other efforts; capacity restraints require us to prioritize major releases or prefer offerings in line with our core services .","title":"Partner"},{"location":"prioritization/#internal","text":"","title":"Internal"},{"location":"prioritization/#sdkcore-updates","text":"We endeavor to keep in step with all minor SDK releases, so these are automatically pulled in by our GitHub automation. Major releases normally include breaking changes and usually require us to bump the provider itself to a major version. We plan to make one major version change a year and try to avoid any more than that.","title":"SDK/Core Updates"},{"location":"prioritization/#technical-debt","text":"We always include capacity for technical debt work in every iteration, but engineers are free to include minor tech debt work on their own recognizance. For larger items, these are discussed and prioritized in an internal meeting aimed at reviewing technical debt.","title":"Technical Debt"},{"location":"prioritization/#adverse-user-experience-or-security-vulnerabilities","text":"Issues with the provider that provide a poor user experience (bugs, crashes), or involve a threat to security are always prioritized for inclusion. The severity of these will determine how soon they are included for release.","title":"Adverse User Experience or Security Vulnerabilities"},{"location":"provider-design/","text":"Provider Design # The Terraform AWS Provider follows the guidelines established in the HashiCorp Provider Design Principles . That general documentation provides many high-level design points gleaned from years of experience with Terraform's design and implementation concepts. Sections below will expand on specific design details between that documentation and this provider, while others will capture other pertinent information that may not be covered there. Other pages of the contributing guide cover implementation details such as code, testing, and documentation specifics. API and SDK Boundary # The AWS provider implements support for the AWS service APIs using the AWS Go SDK . The API and SDK limits extend to the provider. In general, SDK operations manage the lifecycle of AWS components, such as creating, describing, updating, and deleting a database. Operations do not usually handle functionality within those components, such as executing a query on a database. If you are interested in other APIs/SDKs, we invite you to view the many Terraform Providers available, as each has a community of domain expertise. Some examples of functionality that is not expected in this provider: Raw HTTP(S) handling. See the Terraform HTTP Provider and Terraform TLS Provider instead. Kubernetes resource management beyond the EKS service APIs. See the Terraform Kubernetes Provider instead. Active Directory or other protocol clients. See the Terraform Active Directory Provider and other available provider instead. Functionality that requires additional software beyond the Terraform AWS Provider to be installed on the host executing Terraform. This currently includes the AWS CLI. See the Terraform External Provider and other available providers instead. Infrastructure as Code Suitability # The provider maintainers' design goal is to cover as much of the AWS API as pragmatically possible. However, not every aspect of the API is compatible with an infrastructure-as-code (IaC) conception. Specifically: IaC is best suited for immutable rather than mutable infrastrcture -- i.e. for resources with a single desired state described in its entirety, as opposed to resources defined via a dynamic process. If such limits affect you, we recommend that you open an AWS Support case and encourage others to do the same. Request that AWS components be made more self-contained and compatible with IaC. These AWS Support cases can also yield insights into the AWS service and API that are not well documented. Resource Type Considerations # Terraform resources work best as the smallest infrastructure blocks on which practitioners can build more complex configurations and abstractions, such as Terraform Modules . The general heuristic guiding when to implement a new Terraform resource for an aspect of AWS is whether the AWS service API provides create, read, update, and delete (CRUD) operations. However, not all AWS service API functionality falls cleanly into CRUD lifecycle management. In these situations, there is extra consideration necessary for properly mapping API operations to Terraform resources. This section highlights design patterns when to consider an implementation within a singular Terraform resource or as separate Terraform resources. Please note: the overall design and implementation across all AWS functionality is federated: individual services may implement concepts and use terminology differently. As such, this guide is not exhaustive. The aim is to provide general concepts and basic terminology that points contributors in the right direction, especially in understanding previous implementations. Authorization and Acceptance Resources # Some AWS services use an authorization-acceptance model for cross-account associations or access. Examples include: Direct Connect Association Proposals GuardDuty Member Invitations RAM Resource Share Associations Route 53 VPC Associations Security Hub Member Invitations Depending on the API and components, AWS uses two basic ways of creating cross-region and cross-account associations. One way is to generate an invitation (or proposal) identifier from one AWS account to another. Then in the other AWS account, that identifier is used to accept the invitation. The second way is configuring a reference to another AWS account identifier. These may not require explicit acceptance on the receiving account to finish creating the association or begin working. To model creating an association using an invitation or proposal, follow these guidelines. Follow the naming in the AWS service API to determine whether to use the term \"invitation\" or \"proposal.\" For the originating account, create an \"invitation\" or \"proposal\" resource. Make sure that the AWS service API has operations for creating and reading invitations. For the responding account, create an \"accepter\" resource. Ensure that the API has operations for accepting, reading, and rejecting invitations in the responding account. Map the operations as follows: Create: Accepts the invitation. Read: Reads the invitation to determine its status. Note that in some APIs, invitations expire and disappear, complicating associations. If a resource does not find an invitation, the developer should implement a fall back to read the API resource associated with the invitation/proposal. Delete: Rejects or otherwise deletes the invitation. To model the second type of association, implicit associations, create an \"association\" resource and, optionally, an \"authorization\" resource. Map create, read, and delete to the corresponding operations in the AWS service API. Cross-Service Functionality # Many AWS service APIs build on top of other AWS services. Some examples of these include: EKS Node Groups managing Auto Scaling Groups Lambda Functions managing EC2 ENIs Transfer Servers managing EC2 VPC Endpoints Some cross-service API implementations lack the management or description capabilities of the other service. The lack can make the Terraform resource implementation seem incomplete or unsuccessful in end-to-end configurations. Given the overall \u201cresources should represent a single API object\u201d goal from the HashiCorp Provider Design Principles , a resource must only communicate with a single AWS service API. As such, maintainers will not approve cross-service resources. The rationale behind this design decision includes the following: Unexpected IAM permissions being necessary for the resource. In high-security environments, all the service permissions may not be available or acceptable. Unexpected services generating CloudTrail logs for the resource. Needing extra and unexpected API endpoints configuration for organizations using custom endpoints, such as VPC endpoints. Unexpected changes to the AWS service internals for the cross-service implementations. Given that this functionality is not part of the primary service API, these details can change over time and may not be considered as a breaking change by the service team for an API upgrade. A poignant real-world example of the last point involved a Lambda resource. The resource helped clean up extra resources (ENIs) due to a common misconfiguration. Practitioners found the functionality helpful since the issue was hard to diagnose. Years later, AWS updated the Lambda API. Immediately, practitioners reported that Terraform executions were failing. Downgrading the provider was not possible since many configurations depended on recent releases. For environments running many versions behind, forcing an upgrade with the fix would likely cause unrelated and unexpected changes. In the end, HashiCorp and AWS performed a large-scale outreach to help upgrade and fixing the misconfigurations. Provider maintainers and practitioners lost considerable time. Data Sources # A separate class of Terraform resource types are data sources . These are typically intended as a configuration method to lookup or fetch data in a read-only manner. Data sources should not have side effects on the remote system. When discussing data sources, they are typically classified by the intended number of return objects or data. Singular data sources represent a one-to-one lookup or data operation. Plural data sources represent a one-to-many operation. Plural Data Sources # These data sources are intended to return zero, one, or many results, usually associated with a managed resource type. Typically results are a set unless ordering guarantees are provided by the remote system. These should be named with a plural suffix (e.g., s or es ) and should not include any specific attribute in the naming (e.g., prefer aws_ec2_transit_gateways instead of aws_ec2_transit_gateway_ids ). Singular Data Sources # These data sources are intended to return one result or an error. These should not include any specific attribute in the naming (e.g., prefer aws_ec2_transit_gateway instead of aws_ec2_transit_gateway_id ). IAM Resource-Based Policy Resources # For some AWS components, the AWS API allows specifying an IAM resource-based policy , the IAM policy to associate with a component. Some examples include: ECR Repository Policies EFS File System Policies SNS Topic Policies Provider developers should implement this capability in a new resource rather than adding it to the associated resource. Reasons for this include: Many of the policies must include the ARN of the resource. Working around this requirement with custom difference handling within a self-contained resource is unnecessarily cumbersome. Some policies involving multiple resources need to cross-reference each other's ARNs. Without a separate resource, this introduces a configuration cycle. Splitting the resources allows operators to logically split their configurations into purely operational and security boundaries. This allows environments to have distinct practitioners roles and permissions for IAM versus infrastructure changes. One rare exception to this guideline is where the policy is required during resource creation. Managing Resource Running State # The AWS API provides the ability to start, stop, enable, or disable some AWS components. Some examples include: Batch Job Queues CloudFront Distributions RDS DB Event Subscriptions In this situation, provider developers should implement this ability within the resource instead of creating a separate resource. Since a practitioner cannot practically manage interaction with a resource's states in Terraform's declarative configuration, developers should implement the state management in the resource. This design provides consistency and future-proofing even where updating a resource in the current API is not problematic. Task Execution and Waiter Resources # Some AWS operations are asynchronous. Terraform requests that AWS perform a task. Initially, AWS only notifies Terraform that it received the request. Terraform then requests the status while awaiting completion. Examples of this include: ACM Certificate validation EC2 AMI copying RDS DB Cluster Snapshot management In this situation, provider developers should create a separate resource representing the task, assuming that the AWS service API provides operations to start the task and read its status. Adding the task functionality to the parent resource muddies its infrastructure-management purpose. The maintainers prefer this approach even though there is some duplication of an existing resource. For example, the provider has a resource for copying an EC2 AMI in addition to the EC2 AMI resource itself. This modularity allows practitioners to manage the result of the task resource with another resource. For a related consideration, see the Managing Resource Running State section . Versioned Resources # AWS supports having multiple versions of some components. Examples of this include: ECS Task Definitions Lambda Functions Secrets Manager Secrets In general, provider developers should create a separate resource to represent a single version. For example, the provider has both the aws_secretsmanager_secret and aws_secretsmanager_secret_version resources. However, in some cases, developers should handle versioning in the main resource. In deciding when to create a separate resource, follow these guidelines: If AWS necessarily creates a version when you make a new AWS component, include version handling in the same Terraform resource. Creating an AWS component with one Terraform resource and later using a different resource for updates is confusing. If the AWS service API allows deleting versions and practitioners will want to delete versions, provider developers should implement a separate version resource. If the API only supports publishing new versions, either method is acceptable, however most current implementations are self-contained. Terraform's current configuration language does not natively support triggering resource updates or recreation across resources without a state value change. This can make the implementation more difficult for practitioners without special resource and configuration workarounds, such as a triggers attribute. If this changes in the future, then this guidance may be updated towards separate resources, following the Task Execution and Waiter Resources guidance. Other Considerations # AWS Credential Exfiltration # In the interest of security, the maintainers will not approve data sources that provide the ability to reference or export the AWS credentials of the running provider. There are valid use cases for this information, such as to execute AWS CLI calls as part of the same Terraform configuration. However, this mechanism may allow credentials to be discovered and used outside of Terraform. Some specific concerns include: The values may be visible in Terraform user interface output or logging, allowing anyone with user interface or log access to see the credentials. The values are currently stored in plaintext in the Terraform state, allowing anyone with access to the state file or another Terraform configuration that references the state access to the credentials. Any new related functionality, while opt-in to implement, is also opt-in to prevent via security controls or policies. Adopting a weaker default security posture requires advance notice and prevents organizations that implement those controls from updating to a version with any such functionality.","title":"Provider Design"},{"location":"provider-design/#provider-design","text":"The Terraform AWS Provider follows the guidelines established in the HashiCorp Provider Design Principles . That general documentation provides many high-level design points gleaned from years of experience with Terraform's design and implementation concepts. Sections below will expand on specific design details between that documentation and this provider, while others will capture other pertinent information that may not be covered there. Other pages of the contributing guide cover implementation details such as code, testing, and documentation specifics.","title":"Provider Design"},{"location":"provider-design/#api-and-sdk-boundary","text":"The AWS provider implements support for the AWS service APIs using the AWS Go SDK . The API and SDK limits extend to the provider. In general, SDK operations manage the lifecycle of AWS components, such as creating, describing, updating, and deleting a database. Operations do not usually handle functionality within those components, such as executing a query on a database. If you are interested in other APIs/SDKs, we invite you to view the many Terraform Providers available, as each has a community of domain expertise. Some examples of functionality that is not expected in this provider: Raw HTTP(S) handling. See the Terraform HTTP Provider and Terraform TLS Provider instead. Kubernetes resource management beyond the EKS service APIs. See the Terraform Kubernetes Provider instead. Active Directory or other protocol clients. See the Terraform Active Directory Provider and other available provider instead. Functionality that requires additional software beyond the Terraform AWS Provider to be installed on the host executing Terraform. This currently includes the AWS CLI. See the Terraform External Provider and other available providers instead.","title":"API and SDK Boundary"},{"location":"provider-design/#infrastructure-as-code-suitability","text":"The provider maintainers' design goal is to cover as much of the AWS API as pragmatically possible. However, not every aspect of the API is compatible with an infrastructure-as-code (IaC) conception. Specifically: IaC is best suited for immutable rather than mutable infrastrcture -- i.e. for resources with a single desired state described in its entirety, as opposed to resources defined via a dynamic process. If such limits affect you, we recommend that you open an AWS Support case and encourage others to do the same. Request that AWS components be made more self-contained and compatible with IaC. These AWS Support cases can also yield insights into the AWS service and API that are not well documented.","title":"Infrastructure as Code Suitability"},{"location":"provider-design/#resource-type-considerations","text":"Terraform resources work best as the smallest infrastructure blocks on which practitioners can build more complex configurations and abstractions, such as Terraform Modules . The general heuristic guiding when to implement a new Terraform resource for an aspect of AWS is whether the AWS service API provides create, read, update, and delete (CRUD) operations. However, not all AWS service API functionality falls cleanly into CRUD lifecycle management. In these situations, there is extra consideration necessary for properly mapping API operations to Terraform resources. This section highlights design patterns when to consider an implementation within a singular Terraform resource or as separate Terraform resources. Please note: the overall design and implementation across all AWS functionality is federated: individual services may implement concepts and use terminology differently. As such, this guide is not exhaustive. The aim is to provide general concepts and basic terminology that points contributors in the right direction, especially in understanding previous implementations.","title":"Resource Type Considerations"},{"location":"provider-design/#authorization-and-acceptance-resources","text":"Some AWS services use an authorization-acceptance model for cross-account associations or access. Examples include: Direct Connect Association Proposals GuardDuty Member Invitations RAM Resource Share Associations Route 53 VPC Associations Security Hub Member Invitations Depending on the API and components, AWS uses two basic ways of creating cross-region and cross-account associations. One way is to generate an invitation (or proposal) identifier from one AWS account to another. Then in the other AWS account, that identifier is used to accept the invitation. The second way is configuring a reference to another AWS account identifier. These may not require explicit acceptance on the receiving account to finish creating the association or begin working. To model creating an association using an invitation or proposal, follow these guidelines. Follow the naming in the AWS service API to determine whether to use the term \"invitation\" or \"proposal.\" For the originating account, create an \"invitation\" or \"proposal\" resource. Make sure that the AWS service API has operations for creating and reading invitations. For the responding account, create an \"accepter\" resource. Ensure that the API has operations for accepting, reading, and rejecting invitations in the responding account. Map the operations as follows: Create: Accepts the invitation. Read: Reads the invitation to determine its status. Note that in some APIs, invitations expire and disappear, complicating associations. If a resource does not find an invitation, the developer should implement a fall back to read the API resource associated with the invitation/proposal. Delete: Rejects or otherwise deletes the invitation. To model the second type of association, implicit associations, create an \"association\" resource and, optionally, an \"authorization\" resource. Map create, read, and delete to the corresponding operations in the AWS service API.","title":"Authorization and Acceptance Resources"},{"location":"provider-design/#cross-service-functionality","text":"Many AWS service APIs build on top of other AWS services. Some examples of these include: EKS Node Groups managing Auto Scaling Groups Lambda Functions managing EC2 ENIs Transfer Servers managing EC2 VPC Endpoints Some cross-service API implementations lack the management or description capabilities of the other service. The lack can make the Terraform resource implementation seem incomplete or unsuccessful in end-to-end configurations. Given the overall \u201cresources should represent a single API object\u201d goal from the HashiCorp Provider Design Principles , a resource must only communicate with a single AWS service API. As such, maintainers will not approve cross-service resources. The rationale behind this design decision includes the following: Unexpected IAM permissions being necessary for the resource. In high-security environments, all the service permissions may not be available or acceptable. Unexpected services generating CloudTrail logs for the resource. Needing extra and unexpected API endpoints configuration for organizations using custom endpoints, such as VPC endpoints. Unexpected changes to the AWS service internals for the cross-service implementations. Given that this functionality is not part of the primary service API, these details can change over time and may not be considered as a breaking change by the service team for an API upgrade. A poignant real-world example of the last point involved a Lambda resource. The resource helped clean up extra resources (ENIs) due to a common misconfiguration. Practitioners found the functionality helpful since the issue was hard to diagnose. Years later, AWS updated the Lambda API. Immediately, practitioners reported that Terraform executions were failing. Downgrading the provider was not possible since many configurations depended on recent releases. For environments running many versions behind, forcing an upgrade with the fix would likely cause unrelated and unexpected changes. In the end, HashiCorp and AWS performed a large-scale outreach to help upgrade and fixing the misconfigurations. Provider maintainers and practitioners lost considerable time.","title":"Cross-Service Functionality"},{"location":"provider-design/#data-sources","text":"A separate class of Terraform resource types are data sources . These are typically intended as a configuration method to lookup or fetch data in a read-only manner. Data sources should not have side effects on the remote system. When discussing data sources, they are typically classified by the intended number of return objects or data. Singular data sources represent a one-to-one lookup or data operation. Plural data sources represent a one-to-many operation.","title":"Data Sources"},{"location":"provider-design/#plural-data-sources","text":"These data sources are intended to return zero, one, or many results, usually associated with a managed resource type. Typically results are a set unless ordering guarantees are provided by the remote system. These should be named with a plural suffix (e.g., s or es ) and should not include any specific attribute in the naming (e.g., prefer aws_ec2_transit_gateways instead of aws_ec2_transit_gateway_ids ).","title":"Plural Data Sources"},{"location":"provider-design/#singular-data-sources","text":"These data sources are intended to return one result or an error. These should not include any specific attribute in the naming (e.g., prefer aws_ec2_transit_gateway instead of aws_ec2_transit_gateway_id ).","title":"Singular Data Sources"},{"location":"provider-design/#iam-resource-based-policy-resources","text":"For some AWS components, the AWS API allows specifying an IAM resource-based policy , the IAM policy to associate with a component. Some examples include: ECR Repository Policies EFS File System Policies SNS Topic Policies Provider developers should implement this capability in a new resource rather than adding it to the associated resource. Reasons for this include: Many of the policies must include the ARN of the resource. Working around this requirement with custom difference handling within a self-contained resource is unnecessarily cumbersome. Some policies involving multiple resources need to cross-reference each other's ARNs. Without a separate resource, this introduces a configuration cycle. Splitting the resources allows operators to logically split their configurations into purely operational and security boundaries. This allows environments to have distinct practitioners roles and permissions for IAM versus infrastructure changes. One rare exception to this guideline is where the policy is required during resource creation.","title":"IAM Resource-Based Policy Resources"},{"location":"provider-design/#managing-resource-running-state","text":"The AWS API provides the ability to start, stop, enable, or disable some AWS components. Some examples include: Batch Job Queues CloudFront Distributions RDS DB Event Subscriptions In this situation, provider developers should implement this ability within the resource instead of creating a separate resource. Since a practitioner cannot practically manage interaction with a resource's states in Terraform's declarative configuration, developers should implement the state management in the resource. This design provides consistency and future-proofing even where updating a resource in the current API is not problematic.","title":"Managing Resource Running State"},{"location":"provider-design/#task-execution-and-waiter-resources","text":"Some AWS operations are asynchronous. Terraform requests that AWS perform a task. Initially, AWS only notifies Terraform that it received the request. Terraform then requests the status while awaiting completion. Examples of this include: ACM Certificate validation EC2 AMI copying RDS DB Cluster Snapshot management In this situation, provider developers should create a separate resource representing the task, assuming that the AWS service API provides operations to start the task and read its status. Adding the task functionality to the parent resource muddies its infrastructure-management purpose. The maintainers prefer this approach even though there is some duplication of an existing resource. For example, the provider has a resource for copying an EC2 AMI in addition to the EC2 AMI resource itself. This modularity allows practitioners to manage the result of the task resource with another resource. For a related consideration, see the Managing Resource Running State section .","title":"Task Execution and Waiter Resources"},{"location":"provider-design/#versioned-resources","text":"AWS supports having multiple versions of some components. Examples of this include: ECS Task Definitions Lambda Functions Secrets Manager Secrets In general, provider developers should create a separate resource to represent a single version. For example, the provider has both the aws_secretsmanager_secret and aws_secretsmanager_secret_version resources. However, in some cases, developers should handle versioning in the main resource. In deciding when to create a separate resource, follow these guidelines: If AWS necessarily creates a version when you make a new AWS component, include version handling in the same Terraform resource. Creating an AWS component with one Terraform resource and later using a different resource for updates is confusing. If the AWS service API allows deleting versions and practitioners will want to delete versions, provider developers should implement a separate version resource. If the API only supports publishing new versions, either method is acceptable, however most current implementations are self-contained. Terraform's current configuration language does not natively support triggering resource updates or recreation across resources without a state value change. This can make the implementation more difficult for practitioners without special resource and configuration workarounds, such as a triggers attribute. If this changes in the future, then this guidance may be updated towards separate resources, following the Task Execution and Waiter Resources guidance.","title":"Versioned Resources"},{"location":"provider-design/#other-considerations","text":"","title":"Other Considerations"},{"location":"provider-design/#aws-credential-exfiltration","text":"In the interest of security, the maintainers will not approve data sources that provide the ability to reference or export the AWS credentials of the running provider. There are valid use cases for this information, such as to execute AWS CLI calls as part of the same Terraform configuration. However, this mechanism may allow credentials to be discovered and used outside of Terraform. Some specific concerns include: The values may be visible in Terraform user interface output or logging, allowing anyone with user interface or log access to see the credentials. The values are currently stored in plaintext in the Terraform state, allowing anyone with access to the state file or another Terraform configuration that references the state access to the credentials. Any new related functionality, while opt-in to implement, is also opt-in to prevent via security controls or policies. Adopting a weaker default security posture requires advance notice and prevents organizations that implement those controls from updating to a version with any such functionality.","title":"AWS Credential Exfiltration"},{"location":"raising-a-pull-request/","text":"Raising a Pull Request # Fork the GitHub repository allowing you to make the changes in your own copy of the repository. Create a branch using the following naming prefixes: f = feature b = bug fix d = documentation t = tests td = technical debt v = dependencies (\"vendoring\" previously) Some indicative example branch names would be f-aws_emr_instance_group-refactor or td-staticcheck-st1008 Make the changes you would like to include in the provider, add new tests as required, and make sure that all relevant existing tests are passing. Create a changelog entry following the process outlined here Create a pull request . Please ensure (if possible) the 'Allow edits from maintainers' checkbox is checked. This will allow the maintainers to make changes and merge the PR without requiring action from the contributor. You are welcome to submit your pull request for commentary or review before it is fully completed by creating a draft pull request . Please include specific questions or items you'd like feedback on. Once you believe your pull request is ready to be reviewed, ensure the pull request is not a draft pull request by marking it ready for review or removing [WIP] from the pull request title if necessary, and a maintainer will review it. Follow the checklists below to help ensure that your contribution can be easily reviewed and potentially merged. One of Terraform's provider team members will look over your contribution and either approve it or provide comments letting you know if there is anything left to do. We'll try give you the opportunity to make the required changes yourself, but in some cases we may perform the changes ourselves if it makes sense to (minor changes, or for urgent issues). We do our best to keep up with the volume of PRs waiting for review, but it may take some time depending on the complexity of the work. Once all outstanding comments and checklist items have been addressed, your contribution will be merged! Merged PRs will be included in the next Terraform release. In some cases, we might decide that a PR should be closed without merging. We'll make sure to provide clear reasoning when this happens. Go Coding Style # All Go code is automatically checked for compliance with various linters, such as gofmt . These tools can be installed using the GNUMakefile in this repository. $ cd terraform-provider-aws $ make tools Check your code with the linters: $ make lint We use Semgrep to check for other code standards. This can be run directly on the command line, i.e., $ semgrep or it can be run using Docker via the Makefile, i.e., $ make semgrep gofmt will also fix many simple formatting issues for you. The Makefile includes a target for this: $ make fmt The import statement in a Go file follows these rules (see #15903 ): Import declarations are grouped into a maximum of three groups with the following order: Standard packages (also called short import path or built-in packages) Third-party packages (also called long import path packages) Local packages Groups are separated by a single blank line Packages within each group are alphabetized Check your imports: $ make importlint For greater detail, the following Go language resources provide common coding preferences that may be referenced during review, if not automatically handled by the project's linting tools. Effective Go Go Code Review Comments Resource Contribution Guidelines # The following resource checks need to be addressed before your contribution can be merged. The exclusion of any applicable check may result in a delayed time to merge. Some of these are not handled by the automated code testing that occurs during submission, so reviewers (even those outside the maintainers) are encouraged to reach out to contributors about any issues to save time. This Contribution Guide also includes separate sections on topics such as Error Handling , which also applies to contributions. Passes Testing : All code and documentation changes must pass unit testing, code linting, and website link testing. Resource code changes must pass all acceptance testing for the resource. Avoids API Calls Across Account, Region, and Service Boundaries : Resources should not implement cross-account, cross-region, or cross-service API calls. Does Not Set Optional or Required for Non-Configurable Attributes : Resource schema definitions for read-only attributes must not include Optional: true or Required: true . Avoids resource.Retry() without resource.RetryableError() : Resource logic should only implement resource.Retry() if there is a retryable condition (e.g., return resource.RetryableError(err) ). Avoids Reusing Resource Read Function in Data Source Read Function : Data sources should fully implement their own resource Read functionality including duplicating d.Set() calls. Avoids Reading Schema Structure in Resource Code : The resource Schema should not be read in resource Create / Read / Update / Delete functions to perform looping or otherwise complex attribute logic. Use d.Get() and d.Set() directly with individual attributes instead. Avoids ResourceData.GetOkExists() : Resource logic should avoid using ResourceData.GetOkExists() as its expected functionality is not guaranteed in all scenarios. Calls Read After Create and Update : Except where API eventual consistency prohibits immediate reading of resources or updated attributes, resource Create and Update functions should return the resource Read function. Implements Immediate Resource ID Set During Create : Immediately after calling the API creation function, the resource ID should be set with d.SetId() before other API operations or returning the Read function. Implements Attribute Refreshes During Read : All attributes available in the API should have d.Set() called their values in the Terraform state during the Read function. Performs Error Checks with Non-Primitive Attribute Refreshes : When using d.Set() with non-primitive types ( schema.TypeList , schema.TypeSet , or schema.TypeMap ), perform error checking to prevent issues where the code is not properly able to refresh the Terraform state . Implements Import Acceptance Testing and Documentation : Support for resource import ( Importer in resource schema) must include ImportState acceptance testing (see also the Acceptance Testing Guidelines ) and ## Import section in resource documentation. Implements Customizable Timeouts Documentation : Support for customizable timeouts ( Timeouts in resource schema) must include ## Timeouts section in resource documentation. Implements State Migration When Adding New Virtual Attribute : For new \"virtual\" attributes (those only in Terraform and not in the API), the schema should implement State Migration to prevent differences for existing configurations that upgrade. Uses AWS Go SDK Constants : Many AWS services provide string constants for value enumerations, error codes, and status types. See also the \"Constants\" sections under each of the service packages in the AWS Go SDK documentation . Uses AWS Go SDK Pointer Conversion Functions : Many APIs return pointer types and these functions return the zero value for the type if the pointer is nil . This prevents potential panics from unchecked * pointer dereferences and can eliminate boilerplate nil checking in many cases. See also the aws package in the AWS Go SDK documentation . Uses AWS Go SDK Types : Use available SDK structs instead of implementing custom types with indirection. Uses Existing Validation Functions : Schema definitions including ValidateFunc for attribute validation should use available Terraform helper/validation package functions. All() / Any() can be used for combining multiple validation function behaviors. Uses tfresource.TimedOut() with resource.Retry() : Resource logic implementing resource.Retry() should error check with tfresource.TimedOut(err error) and potentially unset the error before returning the error. For example: var output * kms . CreateKeyOutput err := resource . Retry ( 1 * time . Minute , func () * resource . RetryError { var err error output , err = conn . CreateKey ( input ) /* ... */ return nil }) if tfresource . TimedOut ( err ) { output , err = conn . CreateKey ( input ) } if err != nil { return fmt . Errorf ( \"error creating KMS External Key: %s\" , err ) } Uses resource.UniqueId() : API fields for concurrency protection such as CallerReference and IdempotencyToken should use resource.UniqueId() . The implementation includes a monotonic counter which is safer for concurrent operations than solutions such as time.Now() . Skips id Attribute : The id attribute is implicit for all Terraform resources and does not need to be defined in the schema. The below are style-based items that may be noted during review and are recommended for simplicity, consistency, and quality assurance: Implements arn Attribute : APIs that return an ARN should implement arn as an attribute. Alternatively, the ARN can be synthesized using the AWS Go SDK arn.ARN structure. For example: // Direct Connect Virtual Interface ARN. // See https://docs.aws.amazon.com/directconnect/latest/UserGuide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-resources. arn := arn . ARN { Partition : meta .( * AWSClient ). partition , Region : meta .( * AWSClient ). region , Service : \"directconnect\" , AccountID : meta .( * AWSClient ). accountid , Resource : fmt . Sprintf ( \"dxvif/%s\" , d . Id ()), }. String () d . Set ( \"arn\" , arn ) When the arn attribute is synthesized this way, add the resource to the list of those affected by the provider's skip_requesting_account_id attribute. Implements Warning Logging With Resource State Removal : If a resource is removed outside of Terraform (e.g., via different tool, API, or web UI), d.SetId(\"\") and return nil can be used in the resource Read function to trigger resource recreation. When this occurs, a warning log message should be printed beforehand: log.Printf(\"[WARN] {SERVICE} {THING} (%s) not found, removing from state\", d.Id()) Uses American English for Attribute Naming : For any ambiguity with attribute naming, prefer American English over British English. e.g., color instead of colour . Skips Timestamp Attributes : Generally, creation and modification dates from the API should be omitted from the schema. Uses Paginated AWS Go SDK Functions When Iterating Over a Collection of Objects : When the API for listing a collection of objects provides a paginated function, use it instead of looping until the next page token is not set. For example, with the EC2 API, DescribeInstancesPages should be used instead of DescribeInstances when more than one result is expected. Adds Paginated Functions Missing from the AWS Go SDK to Internal Service Package : If the AWS Go SDK does not define a paginated equivalent for a function to list a collection of objects, it should be added to a per-service internal package using the listpages generator . A support case should also be opened with AWS to have the paginated functions added to the AWS Go SDK.","title":"Raising a Pull Request"},{"location":"raising-a-pull-request/#raising-a-pull-request","text":"Fork the GitHub repository allowing you to make the changes in your own copy of the repository. Create a branch using the following naming prefixes: f = feature b = bug fix d = documentation t = tests td = technical debt v = dependencies (\"vendoring\" previously) Some indicative example branch names would be f-aws_emr_instance_group-refactor or td-staticcheck-st1008 Make the changes you would like to include in the provider, add new tests as required, and make sure that all relevant existing tests are passing. Create a changelog entry following the process outlined here Create a pull request . Please ensure (if possible) the 'Allow edits from maintainers' checkbox is checked. This will allow the maintainers to make changes and merge the PR without requiring action from the contributor. You are welcome to submit your pull request for commentary or review before it is fully completed by creating a draft pull request . Please include specific questions or items you'd like feedback on. Once you believe your pull request is ready to be reviewed, ensure the pull request is not a draft pull request by marking it ready for review or removing [WIP] from the pull request title if necessary, and a maintainer will review it. Follow the checklists below to help ensure that your contribution can be easily reviewed and potentially merged. One of Terraform's provider team members will look over your contribution and either approve it or provide comments letting you know if there is anything left to do. We'll try give you the opportunity to make the required changes yourself, but in some cases we may perform the changes ourselves if it makes sense to (minor changes, or for urgent issues). We do our best to keep up with the volume of PRs waiting for review, but it may take some time depending on the complexity of the work. Once all outstanding comments and checklist items have been addressed, your contribution will be merged! Merged PRs will be included in the next Terraform release. In some cases, we might decide that a PR should be closed without merging. We'll make sure to provide clear reasoning when this happens.","title":"Raising a Pull Request"},{"location":"raising-a-pull-request/#go-coding-style","text":"All Go code is automatically checked for compliance with various linters, such as gofmt . These tools can be installed using the GNUMakefile in this repository. $ cd terraform-provider-aws $ make tools Check your code with the linters: $ make lint We use Semgrep to check for other code standards. This can be run directly on the command line, i.e., $ semgrep or it can be run using Docker via the Makefile, i.e., $ make semgrep gofmt will also fix many simple formatting issues for you. The Makefile includes a target for this: $ make fmt The import statement in a Go file follows these rules (see #15903 ): Import declarations are grouped into a maximum of three groups with the following order: Standard packages (also called short import path or built-in packages) Third-party packages (also called long import path packages) Local packages Groups are separated by a single blank line Packages within each group are alphabetized Check your imports: $ make importlint For greater detail, the following Go language resources provide common coding preferences that may be referenced during review, if not automatically handled by the project's linting tools. Effective Go Go Code Review Comments","title":"Go Coding Style"},{"location":"raising-a-pull-request/#resource-contribution-guidelines","text":"The following resource checks need to be addressed before your contribution can be merged. The exclusion of any applicable check may result in a delayed time to merge. Some of these are not handled by the automated code testing that occurs during submission, so reviewers (even those outside the maintainers) are encouraged to reach out to contributors about any issues to save time. This Contribution Guide also includes separate sections on topics such as Error Handling , which also applies to contributions. Passes Testing : All code and documentation changes must pass unit testing, code linting, and website link testing. Resource code changes must pass all acceptance testing for the resource. Avoids API Calls Across Account, Region, and Service Boundaries : Resources should not implement cross-account, cross-region, or cross-service API calls. Does Not Set Optional or Required for Non-Configurable Attributes : Resource schema definitions for read-only attributes must not include Optional: true or Required: true . Avoids resource.Retry() without resource.RetryableError() : Resource logic should only implement resource.Retry() if there is a retryable condition (e.g., return resource.RetryableError(err) ). Avoids Reusing Resource Read Function in Data Source Read Function : Data sources should fully implement their own resource Read functionality including duplicating d.Set() calls. Avoids Reading Schema Structure in Resource Code : The resource Schema should not be read in resource Create / Read / Update / Delete functions to perform looping or otherwise complex attribute logic. Use d.Get() and d.Set() directly with individual attributes instead. Avoids ResourceData.GetOkExists() : Resource logic should avoid using ResourceData.GetOkExists() as its expected functionality is not guaranteed in all scenarios. Calls Read After Create and Update : Except where API eventual consistency prohibits immediate reading of resources or updated attributes, resource Create and Update functions should return the resource Read function. Implements Immediate Resource ID Set During Create : Immediately after calling the API creation function, the resource ID should be set with d.SetId() before other API operations or returning the Read function. Implements Attribute Refreshes During Read : All attributes available in the API should have d.Set() called their values in the Terraform state during the Read function. Performs Error Checks with Non-Primitive Attribute Refreshes : When using d.Set() with non-primitive types ( schema.TypeList , schema.TypeSet , or schema.TypeMap ), perform error checking to prevent issues where the code is not properly able to refresh the Terraform state . Implements Import Acceptance Testing and Documentation : Support for resource import ( Importer in resource schema) must include ImportState acceptance testing (see also the Acceptance Testing Guidelines ) and ## Import section in resource documentation. Implements Customizable Timeouts Documentation : Support for customizable timeouts ( Timeouts in resource schema) must include ## Timeouts section in resource documentation. Implements State Migration When Adding New Virtual Attribute : For new \"virtual\" attributes (those only in Terraform and not in the API), the schema should implement State Migration to prevent differences for existing configurations that upgrade. Uses AWS Go SDK Constants : Many AWS services provide string constants for value enumerations, error codes, and status types. See also the \"Constants\" sections under each of the service packages in the AWS Go SDK documentation . Uses AWS Go SDK Pointer Conversion Functions : Many APIs return pointer types and these functions return the zero value for the type if the pointer is nil . This prevents potential panics from unchecked * pointer dereferences and can eliminate boilerplate nil checking in many cases. See also the aws package in the AWS Go SDK documentation . Uses AWS Go SDK Types : Use available SDK structs instead of implementing custom types with indirection. Uses Existing Validation Functions : Schema definitions including ValidateFunc for attribute validation should use available Terraform helper/validation package functions. All() / Any() can be used for combining multiple validation function behaviors. Uses tfresource.TimedOut() with resource.Retry() : Resource logic implementing resource.Retry() should error check with tfresource.TimedOut(err error) and potentially unset the error before returning the error. For example: var output * kms . CreateKeyOutput err := resource . Retry ( 1 * time . Minute , func () * resource . RetryError { var err error output , err = conn . CreateKey ( input ) /* ... */ return nil }) if tfresource . TimedOut ( err ) { output , err = conn . CreateKey ( input ) } if err != nil { return fmt . Errorf ( \"error creating KMS External Key: %s\" , err ) } Uses resource.UniqueId() : API fields for concurrency protection such as CallerReference and IdempotencyToken should use resource.UniqueId() . The implementation includes a monotonic counter which is safer for concurrent operations than solutions such as time.Now() . Skips id Attribute : The id attribute is implicit for all Terraform resources and does not need to be defined in the schema. The below are style-based items that may be noted during review and are recommended for simplicity, consistency, and quality assurance: Implements arn Attribute : APIs that return an ARN should implement arn as an attribute. Alternatively, the ARN can be synthesized using the AWS Go SDK arn.ARN structure. For example: // Direct Connect Virtual Interface ARN. // See https://docs.aws.amazon.com/directconnect/latest/UserGuide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-resources. arn := arn . ARN { Partition : meta .( * AWSClient ). partition , Region : meta .( * AWSClient ). region , Service : \"directconnect\" , AccountID : meta .( * AWSClient ). accountid , Resource : fmt . Sprintf ( \"dxvif/%s\" , d . Id ()), }. String () d . Set ( \"arn\" , arn ) When the arn attribute is synthesized this way, add the resource to the list of those affected by the provider's skip_requesting_account_id attribute. Implements Warning Logging With Resource State Removal : If a resource is removed outside of Terraform (e.g., via different tool, API, or web UI), d.SetId(\"\") and return nil can be used in the resource Read function to trigger resource recreation. When this occurs, a warning log message should be printed beforehand: log.Printf(\"[WARN] {SERVICE} {THING} (%s) not found, removing from state\", d.Id()) Uses American English for Attribute Naming : For any ambiguity with attribute naming, prefer American English over British English. e.g., color instead of colour . Skips Timestamp Attributes : Generally, creation and modification dates from the API should be omitted from the schema. Uses Paginated AWS Go SDK Functions When Iterating Over a Collection of Objects : When the API for listing a collection of objects provides a paginated function, use it instead of looping until the next page token is not set. For example, with the EC2 API, DescribeInstancesPages should be used instead of DescribeInstances when more than one result is expected. Adds Paginated Functions Missing from the AWS Go SDK to Internal Service Package : If the AWS Go SDK does not define a paginated equivalent for a function to list a collection of objects, it should be added to a per-service internal package using the listpages generator . A support case should also be opened with AWS to have the paginated functions added to the AWS Go SDK.","title":"Resource Contribution Guidelines"},{"location":"resource-filtering/","text":"Adding Resource Filtering Support # AWS provides server-side filtering across many services and resources, which can be used when listing resources of that type, for example in the implementation of a data source. See the EC2 Listing and filtering your resources page for information about how server-side filtering can be used with EC2 resources. To determine if the supporting AWS API supports this functionality: Open the AWS Go SDK documentation for the service, e.g., for service/rds . Note: there can be a delay between the AWS announcement and the updated AWS Go SDK documentation. Determine if the service API includes functionality for filtering resources (usually a Filters argument to a DescribeThing API call). Implementing server-side filtering support for Terraform AWS Provider resources requires the following, each with its own section below: Generated Service Filtering Code : In the internal code generators (e.g., internal/generate/namevaluesfilters ), implementation and customization of how a service handles filtering, which is standardized for the resources. Resource Filtering Code Implementation : In the resource's equivalent data source code (e.g., internal/service/{servicename}/thing_data_source.go ), implementation of filter schema attribute, along with handling in the Read function. Resource Filtering Documentation Implementation : In the resource's equivalent data source documentation (e.g., website/docs/d/service_thing.html.markdown ), addition of filter argument Adding Service to Filter Generating Code # This step is only necessary for the first implementation and may have been previously completed. If so, move on to the next section. More details about this code generation can be found in the namevaluesfilters documentation . Add the AWS Go SDK service name (e.g., rds ) to sliceServiceNames in internal/generate/namevaluesfilters/generators/servicefilters/main.go . Run make gen ( go generate ./... ) and ensure there are no errors via make test ( go test ./... ) Resource Filter Code Implementation # In the resource's equivalent data source Go file (e.g., internal/service/ec2/internet_gateway_data_source.go ), add the following Go import: \"github.com/hashicorp/terraform-provider-aws/internal/generate/namevaluesfilters\" In the resource schema, add \"filter\": namevaluesfilters.Schema(), Implement the logic to build the list of filters: input := & ec2 . DescribeInternetGatewaysInput {} // Filters based on attributes. filters := namevaluesfilters . New ( map [ string ] string { \"internet-gateway-id\" : d . Get ( \"internet_gateway_id\" ).( string ), }) // Add filters based on keyvalue tags (N.B. Not applicable to all AWS services that support filtering) filters . Add ( namevaluesfilters . EC2Tags ( keyvaluetags . New ( d . Get ( \"tags\" ).( map [ string ] interface {})). IgnoreAWS (). IgnoreConfig ( ignoreTagsConfig ). Map ())) // Add filters based on the custom filtering \"filter\" attribute. filters . Add ( d . Get ( \"filter\" ).( * schema . Set )) input . Filters = filters . EC2Filters () Resource Filtering Documentation Implementation # In the resource's equivalent data source documentation (e.g., website/docs/d/internet_gateway.html.markdown ), add the following to the arguments reference: * `filter` - (Optional) Custom filter block as described below. More complex filters can be expressed using one or more `filter` sub-blocks, which take the following arguments: * `name` - (Required) Name of the field to filter by, as defined by [ the underlying AWS API ]( https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInternetGateways.html ). * `values` - (Required) Set of values that are accepted for the given field. An Internet Gateway will be selected if any one of the given values matches.","title":"Resource Filtering"},{"location":"resource-filtering/#adding-resource-filtering-support","text":"AWS provides server-side filtering across many services and resources, which can be used when listing resources of that type, for example in the implementation of a data source. See the EC2 Listing and filtering your resources page for information about how server-side filtering can be used with EC2 resources. To determine if the supporting AWS API supports this functionality: Open the AWS Go SDK documentation for the service, e.g., for service/rds . Note: there can be a delay between the AWS announcement and the updated AWS Go SDK documentation. Determine if the service API includes functionality for filtering resources (usually a Filters argument to a DescribeThing API call). Implementing server-side filtering support for Terraform AWS Provider resources requires the following, each with its own section below: Generated Service Filtering Code : In the internal code generators (e.g., internal/generate/namevaluesfilters ), implementation and customization of how a service handles filtering, which is standardized for the resources. Resource Filtering Code Implementation : In the resource's equivalent data source code (e.g., internal/service/{servicename}/thing_data_source.go ), implementation of filter schema attribute, along with handling in the Read function. Resource Filtering Documentation Implementation : In the resource's equivalent data source documentation (e.g., website/docs/d/service_thing.html.markdown ), addition of filter argument","title":"Adding Resource Filtering Support"},{"location":"resource-filtering/#adding-service-to-filter-generating-code","text":"This step is only necessary for the first implementation and may have been previously completed. If so, move on to the next section. More details about this code generation can be found in the namevaluesfilters documentation . Add the AWS Go SDK service name (e.g., rds ) to sliceServiceNames in internal/generate/namevaluesfilters/generators/servicefilters/main.go . Run make gen ( go generate ./... ) and ensure there are no errors via make test ( go test ./... )","title":"Adding Service to Filter Generating Code"},{"location":"resource-filtering/#resource-filter-code-implementation","text":"In the resource's equivalent data source Go file (e.g., internal/service/ec2/internet_gateway_data_source.go ), add the following Go import: \"github.com/hashicorp/terraform-provider-aws/internal/generate/namevaluesfilters\" In the resource schema, add \"filter\": namevaluesfilters.Schema(), Implement the logic to build the list of filters: input := & ec2 . DescribeInternetGatewaysInput {} // Filters based on attributes. filters := namevaluesfilters . New ( map [ string ] string { \"internet-gateway-id\" : d . Get ( \"internet_gateway_id\" ).( string ), }) // Add filters based on keyvalue tags (N.B. Not applicable to all AWS services that support filtering) filters . Add ( namevaluesfilters . EC2Tags ( keyvaluetags . New ( d . Get ( \"tags\" ).( map [ string ] interface {})). IgnoreAWS (). IgnoreConfig ( ignoreTagsConfig ). Map ())) // Add filters based on the custom filtering \"filter\" attribute. filters . Add ( d . Get ( \"filter\" ).( * schema . Set )) input . Filters = filters . EC2Filters ()","title":"Resource Filter Code Implementation"},{"location":"resource-filtering/#resource-filtering-documentation-implementation","text":"In the resource's equivalent data source documentation (e.g., website/docs/d/internet_gateway.html.markdown ), add the following to the arguments reference: * `filter` - (Optional) Custom filter block as described below. More complex filters can be expressed using one or more `filter` sub-blocks, which take the following arguments: * `name` - (Required) Name of the field to filter by, as defined by [ the underlying AWS API ]( https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInternetGateways.html ). * `values` - (Required) Set of values that are accepted for the given field. An Internet Gateway will be selected if any one of the given values matches.","title":"Resource Filtering Documentation Implementation"},{"location":"resource-name-generation/","text":"Adding Resource Name Generation Support # Terraform AWS Provider resources can use shared logic to support and test name generation, where the operator can choose between an expected naming value, a generated naming value with a prefix, or a fully generated name. Implementing name generation support for Terraform AWS Provider resources requires the following, each with its own section below: Resource Name Generation Code Implementation : In the resource code (e.g., internal/service/{service}/{thing}.go ), implementation of name_prefix attribute, along with handling in Create function. Resource Name Generation Testing Implementation : In the resource acceptance testing (e.g., internal/service/{service}/{thing}_test.go ), implementation of new acceptance test functions and configurations to exercise new naming logic. Resource Name Generation Documentation Implementation : In the resource documentation (e.g., website/docs/r/service_thing.html.markdown ), addition of name_prefix argument and update of name argument description. Resource name generation code implementation # In the resource Go file (e.g., internal/service/{service}/{thing}.go ), add the following Go import: \"github.com/hashicorp/terraform-provider-aws/internal/create\" In the resource schema, add the new name_prefix attribute and adjust the name attribute to be Optional , Computed , and ConflictsWith the name_prefix attribute. Ensure to keep any existing schema fields on name such as ValidateFunc . E.g. \"name\" : { Type : schema . TypeString , Optional : true , Computed : true , ForceNew : true , ConflictsWith : [] string { \"name_prefix\" }, }, \"name_prefix\" : { Type : schema . TypeString , Optional : true , Computed : true , ForceNew : true , ConflictsWith : [] string { \"name\" }, }, In the resource Create function, switch any calls from d.Get(\"name\").(string) to instead use the create.Name() function, e.g. name := create . Name ( d . Get ( \"name\" ).( string ), d . Get ( \"name_prefix\" ).( string )) // ... in AWS Go SDK Input types, etc. use aws.String(name) If the resource supports import, in the resource Read function add a call to d.Set(\"name_prefix\", ...) , e.g. d . Set ( \"name\" , resp . Name ) d . Set ( \"name_prefix\" , create . NamePrefixFromName ( aws . StringValue ( resp . Name ))) Resource name generation testing implementation # In the resource testing (e.g., internal/service/{service}/{thing}_test.go ), add the following Go import: \"github.com/hashicorp/terraform-provider-aws/internal/create\" In the resource testing, implement two new tests named _Name_Generated and _NamePrefix with associated configurations, that verifies creating the resource without name and name_prefix arguments (for the former) and with only the name_prefix argument (for the latter). E.g. func TestAccServiceThing_nameGenerated ( t * testing . T ) { var thing service . ServiceThing resourceName := \"aws_service_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckThingDestroy , Steps : [] resource . TestStep { { Config : testAccThingConfig_nameGenerated (), Check : resource . ComposeTestCheckFunc ( testAccCheckThingExists ( resourceName , & thing ), acctest . CheckResourceAttrNameGenerated ( resourceName , \"name\" ), resource . TestCheckResourceAttr ( resourceName , \"name_prefix\" , resource . UniqueIdPrefix ), ), }, // If the resource supports import: { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func TestAccServiceThing_namePrefix ( t * testing . T ) { var thing service . ServiceThing resourceName := \"aws_service_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckThingDestroy , Steps : [] resource . TestStep { { Config : testAccThingConfig_namePrefix ( \"tf-acc-test-prefix-\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckThingExists ( resourceName , & thing ), acctest . CheckResourceAttrNameFromPrefix ( resourceName , \"name\" , \"tf-acc-test-prefix-\" ), resource . TestCheckResourceAttr ( resourceName , \"name_prefix\" , \"tf-acc-test-prefix-\" ), ), }, // If the resource supports import: { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func testAccThingConfig_nameGenerated () string { return fmt . Sprintf ( ` resource \"aws_service_thing\" \"test\" { # ... other configuration ... } ` ) } func testAccThingConfig_namePrefix ( namePrefix string ) string { return fmt . Sprintf ( ` resource \"aws_service_thing\" \"test\" { # ... other configuration ... name_prefix = %[1]q } ` , namePrefix ) } Resource name generation documentation implementation # In the resource documentation (e.g., website/docs/r/service_thing.html.markdown ), add the following to the arguments reference: * `name_prefix` - (Optional) Creates a unique name beginning with the specified prefix. Conflicts with `name` . Adjust the existing name argument reference to ensure its denoted as Optional , includes a mention that it can be generated, and that it conflicts with name_prefix : * `name` - (Optional) Name of the thing. If omitted, Terraform will assign a random, unique name. Conflicts with `name_prefix` . Resource name generation with suffix # Some generated resource names require a fixed suffix (for example Amazon SNS FIFO topic names must end in .fifo ). In these cases use create.NameWithSuffix() in the resource Create function and create.NamePrefixFromNameWithSuffix() in the resource Read function, e.g. name := create . NameWithSuffix ( d . Get ( \"name\" ).( string ), d . Get ( \"name_prefix\" ).( string ), \".fifo\" ) and d . Set ( \"name\" , resp . Name ) d . Set ( \"name_prefix\" , create . NamePrefixFromNameWithSuffix ( aws . StringValue ( resp . Name ), \".fifo\" )) There are also functions acctest.CheckResourceAttrNameWithSuffixGenerated and acctest.CheckResourceAttrNameWithSuffixFromPrefix for use in tests.","title":"Resource Name Generation"},{"location":"resource-name-generation/#adding-resource-name-generation-support","text":"Terraform AWS Provider resources can use shared logic to support and test name generation, where the operator can choose between an expected naming value, a generated naming value with a prefix, or a fully generated name. Implementing name generation support for Terraform AWS Provider resources requires the following, each with its own section below: Resource Name Generation Code Implementation : In the resource code (e.g., internal/service/{service}/{thing}.go ), implementation of name_prefix attribute, along with handling in Create function. Resource Name Generation Testing Implementation : In the resource acceptance testing (e.g., internal/service/{service}/{thing}_test.go ), implementation of new acceptance test functions and configurations to exercise new naming logic. Resource Name Generation Documentation Implementation : In the resource documentation (e.g., website/docs/r/service_thing.html.markdown ), addition of name_prefix argument and update of name argument description.","title":"Adding Resource Name Generation Support"},{"location":"resource-name-generation/#resource-name-generation-code-implementation","text":"In the resource Go file (e.g., internal/service/{service}/{thing}.go ), add the following Go import: \"github.com/hashicorp/terraform-provider-aws/internal/create\" In the resource schema, add the new name_prefix attribute and adjust the name attribute to be Optional , Computed , and ConflictsWith the name_prefix attribute. Ensure to keep any existing schema fields on name such as ValidateFunc . E.g. \"name\" : { Type : schema . TypeString , Optional : true , Computed : true , ForceNew : true , ConflictsWith : [] string { \"name_prefix\" }, }, \"name_prefix\" : { Type : schema . TypeString , Optional : true , Computed : true , ForceNew : true , ConflictsWith : [] string { \"name\" }, }, In the resource Create function, switch any calls from d.Get(\"name\").(string) to instead use the create.Name() function, e.g. name := create . Name ( d . Get ( \"name\" ).( string ), d . Get ( \"name_prefix\" ).( string )) // ... in AWS Go SDK Input types, etc. use aws.String(name) If the resource supports import, in the resource Read function add a call to d.Set(\"name_prefix\", ...) , e.g. d . Set ( \"name\" , resp . Name ) d . Set ( \"name_prefix\" , create . NamePrefixFromName ( aws . StringValue ( resp . Name )))","title":"Resource name generation code implementation"},{"location":"resource-name-generation/#resource-name-generation-testing-implementation","text":"In the resource testing (e.g., internal/service/{service}/{thing}_test.go ), add the following Go import: \"github.com/hashicorp/terraform-provider-aws/internal/create\" In the resource testing, implement two new tests named _Name_Generated and _NamePrefix with associated configurations, that verifies creating the resource without name and name_prefix arguments (for the former) and with only the name_prefix argument (for the latter). E.g. func TestAccServiceThing_nameGenerated ( t * testing . T ) { var thing service . ServiceThing resourceName := \"aws_service_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckThingDestroy , Steps : [] resource . TestStep { { Config : testAccThingConfig_nameGenerated (), Check : resource . ComposeTestCheckFunc ( testAccCheckThingExists ( resourceName , & thing ), acctest . CheckResourceAttrNameGenerated ( resourceName , \"name\" ), resource . TestCheckResourceAttr ( resourceName , \"name_prefix\" , resource . UniqueIdPrefix ), ), }, // If the resource supports import: { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func TestAccServiceThing_namePrefix ( t * testing . T ) { var thing service . ServiceThing resourceName := \"aws_service_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckThingDestroy , Steps : [] resource . TestStep { { Config : testAccThingConfig_namePrefix ( \"tf-acc-test-prefix-\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckThingExists ( resourceName , & thing ), acctest . CheckResourceAttrNameFromPrefix ( resourceName , \"name\" , \"tf-acc-test-prefix-\" ), resource . TestCheckResourceAttr ( resourceName , \"name_prefix\" , \"tf-acc-test-prefix-\" ), ), }, // If the resource supports import: { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func testAccThingConfig_nameGenerated () string { return fmt . Sprintf ( ` resource \"aws_service_thing\" \"test\" { # ... other configuration ... } ` ) } func testAccThingConfig_namePrefix ( namePrefix string ) string { return fmt . Sprintf ( ` resource \"aws_service_thing\" \"test\" { # ... other configuration ... name_prefix = %[1]q } ` , namePrefix ) }","title":"Resource name generation testing implementation"},{"location":"resource-name-generation/#resource-name-generation-documentation-implementation","text":"In the resource documentation (e.g., website/docs/r/service_thing.html.markdown ), add the following to the arguments reference: * `name_prefix` - (Optional) Creates a unique name beginning with the specified prefix. Conflicts with `name` . Adjust the existing name argument reference to ensure its denoted as Optional , includes a mention that it can be generated, and that it conflicts with name_prefix : * `name` - (Optional) Name of the thing. If omitted, Terraform will assign a random, unique name. Conflicts with `name_prefix` .","title":"Resource name generation documentation implementation"},{"location":"resource-name-generation/#resource-name-generation-with-suffix","text":"Some generated resource names require a fixed suffix (for example Amazon SNS FIFO topic names must end in .fifo ). In these cases use create.NameWithSuffix() in the resource Create function and create.NamePrefixFromNameWithSuffix() in the resource Read function, e.g. name := create . NameWithSuffix ( d . Get ( \"name\" ).( string ), d . Get ( \"name_prefix\" ).( string ), \".fifo\" ) and d . Set ( \"name\" , resp . Name ) d . Set ( \"name_prefix\" , create . NamePrefixFromNameWithSuffix ( aws . StringValue ( resp . Name ), \".fifo\" )) There are also functions acctest.CheckResourceAttrNameWithSuffixGenerated and acctest.CheckResourceAttrNameWithSuffixFromPrefix for use in tests.","title":"Resource name generation with suffix"},{"location":"resource-tagging/","text":"Adding Resource Tagging Support # AWS provides key-value metadata across many services and resources, which can be used for a variety of use cases including billing, ownership, and more. See the AWS Tagging Strategy page for more information about tagging at a high level. The Terraform AWS Provider supports default tags configured on the provider in addition to tags configured on the resource. Implementing tagging support for Terraform AWS Provider resources requires the following, each with its own section below: Generated Service Tagging Code : Each service has a generate.go file where generator directives live. Through these directives and their flags, you can customize code generation for the service. You can find the code that the tagging generator generates in a tags_gen.go file in a service, such as internal/service/ec2/tags_gen.go . You should generally not need to edit the generator code itself (i.e., in internal/generate/tags ). Resource Tagging Code Implementation : In the resource code (e.g., internal/service/{service}/{thing}.go ), implementation of tags and tags_all schema attributes, along with implementation of CustomizeDiff in the resource definition and handling in Create , Read , and Update functions. Resource Tagging Acceptance Testing Implementation : In the resource acceptance testing (e.g., internal/service/{service}/{thing}_test.go ), implementation of new acceptance test function and configurations to exercise new tagging logic. Resource Tagging Documentation Implementation : In the resource documentation (e.g., website/docs/r/service_thing.html.markdown ), addition of tags argument and tags_all attributes. Generating Tag Code for a Service # This step is generally only necessary for the first implementation and may have been previously completed. More details about this code generation, including fixes for potential error messages in this process, can be found in the generate package documentation . The generator will create several types of tagging-related code. All services that support tagging will generate the function KeyValueTags , which converts from service-specific structs returned by the AWS SDK into a common format used by the provider, and the function Tags , which converts from the common format back to the service-specific structs. In addition, many services have separate functions to list or update tags, so the corresponding ListTags and UpdateTags can be generated. Optionally, to retrieve a specific tag, you can generate the GetTag function. If the service directory does not contain a generate.go file, create one. This file must only contain generate directives and a package declaration (e.g., package eks ). For examples of the generate.go file, many service directories contain one, e.g., internal/service/eks/generate.go . If the generate.go file does not contain a generate directive for tagging code, i.e., //go:generate go run ../../generate/tags/main.go , add it. Note that without flags, the directive itself will not do anything useful. You must not include more than one generate/tags/main.go directive, as subsequent directives will overwrite previous directives. To generate multiple types of tag code, use multiple flags with the directive. Generating Tagging Types # Determine how the service implements tagging: Some services will use a simple map style ( map[string]*string in Go), while others will have a separate structure, often a []service.Tag struct with Key and Value fields. If the service uses the simple map style, pass the flag -ServiceTagsMap . If the service uses a slice of structs, pass the flag -ServiceTagsSlice . If the name of the tag struct is not Tag , pass the flag -TagType=<struct name> . Note that the struct name is used without the package name. For example, the AppMesh service uses the struct TagRef , so the flag is -TagType=TagRef . If the key and value fields on the struct are not Key and Value , specify the names using the flags -TagTypeKeyElem and -TagTypeValElem respectively. For example, the KMS service uses the struct Tag , but the key and value fields are TagKey and TagValue , so the flags are -TagTypeKeyElem=TagKey and -TagTypeValElem=TagValue . Some services, such as EC2 and Auto Scaling, return a different type depending on the API call used to retrieve the tag. To indicate the additional type, include the flag -TagType2=<struct name> . For example, the Auto Scaling uses the struct Tag as part of resource calls, but returns the struct TagDescription from the DescribeTags API call. The flag used is -TagType2=TagDescription . For more details on flags for generating service keys, see the documentation for the tag generator Generating Standalone Tag Listing Functions # If the service API uses a standalone function to retrieve tags instead of including them with the resource (usually a ListTags or ListTagsForResource API call), pass the flag -ListTags . If the API call is not ListTagsForResource , pass the flag -ListTagsOp=<API call name> . Note that this does not include the package name. For example, the Auto Scaling service uses the API call DescribeTags , so the flag is -ListTagsOp=DescribeTags . If the API call uses a field other than ResourceArn to identify the resource, pass the flag -ListTagsInIDElem=<field name> . For example, the CloudWatch service uses the field ResourceARN , so the flag is -ListTagsInIDElem=ResourceARN . Some API calls take a slice of identifiers instead of a single identifier. In this case, pass the flag -ListTagsInIDNeedSlice=yes . If the field containing the tags in the result of the API call is not named Tags , pass the flag -ListTagsOutTagsElem=<struct name> . For example, the CloudTrail service returns a nested structure, where the resulting flag is -ListTagsOutTagsElem=ResourceTagList[0].TagsList . In some cases, it can be useful to retrieve single tags. Pass the flag -GetTag to generate a function to do so. For more details on flags for generating tag listing functions, see the documentation for the tag generator Generating Standalone Tag Updating Functions # If the service API uses a standalone function to update tags instead of including them when updating the resource (usually a TagResource and UntagResource API call), pass the flag -UpdateTags . If the API call to add tags is not TagResource , pass the flag -TagOp=<API call name> . Note that this does not include the package name. For example, the ElastiCache service uses the API call AddTagsToResource , so the flag is -TagOp=AddTagsToResource . If the API call to add tags uses a field other than ResourceArn to identify the resource, pass the flag -TagInIDElem=<field name> . For example, the EC2 service uses the field Resources , so the flag is -TagInIDElem=Resources . Some API calls take a slice of identifiers instead of a single identifier. In this case, pass the flag -TagInIDNeedSlice=yes . If the API call to remove tags is not UntagResource , pass the flag -UntagOp=<API call name> . Note that this does not include the package name. For example, the ElastiCache service uses the API call RemoveTagsFromResource , so the flag is -UntagOp=RemoveTagsFromResource . If the API call to remove tags uses a field other than ResourceArn to identify the resource, pass the flag -UntagInTagsElem=<field name> . For example, the Route 53 service uses the field Keys , so the flag is -UntagInTagsElem=Keys . For more details on flags for generating tag updating functions, see the documentation for the tag generator Specifying the AWS SDK for Go version # The vast majority of the Terraform AWS Provider is implemented using version 1 of the AWS SDK for Go . For new services, however, we have started to use version 2 of the SDK . By default, the generated code uses the AWS SDK for Go v1. To generate code using the AWS SDK for Go v2, pass the flag -AwsSdkVersion=2 . For more information, see the documentation on AWS SDK versions . Running Code generation # Run the command make gen to run the code generators for the project. To ensure that the code compiles, run make test . Resource Tagging Code Implementation # Resource Schema # Add the following imports to the resource's Go source file: imports ( /* ... other imports ... */ tftags \"github.com/hashicorp/terraform-provider-aws/internal/tags\" \"github.com/hashicorp/terraform-provider-aws/internal/verify\" ) Add the tags parameter and tags_all attribute to the schema. The tags parameter contains the tags set directly on the resource. The tags_all attribute contains union of the tags set directly on the resource and default tags configured on the provider. func ResourceCluster () * schema . Resource { return & schema . Resource { /* ... other configuration ... */ Schema : map [ string ] * schema . Schema { /* ... other configuration ... */ \"tags\" : tftags . TagsSchema (), \"tags_all\" : tftags . TagsSchemaComputed (), }, } } The function verify.SetTagsDiff handles the combination of tags set on the resource and default tags, and must be added to the resource's CustomizeDiff function. If the resource has no other CustomizeDiff handler functions, set it directly: func ResourceCluster () * schema . Resource { return & schema . Resource { /* ... other configuration ... */ CustomizeDiff : verify . SetTagsDiff , } } Otherwise, if the resource already contains a CustomizeDiff function, append the SetTagsDiff via the customdiff.All method: func ResourceExample () * schema . Resource { return & schema . Resource { /* ... other configuration ... */ CustomizeDiff : customdiff . All ( resourceExampleCustomizeDiff , verify . SetTagsDiff , ), } } Resource Create Operation # When creating a resource, some AWS APIs support passing tags in the Create call while others require setting the tags after the initial creation. If the API supports tagging on creation (e.g., the Input struct accepts a Tags field), implement the logic to convert the configuration tags into the service tags, e.g., with EKS Clusters: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig tags := defaultTagsConfig . MergeTags ( tftags . New ( d . Get ( \"tags\" ).( map [ string ] interface {}))) input := & eks . CreateClusterInput { /* ... other configuration ... */ Tags : Tags ( tags . IgnoreAWS ()), } If the service API does not allow passing an empty list, the logic can be adjusted similar to: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig tags := defaultTagsConfig . MergeTags ( tftags . New ( d . Get ( \"tags\" ).( map [ string ] interface {}))) input := & eks . CreateClusterInput { /* ... other configuration ... */ } if len ( tags ) > 0 { input . Tags = Tags ( tags . IgnoreAWS ()) } Otherwise, if the API does not support tagging on creation, implement the logic to convert the configuration tags into the service API call to tag a resource, e.g., with Device Farm device pools: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig tags := defaultTagsConfig . MergeTags ( tftags . New ( d . Get ( \"tags\" ).( map [ string ] interface {}))) /* ... creation steps ... */ if len ( tags ) > 0 { if err := UpdateTags ( conn , d . Id (), nil , tags ); err != nil { return fmt . Errorf ( \"adding DeviceFarm Device Pool (%s) tags: %w\" , d . Id (), err ) } } Some EC2 resources (e.g., aws_ec2_fleet ) have a TagSpecifications field in the InputStruct instead of a Tags field. In these cases the tagSpecificationsFromKeyValueTags() helper function should be used. This example shows using TagSpecifications : // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig tags := defaultTagsConfig . MergeTags ( tftags . New ( d . Get ( \"tags\" ).( map [ string ] interface {}))) input := & ec2 . CreateFleetInput { /* ... other configuration ... */ TagSpecifications : tagSpecificationsFromKeyValueTags ( tags , ec2 . ResourceTypeFleet ), } Resource Read Operation # In the resource Read operation, implement the logic to convert the service tags to save them into the Terraform state for drift detection, e.g., with EKS Clusters: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig ignoreTagsConfig := meta .( * AWSClient ). IgnoreTagsConfig /* ... other d.Set(...) logic ... */ tags := KeyValueTags ( cluster . Tags ). IgnoreAWS (). IgnoreConfig ( ignoreTagsConfig ) if err := d . Set ( \"tags\" , tags . RemoveDefaultConfig ( defaultTagsConfig ). Map ()); err != nil { return fmt . Errorf ( \"setting tags: %w\" , err ) } if err := d . Set ( \"tags_all\" , tags . Map ()); err != nil { return fmt . Errorf ( \"setting tags_all: %w\" , err ) } If the service API does not return the tags directly from reading the resource and requires a separate API call, use the generated ListTags function, e.g., with Athena Workgroups: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig ignoreTagsConfig := meta .( * AWSClient ). IgnoreTagsConfig /* ... other d.Set(...) logic ... */ tags , err := ListTags ( conn , arn . String ()) if err != nil { return fmt . Errorf ( \"listing tags for resource (%s): %w\" , arn , err ) } tags = tags . IgnoreAWS (). IgnoreConfig ( ignoreTagsConfig ) if err := d . Set ( \"tags\" , tags . RemoveDefaultConfig ( defaultTagsConfig ). Map ()); err != nil { return fmt . Errorf ( \"setting tags: %w\" , err ) } if err := d . Set ( \"tags_all\" , tags . Map ()); err != nil { return fmt . Errorf ( \"setting tags_all: %w\" , err ) } Resource Update Operation # In the resource Update operation, implement the logic to handle tagging updates, e.g., with EKS Clusters: if d . HasChange ( \"tags_all\" ) { o , n := d . GetChange ( \"tags_all\" ) if err := UpdateTags ( conn , d . Get ( \"arn\" ).( string ), o , n ); err != nil { return fmt . Errorf ( \"updating tags: %w\" , err ) } } If the resource Update function applies specific updates to attributes regardless of changes to tags, implement the following e.g., with IAM Policy: if d . HasChangesExcept ( \"tags\" , \"tags_all\" ) { /* ... other logic ...*/ request := & iam . CreatePolicyVersionInput { PolicyArn : aws . String ( d . Id ()), PolicyDocument : aws . String ( d . Get ( \"policy\" ).( string )), SetAsDefault : aws . Bool ( true ), } if _ , err := conn . CreatePolicyVersion ( request ); err != nil { return fmt . Errorf ( \"updating IAM policy (%s): %w\" , d . Id (), err ) } } Resource Tagging Acceptance Testing Implementation # In the resource testing (e.g., internal/service/eks/cluster_test.go ), verify that existing resources without tagging are unaffected and do not have tags saved into their Terraform state. This should be done in the _basic acceptance test by adding one line similar to resource.TestCheckResourceAttr(resourceName, \"tags.%\", \"0\"), and one similar to resource.TestCheckResourceAttr(resourceName, \"tags_all.%\", \"0\"), In the resource testing, implement a new test named _tags with associated configurations, that verifies creating the resource with tags and updating tags. E.g., EKS Clusters: func TestAccEKSCluster_tags ( t * testing . T ) { var cluster1 , cluster2 , cluster3 eks . Cluster rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_eks_cluster.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ); testAccPreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , eks . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckClusterDestroy , Steps : [] resource . TestStep { { Config : testAccClusterConfigTags1 ( rName , \"key1\" , \"value1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckClusterExists ( resourceName , & cluster1 ), resource . TestCheckResourceAttr ( resourceName , \"tags.%\" , \"1\" ), resource . TestCheckResourceAttr ( resourceName , \"tags.key1\" , \"value1\" ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, { Config : testAccClusterConfigTags2 ( rName , \"key1\" , \"value1updated\" , \"key2\" , \"value2\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckClusterExists ( resourceName , & cluster2 ), resource . TestCheckResourceAttr ( resourceName , \"tags.%\" , \"2\" ), resource . TestCheckResourceAttr ( resourceName , \"tags.key1\" , \"value1updated\" ), resource . TestCheckResourceAttr ( resourceName , \"tags.key2\" , \"value2\" ), ), }, { Config : testAccClusterConfigTags1 ( rName , \"key2\" , \"value2\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckClusterExists ( resourceName , & cluster3 ), resource . TestCheckResourceAttr ( resourceName , \"tags.%\" , \"1\" ), resource . TestCheckResourceAttr ( resourceName , \"tags.key2\" , \"value2\" ), ), }, }, }) } func testAccClusterConfigTags1 ( rName , tagKey1 , tagValue1 string ) string { return acctest . ConfigCompose ( testAccClusterConfig_base ( rName ), fmt . Sprintf ( ` resource \"aws_eks_cluster\" \"test\" { name = %[1]q role_arn = aws_iam_role.test.arn tags = { %[2]q = %[3]q } vpc_config { subnet_ids = aws_subnet.test[*].id } depends_on = [aws_iam_role_policy_attachment.test-AmazonEKSClusterPolicy] } ` , rName , tagKey1 , tagValue1 )) } func testAccClusterConfigTags2 ( rName , tagKey1 , tagValue1 , tagKey2 , tagValue2 string ) string { return acctest . ConfigCompose ( testAccClusterConfig_base ( rName ), fmt . Sprintf ( ` resource \"aws_eks_cluster\" \"test\" { name = %[1]q role_arn = aws_iam_role.test.arn tags = { %[2]q = %[3]q %[4]q = %[5]q } vpc_config { subnet_ids = aws_subnet.test[*].id } depends_on = [aws_iam_role_policy_attachment.test-AmazonEKSClusterPolicy] } ` , rName , tagKey1 , tagValue1 , tagKey2 , tagValue2 )) } Verify all acceptance testing passes for the resource (e.g., make testacc TESTS=TestAccEKSCluster_ PKG=eks ) Resource Tagging Documentation Implementation # In the resource documentation (e.g., website/docs/r/eks_cluster.html.markdown ), add the following to the arguments reference: * `tags` - (Optional) Key-value mapping of resource tags. If configured with a provider [ `default_tags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block) present, tags with matching keys will overwrite those defined at the provider-level. In the resource documentation (e.g., website/docs/r/eks_cluster.html.markdown ), add the following to the attributes reference: * `tags_all` - Map of tags assigned to the resource, including those inherited from the provider [ `default_tags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block).","title":"Resource Tagging"},{"location":"resource-tagging/#adding-resource-tagging-support","text":"AWS provides key-value metadata across many services and resources, which can be used for a variety of use cases including billing, ownership, and more. See the AWS Tagging Strategy page for more information about tagging at a high level. The Terraform AWS Provider supports default tags configured on the provider in addition to tags configured on the resource. Implementing tagging support for Terraform AWS Provider resources requires the following, each with its own section below: Generated Service Tagging Code : Each service has a generate.go file where generator directives live. Through these directives and their flags, you can customize code generation for the service. You can find the code that the tagging generator generates in a tags_gen.go file in a service, such as internal/service/ec2/tags_gen.go . You should generally not need to edit the generator code itself (i.e., in internal/generate/tags ). Resource Tagging Code Implementation : In the resource code (e.g., internal/service/{service}/{thing}.go ), implementation of tags and tags_all schema attributes, along with implementation of CustomizeDiff in the resource definition and handling in Create , Read , and Update functions. Resource Tagging Acceptance Testing Implementation : In the resource acceptance testing (e.g., internal/service/{service}/{thing}_test.go ), implementation of new acceptance test function and configurations to exercise new tagging logic. Resource Tagging Documentation Implementation : In the resource documentation (e.g., website/docs/r/service_thing.html.markdown ), addition of tags argument and tags_all attributes.","title":"Adding Resource Tagging Support"},{"location":"resource-tagging/#generating-tag-code-for-a-service","text":"This step is generally only necessary for the first implementation and may have been previously completed. More details about this code generation, including fixes for potential error messages in this process, can be found in the generate package documentation . The generator will create several types of tagging-related code. All services that support tagging will generate the function KeyValueTags , which converts from service-specific structs returned by the AWS SDK into a common format used by the provider, and the function Tags , which converts from the common format back to the service-specific structs. In addition, many services have separate functions to list or update tags, so the corresponding ListTags and UpdateTags can be generated. Optionally, to retrieve a specific tag, you can generate the GetTag function. If the service directory does not contain a generate.go file, create one. This file must only contain generate directives and a package declaration (e.g., package eks ). For examples of the generate.go file, many service directories contain one, e.g., internal/service/eks/generate.go . If the generate.go file does not contain a generate directive for tagging code, i.e., //go:generate go run ../../generate/tags/main.go , add it. Note that without flags, the directive itself will not do anything useful. You must not include more than one generate/tags/main.go directive, as subsequent directives will overwrite previous directives. To generate multiple types of tag code, use multiple flags with the directive.","title":"Generating Tag Code for a Service"},{"location":"resource-tagging/#generating-tagging-types","text":"Determine how the service implements tagging: Some services will use a simple map style ( map[string]*string in Go), while others will have a separate structure, often a []service.Tag struct with Key and Value fields. If the service uses the simple map style, pass the flag -ServiceTagsMap . If the service uses a slice of structs, pass the flag -ServiceTagsSlice . If the name of the tag struct is not Tag , pass the flag -TagType=<struct name> . Note that the struct name is used without the package name. For example, the AppMesh service uses the struct TagRef , so the flag is -TagType=TagRef . If the key and value fields on the struct are not Key and Value , specify the names using the flags -TagTypeKeyElem and -TagTypeValElem respectively. For example, the KMS service uses the struct Tag , but the key and value fields are TagKey and TagValue , so the flags are -TagTypeKeyElem=TagKey and -TagTypeValElem=TagValue . Some services, such as EC2 and Auto Scaling, return a different type depending on the API call used to retrieve the tag. To indicate the additional type, include the flag -TagType2=<struct name> . For example, the Auto Scaling uses the struct Tag as part of resource calls, but returns the struct TagDescription from the DescribeTags API call. The flag used is -TagType2=TagDescription . For more details on flags for generating service keys, see the documentation for the tag generator","title":"Generating Tagging Types"},{"location":"resource-tagging/#generating-standalone-tag-listing-functions","text":"If the service API uses a standalone function to retrieve tags instead of including them with the resource (usually a ListTags or ListTagsForResource API call), pass the flag -ListTags . If the API call is not ListTagsForResource , pass the flag -ListTagsOp=<API call name> . Note that this does not include the package name. For example, the Auto Scaling service uses the API call DescribeTags , so the flag is -ListTagsOp=DescribeTags . If the API call uses a field other than ResourceArn to identify the resource, pass the flag -ListTagsInIDElem=<field name> . For example, the CloudWatch service uses the field ResourceARN , so the flag is -ListTagsInIDElem=ResourceARN . Some API calls take a slice of identifiers instead of a single identifier. In this case, pass the flag -ListTagsInIDNeedSlice=yes . If the field containing the tags in the result of the API call is not named Tags , pass the flag -ListTagsOutTagsElem=<struct name> . For example, the CloudTrail service returns a nested structure, where the resulting flag is -ListTagsOutTagsElem=ResourceTagList[0].TagsList . In some cases, it can be useful to retrieve single tags. Pass the flag -GetTag to generate a function to do so. For more details on flags for generating tag listing functions, see the documentation for the tag generator","title":"Generating Standalone Tag Listing Functions"},{"location":"resource-tagging/#generating-standalone-tag-updating-functions","text":"If the service API uses a standalone function to update tags instead of including them when updating the resource (usually a TagResource and UntagResource API call), pass the flag -UpdateTags . If the API call to add tags is not TagResource , pass the flag -TagOp=<API call name> . Note that this does not include the package name. For example, the ElastiCache service uses the API call AddTagsToResource , so the flag is -TagOp=AddTagsToResource . If the API call to add tags uses a field other than ResourceArn to identify the resource, pass the flag -TagInIDElem=<field name> . For example, the EC2 service uses the field Resources , so the flag is -TagInIDElem=Resources . Some API calls take a slice of identifiers instead of a single identifier. In this case, pass the flag -TagInIDNeedSlice=yes . If the API call to remove tags is not UntagResource , pass the flag -UntagOp=<API call name> . Note that this does not include the package name. For example, the ElastiCache service uses the API call RemoveTagsFromResource , so the flag is -UntagOp=RemoveTagsFromResource . If the API call to remove tags uses a field other than ResourceArn to identify the resource, pass the flag -UntagInTagsElem=<field name> . For example, the Route 53 service uses the field Keys , so the flag is -UntagInTagsElem=Keys . For more details on flags for generating tag updating functions, see the documentation for the tag generator","title":"Generating Standalone Tag Updating Functions"},{"location":"resource-tagging/#specifying-the-aws-sdk-for-go-version","text":"The vast majority of the Terraform AWS Provider is implemented using version 1 of the AWS SDK for Go . For new services, however, we have started to use version 2 of the SDK . By default, the generated code uses the AWS SDK for Go v1. To generate code using the AWS SDK for Go v2, pass the flag -AwsSdkVersion=2 . For more information, see the documentation on AWS SDK versions .","title":"Specifying the AWS SDK for Go version"},{"location":"resource-tagging/#running-code-generation","text":"Run the command make gen to run the code generators for the project. To ensure that the code compiles, run make test .","title":"Running Code generation"},{"location":"resource-tagging/#resource-tagging-code-implementation","text":"","title":"Resource Tagging Code Implementation"},{"location":"resource-tagging/#resource-schema","text":"Add the following imports to the resource's Go source file: imports ( /* ... other imports ... */ tftags \"github.com/hashicorp/terraform-provider-aws/internal/tags\" \"github.com/hashicorp/terraform-provider-aws/internal/verify\" ) Add the tags parameter and tags_all attribute to the schema. The tags parameter contains the tags set directly on the resource. The tags_all attribute contains union of the tags set directly on the resource and default tags configured on the provider. func ResourceCluster () * schema . Resource { return & schema . Resource { /* ... other configuration ... */ Schema : map [ string ] * schema . Schema { /* ... other configuration ... */ \"tags\" : tftags . TagsSchema (), \"tags_all\" : tftags . TagsSchemaComputed (), }, } } The function verify.SetTagsDiff handles the combination of tags set on the resource and default tags, and must be added to the resource's CustomizeDiff function. If the resource has no other CustomizeDiff handler functions, set it directly: func ResourceCluster () * schema . Resource { return & schema . Resource { /* ... other configuration ... */ CustomizeDiff : verify . SetTagsDiff , } } Otherwise, if the resource already contains a CustomizeDiff function, append the SetTagsDiff via the customdiff.All method: func ResourceExample () * schema . Resource { return & schema . Resource { /* ... other configuration ... */ CustomizeDiff : customdiff . All ( resourceExampleCustomizeDiff , verify . SetTagsDiff , ), } }","title":"Resource Schema"},{"location":"resource-tagging/#resource-create-operation","text":"When creating a resource, some AWS APIs support passing tags in the Create call while others require setting the tags after the initial creation. If the API supports tagging on creation (e.g., the Input struct accepts a Tags field), implement the logic to convert the configuration tags into the service tags, e.g., with EKS Clusters: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig tags := defaultTagsConfig . MergeTags ( tftags . New ( d . Get ( \"tags\" ).( map [ string ] interface {}))) input := & eks . CreateClusterInput { /* ... other configuration ... */ Tags : Tags ( tags . IgnoreAWS ()), } If the service API does not allow passing an empty list, the logic can be adjusted similar to: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig tags := defaultTagsConfig . MergeTags ( tftags . New ( d . Get ( \"tags\" ).( map [ string ] interface {}))) input := & eks . CreateClusterInput { /* ... other configuration ... */ } if len ( tags ) > 0 { input . Tags = Tags ( tags . IgnoreAWS ()) } Otherwise, if the API does not support tagging on creation, implement the logic to convert the configuration tags into the service API call to tag a resource, e.g., with Device Farm device pools: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig tags := defaultTagsConfig . MergeTags ( tftags . New ( d . Get ( \"tags\" ).( map [ string ] interface {}))) /* ... creation steps ... */ if len ( tags ) > 0 { if err := UpdateTags ( conn , d . Id (), nil , tags ); err != nil { return fmt . Errorf ( \"adding DeviceFarm Device Pool (%s) tags: %w\" , d . Id (), err ) } } Some EC2 resources (e.g., aws_ec2_fleet ) have a TagSpecifications field in the InputStruct instead of a Tags field. In these cases the tagSpecificationsFromKeyValueTags() helper function should be used. This example shows using TagSpecifications : // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig tags := defaultTagsConfig . MergeTags ( tftags . New ( d . Get ( \"tags\" ).( map [ string ] interface {}))) input := & ec2 . CreateFleetInput { /* ... other configuration ... */ TagSpecifications : tagSpecificationsFromKeyValueTags ( tags , ec2 . ResourceTypeFleet ), }","title":"Resource Create Operation"},{"location":"resource-tagging/#resource-read-operation","text":"In the resource Read operation, implement the logic to convert the service tags to save them into the Terraform state for drift detection, e.g., with EKS Clusters: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig ignoreTagsConfig := meta .( * AWSClient ). IgnoreTagsConfig /* ... other d.Set(...) logic ... */ tags := KeyValueTags ( cluster . Tags ). IgnoreAWS (). IgnoreConfig ( ignoreTagsConfig ) if err := d . Set ( \"tags\" , tags . RemoveDefaultConfig ( defaultTagsConfig ). Map ()); err != nil { return fmt . Errorf ( \"setting tags: %w\" , err ) } if err := d . Set ( \"tags_all\" , tags . Map ()); err != nil { return fmt . Errorf ( \"setting tags_all: %w\" , err ) } If the service API does not return the tags directly from reading the resource and requires a separate API call, use the generated ListTags function, e.g., with Athena Workgroups: // Typically declared near conn := /* ... */ defaultTagsConfig := meta .( * AWSClient ). DefaultTagsConfig ignoreTagsConfig := meta .( * AWSClient ). IgnoreTagsConfig /* ... other d.Set(...) logic ... */ tags , err := ListTags ( conn , arn . String ()) if err != nil { return fmt . Errorf ( \"listing tags for resource (%s): %w\" , arn , err ) } tags = tags . IgnoreAWS (). IgnoreConfig ( ignoreTagsConfig ) if err := d . Set ( \"tags\" , tags . RemoveDefaultConfig ( defaultTagsConfig ). Map ()); err != nil { return fmt . Errorf ( \"setting tags: %w\" , err ) } if err := d . Set ( \"tags_all\" , tags . Map ()); err != nil { return fmt . Errorf ( \"setting tags_all: %w\" , err ) }","title":"Resource Read Operation"},{"location":"resource-tagging/#resource-update-operation","text":"In the resource Update operation, implement the logic to handle tagging updates, e.g., with EKS Clusters: if d . HasChange ( \"tags_all\" ) { o , n := d . GetChange ( \"tags_all\" ) if err := UpdateTags ( conn , d . Get ( \"arn\" ).( string ), o , n ); err != nil { return fmt . Errorf ( \"updating tags: %w\" , err ) } } If the resource Update function applies specific updates to attributes regardless of changes to tags, implement the following e.g., with IAM Policy: if d . HasChangesExcept ( \"tags\" , \"tags_all\" ) { /* ... other logic ...*/ request := & iam . CreatePolicyVersionInput { PolicyArn : aws . String ( d . Id ()), PolicyDocument : aws . String ( d . Get ( \"policy\" ).( string )), SetAsDefault : aws . Bool ( true ), } if _ , err := conn . CreatePolicyVersion ( request ); err != nil { return fmt . Errorf ( \"updating IAM policy (%s): %w\" , d . Id (), err ) } }","title":"Resource Update Operation"},{"location":"resource-tagging/#resource-tagging-acceptance-testing-implementation","text":"In the resource testing (e.g., internal/service/eks/cluster_test.go ), verify that existing resources without tagging are unaffected and do not have tags saved into their Terraform state. This should be done in the _basic acceptance test by adding one line similar to resource.TestCheckResourceAttr(resourceName, \"tags.%\", \"0\"), and one similar to resource.TestCheckResourceAttr(resourceName, \"tags_all.%\", \"0\"), In the resource testing, implement a new test named _tags with associated configurations, that verifies creating the resource with tags and updating tags. E.g., EKS Clusters: func TestAccEKSCluster_tags ( t * testing . T ) { var cluster1 , cluster2 , cluster3 eks . Cluster rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_eks_cluster.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ); testAccPreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , eks . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckClusterDestroy , Steps : [] resource . TestStep { { Config : testAccClusterConfigTags1 ( rName , \"key1\" , \"value1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckClusterExists ( resourceName , & cluster1 ), resource . TestCheckResourceAttr ( resourceName , \"tags.%\" , \"1\" ), resource . TestCheckResourceAttr ( resourceName , \"tags.key1\" , \"value1\" ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, { Config : testAccClusterConfigTags2 ( rName , \"key1\" , \"value1updated\" , \"key2\" , \"value2\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckClusterExists ( resourceName , & cluster2 ), resource . TestCheckResourceAttr ( resourceName , \"tags.%\" , \"2\" ), resource . TestCheckResourceAttr ( resourceName , \"tags.key1\" , \"value1updated\" ), resource . TestCheckResourceAttr ( resourceName , \"tags.key2\" , \"value2\" ), ), }, { Config : testAccClusterConfigTags1 ( rName , \"key2\" , \"value2\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckClusterExists ( resourceName , & cluster3 ), resource . TestCheckResourceAttr ( resourceName , \"tags.%\" , \"1\" ), resource . TestCheckResourceAttr ( resourceName , \"tags.key2\" , \"value2\" ), ), }, }, }) } func testAccClusterConfigTags1 ( rName , tagKey1 , tagValue1 string ) string { return acctest . ConfigCompose ( testAccClusterConfig_base ( rName ), fmt . Sprintf ( ` resource \"aws_eks_cluster\" \"test\" { name = %[1]q role_arn = aws_iam_role.test.arn tags = { %[2]q = %[3]q } vpc_config { subnet_ids = aws_subnet.test[*].id } depends_on = [aws_iam_role_policy_attachment.test-AmazonEKSClusterPolicy] } ` , rName , tagKey1 , tagValue1 )) } func testAccClusterConfigTags2 ( rName , tagKey1 , tagValue1 , tagKey2 , tagValue2 string ) string { return acctest . ConfigCompose ( testAccClusterConfig_base ( rName ), fmt . Sprintf ( ` resource \"aws_eks_cluster\" \"test\" { name = %[1]q role_arn = aws_iam_role.test.arn tags = { %[2]q = %[3]q %[4]q = %[5]q } vpc_config { subnet_ids = aws_subnet.test[*].id } depends_on = [aws_iam_role_policy_attachment.test-AmazonEKSClusterPolicy] } ` , rName , tagKey1 , tagValue1 , tagKey2 , tagValue2 )) } Verify all acceptance testing passes for the resource (e.g., make testacc TESTS=TestAccEKSCluster_ PKG=eks )","title":"Resource Tagging Acceptance Testing Implementation"},{"location":"resource-tagging/#resource-tagging-documentation-implementation","text":"In the resource documentation (e.g., website/docs/r/eks_cluster.html.markdown ), add the following to the arguments reference: * `tags` - (Optional) Key-value mapping of resource tags. If configured with a provider [ `default_tags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block) present, tags with matching keys will overwrite those defined at the provider-level. In the resource documentation (e.g., website/docs/r/eks_cluster.html.markdown ), add the following to the attributes reference: * `tags_all` - Map of tags assigned to the resource, including those inherited from the provider [ `default_tags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block).","title":"Resource Tagging Documentation Implementation"},{"location":"retries-and-waiters/","text":"Retries and Waiters # Terraform plugins may run into situations where calling the remote system after an operation may be necessary. These typically fall under three classes where: The request never reaches the remote system. The request reaches the remote system and responds that it cannot handle the request temporarily. The implementation of the remote system requires additional requests to ensure success. This guide describes the behavior of the Terraform AWS Provider and provides code implementations that help ensure success in each of these situations. Terraform Plugin SDK Functionality # The Terraform Plugin SDK , which the AWS Provider uses, provides vital tools for handling consistency: the resource.StateChangeConf{} struct, and the retry functions, resource.Retry() and resource.RetryContext() . We will discuss these throughout the rest of this guide. Since they help keep the AWS Provider code consistent, we heavily prefer them over custom implementations. This guide goes beyond the Terraform Plugin SDK v2 documentation by providing additional context and emergent implementations specific to the Terraform AWS Provider. State Change Configuration and Functions # The resource.StateChangeConf type along with its receiver methods WaitForState() and WaitForStateContext() is a generic primitive for repeating operations in Terraform resource logic until desired value(s) are received. The \"state change\" in this case is generic to any value and not specific to the Terraform State. Among other functionality, it supports some of these desirable optional properties: Expecting specific value(s) while waiting for the target value(s) to be reached. Unexpected values are returned as an error which can be augmented with additional details. Expecting the target value(s) to be returned multiple times in succession. Allowing various polling configurations such as delaying the initial request and setting the time between polls. Retry Functions # The resource.Retry() and resource.RetryContext() functions provide a simplified retry implementation around resource.StateChangeConf . Their most common use is for simple error-based retries. AWS Request Handling # The Terraform AWS Provider's requests to AWS service APIs happen on top of Hypertext Transfer Protocol (HTTP). The following is a simplified description of the layers and handling that requests pass through: A Terraform resource calls an AWS Go SDK function. The AWS Go SDK generates an AWS-compatible HTTP request using the Go standard library net/http package . This includes the following: Adding HTTP headers for authentication and signing of requests to ensure authenticity. Converting operation inputs into required HTTP URI parameters and/or request body type (XML or JSON). If debug logging is enabled, logging of the HTTP request. The AWS Go SDK transmits the net/http request using Go's standard handling of the Operating System (OS) and Domain Name System (DNS) configuration. The AWS service potentially receives the request and responds, typically adding a request identifier HTTP header which can be used for AWS Support cases. The OS and Go net/http receive the response and pass it to the AWS Go SDK. The AWS Go SDK attempts to handle the response. This may include: Parsing output Converting errors into operation errors (Go error type of wrapped awserr.Error type ). Converting response elements into operation outputs (AWS Go SDK operation-specific types). Triggering automatic request retries based on default and custom logic. The Terraform resource receives the response, including any output and errors, from the AWS Go SDK. The Terraform AWS Provider specific configuration for AWS Go SDK operation handling can be found in aws/conns/conns.go in this codebase and the hashicorp/aws-sdk-go-base codebase . NOTE: The section descibes the current handling with version 1 of the AWS Go SDK. In the future, this codebase will be migrated to version 2 of the AWS Go SDK. The newer version implements a very similar request flow but uses a simpler credential and request handling configuration. As such, the aws-sdk-go-base dependency will likely not receive further updates and will be removed after that migration. Default AWS Go SDK Retries # In some situations, while handling a response, the AWS Go SDK automatically retries a request before returning the output and error. The retry mechanism implements an exponential backoff algorithm. The default conditions triggering automatic retries (implemented through client.DefaultRetryer ) include: Certain network errors. A common exception to this is connection reset errors. HTTP status codes 429 and 5xx. Certain API error codes, which are common across various AWS services (e.g., ThrottledException ). However, not all AWS services implement these error codes consistently. A common exception to this is certain expired credentials errors. By default, the Terraform AWS Provider sets the maximum number of AWS Go SDK retries based on the max_retries provider configuration . The provider configuration defaults to 25 and the exponential backoff roughly equates to one hour of retries. This very high default value was present before the Terraform AWS Provider codebase was split from Terraform CLI in version 0.10. NOTE: The section describes the current handling with version 1 of the AWS Go SDK. In the future, this codebase will be migrated to version 2 of the AWS Go SDK. The newer version implements additional retry conditions by default, such as consistently retrying all common network errors. NOTE: The section describes the current handling with Terraform Plugin SDK resource signatures without context.Context . In the future, this codebase will be migrated to the context-aware resource signatures which currently enforce a 20-minute default timeout that conflicts with the timeout with the default max_retries value. The Terraform Plugin SDK may be updated to support removing this default 20-minute timeout or the default retry mechanism described here will be updated to prevent context cancellation errors where possible. Lower Network Error Retries # Given the very high default number of AWS Go SDK retries configured in the Terraform AWS Provider and the excessive wait that practitioners would face, the hashicorp/aws-sdk-go-base codebase lowers retries to 10 for certain network errors that typically cannot be remediated via retries. This roughly equates to 30 seconds of retries. Terraform AWS Provider Service Retries # The AWS Go SDK provides hooks for injecting custom logic into the service client handlers. We prefer this handling in situations where contributors would need to apply the retry behavior to many resources. For example, in cases where the AWS service API does not mark an error code as automatically retriable. The AWS Provider includes other retry-changing behaviors using this method. You can find them in the aws/config.go file. For example: client . kafkaconn . Handlers . Retry . PushBack ( func ( r * request . Request ) { if tfawserr . ErrMessageContains ( r . Error , kafka . ErrCodeTooManyRequestsException , \"Too Many Requests\" ) { r . Retryable = aws . Bool ( true ) } }) Eventual Consistency # Eventual consistency is a temporary condition where the remote system can return outdated information or errors due to not being strongly read-after-write consistent. This is a pattern found in remote systems that must be highly scaled for broad usage. Terraform expects any planned resource lifecycle change (create, update, destroy of the resource itself) and planned resource attribute value change to match after being applied. Conversely, operators typically expect that Terraform resources also implement the concept of drift detection for resources and their attributes, which requires reading information back from the remote system after an operation. A common implementation is refreshing the Terraform State information ( d.Set() ) during the Read function of a resource after Create and Update . These two concepts conflict with each other and require additional handling in Terraform resource logic as shown in the following sections. These issues are not reliably reproducible, especially in the case of writing acceptance testing, so they can be elusive with false positives to verify fixes. Operation Specific Error Retries # Even given a properly ordered Terraform configuration, eventual consistency can unexpectedly prevent downstream operations from succeeding. A simple retry after a few seconds resolves many of these issues. To reduce frustrating behavior for operators, wrap AWS Go SDK operations with the resource.Retry() or resource.RetryContext() functions. These retries should have a reasonably low timeout (typically two minutes but up to five minutes). Save them in a constant for reusability. These functions are preferably in line with the associated resource logic to remove any indirection with the code. Do not use this type of logic to overcome improperly ordered Terraform configurations. The approach may not work in larger environments. // internal/service/example/wait.go (created if does not exist) const ( // Maximum amount of time to wait for Thing operation eventual consistency ThingOperationTimeout = 2 * time . Minute ) // internal/service/{service}/{thing}.go // ... Create, Read, Update, or Delete function ... err := resource . Retry ( ThingOperationTimeout , func () * resource . RetryError { _ , err := conn . /* ... AWS Go SDK operation with eventual consistency errors ... */ // Retryable conditions which can be checked. // These must be updated to match the AWS service API error code and message. if tfawserr . ErrMessageContains ( err , /* error code */ , /* error message */ ) { return resource . RetryableError ( err ) } if err != nil { return resource . NonRetryableError ( err ) } return nil }) // This check is important - it handles when the AWS Go SDK operation retries without returning. // e.g., any automatic retries due to network or throttling errors. if tfresource . TimedOut ( err ) { // The use of equals assignment (over colon equals) is also important here. // This overwrites the error variable to simplify logic. _ , err = conn . /* ... AWS Go SDK operation with IAM eventual consistency errors ... */ } if err != nil { return fmt . Errorf ( \"... error message context ... : %w\" , err ) } NOTE: The section descibes the current handling with version 1 of the AWS Go SDK. In the future, this codebase will be migrated to version 2 of the AWS Go SDK. The newer version natively supports operation-specific retries in a more friendly manner, which may replace this type of implementation. IAM Error Retries # A common eventual consistency issue is an error returned due to IAM permissions. The IAM service itself is eventually consistent along with the propagation of its components and permissions to other AWS services. For example, if the following operations occur in quick succession: Create an IAM Role Attach an IAM Policy to the IAM Role Reference the new IAM Role in another AWS service, such as creating a Lambda Function The last operation can receive varied API errors ranging from: IAM Role being reported as not existing IAM Role being reported as not having permissions for the other service to use it (assume role permissions) IAM Role being reported as not having sufficient permissions (inline or attached role permissions) Each AWS service API (and sometimes even operations within the same API) varies in the implementation of these errors. To handle them, it is recommended to use the Operation Specific Error Retries pattern. The Terraform AWS Provider implements a standard timeout constant of two minutes in the internal/service/iam package which should be used for all retry timeouts associated with IAM errors. This timeout was derived from years of Terraform operational experience with all AWS APIs. // internal/service/{service}/{thing}.go import ( // ... other imports ... tfiam \"github.com/hashicorp/terraform-provider-aws/internal/service/iam\" ) // ... Create and typically Update function ... err := resource . Retry ( iamwaiter . PropagationTimeout , func () * resource . RetryError { _ , err := conn . /* ... AWS Go SDK operation with IAM eventual consistency errors ... */ // Example retryable condition // This must be updated to match the AWS service API error code and message. if tfawserr . ErrMessageContains ( err , /* error code */ , /* error message */ ) { return resource . RetryableError ( err ) } if err != nil { return resource . NonRetryableError ( err ) } return nil }) if tfresource . TimedOut ( err ) { _ , err = conn . /* ... AWS Go SDK operation with IAM eventual consistency errors ... */ } if err != nil { return fmt . Errorf ( \"... error message context ... : %w\" , err ) } Asynchronous Operation Error Retries # Some remote system operations run asynchronously as detailed in the Asynchronous Operations section . In these cases, it is possible that the initial operation will immediately return as successful, but potentially return a retryable failure while checking the operation status that requires starting everything over. The handling for these is complicated by the fact that there are two timeouts, one for the retryable failure and one for the asynchronous operation status checking. The below code example highlights this situation for a resource creation that also exhibited IAM eventual consistency. // internal/service/{service}/{thing}.go import ( // ... other imports ... tfiam \"github.com/hashicorp/terraform-provider-aws/internal/service/iam\" ) // ... Create function ... // Underlying IAM eventual consistency errors can occur after the creation // operation. The goal is only retry these types of errors up to the IAM // timeout. Since the creation process is asynchronous and can take up to // its own timeout, we store a stop time upfront for checking. iamwaiterStopTime := time . Now (). Add ( tfiam . PropagationTimeout ) // Ensure to add IAM eventual consistency timeout in case of retries err = resource . Retry ( tfiam . PropagationTimeout + ThingOperationTimeout , func () * resource . RetryError { // Only retry IAM eventual consistency errors up to that timeout iamwaiterRetry := time . Now (). Before ( iamwaiterStopTime ) _ , err := conn . /* ... AWS Go SDK operation without eventual consistency errors ... */ if err != nil { return resource . NonRetryableError ( err ) } _ , err = ThingOperation ( conn , d . Id ()) if err != nil { if iamwaiterRetry && /* eventual consistency error checking */ { return resource . RetryableError ( err ) } return resource . NonRetryableError ( err ) } return nil }) if tfresource . TimedOut ( err ) { _ , err = conn . /* ... AWS Go SDK operation without eventual consistency errors ... */ if err != nil { return err } _ , err = ThingOperation ( conn , d . Id ()) if err != nil { return err } } Resource Lifecycle Retries # Resource lifecycle eventual consistency is a type of consistency issue that relates to the existence or state of an AWS infrastructure component. For example, if you create a resource and immediately try to get information about it, some AWS services and operations will return a \"not found\" error. Depending on the service and general AWS load, these errors can be frequent or rare. In order to avoid this issue, identify operations that make changes. Then, when calling any other operations that rely on the changes, account for the possibility that the AWS service has not yet fully realized them. A typical example is creating an AWS component. After creation, when attempting to read the component's information, provide logic to retry the read if the AWS service returns a \"not found\" error. The pattern that most resources should follow is to have the Create function return calling the Read function. This fills in computed attributes and ensures that the AWS service applied the configuration correctly. Add retry logic to the Read function to overcome the temporary condition on resource creation. Note that for eventually consistent resources, \"not found\" errors can still occur in the Read function even after implementing Resource Lifecycle Waiters for the Create function. // internal/service/example/wait.go (created if does not exist) const ( // Maximum amount of time to wait for Thing eventual consistency on creation ThingCreationTimeout = 2 * time . Minute ) // internal/service/{service}/{thing}.go function ExampleThingCreate ( d * schema . ResourceData , meta interface {}) error { // ... return ExampleThingRead ( d , meta ) } function ExampleThingRead ( d * schema . ResourceData , meta interface {}) error { conn := meta .( * AWSClient ). exampleconn input := & example . OperationInput { /* ... */ } var output * example . OperationOutput err := resource . Retry ( ThingCreationTimeout , func () * resource . RetryError { var err error output , err = conn . Operation ( input ) // Retry on any API \"not found\" errors, but only on new resources. if d . IsNewResource () && tfawserr . ErrorCodeEquals ( err , example . ErrCodeResourceNotFoundException ) { return resource . RetryableError ( err ) } if err != nil { return resource . NonRetryableError ( err ) } return nil }) // Retry AWS Go SDK operation if no response from automatic retries. if tfresource . TimedOut ( err ) { output , err = exampleconn . Operation ( input ) } // Prevent confusing Terraform error messaging to operators by // Only ignoring API \"not found\" errors if not a new resource. if ! d . IsNewResource () && tfawserr . ErrorCodeEquals ( err , example . ErrCodeNoSuchEntityException ) { log . Printf ( \"[WARN] Example Thing (%s) not found, removing from state\" , d . Id ()) d . SetId ( \"\" ) return nil } if err != nil { return fmt . Errorf ( \"reading Example Thing (%s): %w\" , d . Id (), err ) } // Prevent panics. if output == nil { return fmt . Errorf ( \"reading Example Thing (%s): empty response\" , d . Id ()) } // ... refresh Terraform state as normal ... d . Set ( \"arn\" , output . Arn ) } Some other general guidelines are: If the Create function uses resource.StateChangeConf , the underlying resource.RefreshStateFunc should return nil, \"\", nil instead of the API \"not found\" error. This way the StateChangeConf logic will automatically retry. If the Create function uses resource.Retry() , the API \"not found\" error should be caught and return resource.RetryableError(err) to automatically retry. In rare cases, it may be easier to duplicate all Read function logic in the Create function to handle all retries in one place. Resource Attribute Value Waiters # An emergent solution for handling eventual consistency with attribute values on updates is to introduce a custom resource.StateChangeConf and resource.RefreshStateFunc handlers. For example: // internal/service/example/status.go (created if does not exist) // ThingAttribute fetches the Thing and its Attribute func ThingAttribute ( conn * example . Example , id string ) resource . StateRefreshFunc { return func () ( interface {}, string , error ) { output , err := /* ... AWS Go SDK operation to fetch resource/value ... */ if tfawserr . ErrCodeEquals ( err , example . ErrCodeResourceNotFoundException ) { return nil , \"\" , nil } if err != nil { return nil , \"\" , err } if output == nil { return nil , \"\" , nil } return output , aws . StringValue ( output . Attribute ), nil } } // internal/service/example/wait.go (created if does not exist) const ( ThingAttributePropagationTimeout = 2 * time . Minute ) // ThingAttributeUpdated is an attribute waiter for ThingAttribute func ThingAttributeUpdated ( conn * example . Example , id string , expectedValue string ) ( * example . Thing , error ) { stateConf := & resource . StateChangeConf { Target : [] string { expectedValue }, Refresh : ThingAttribute ( conn , id ), Timeout : ThingAttributePropagationTimeout , } outputRaw , err := stateConf . WaitForState () if output , ok := outputRaw .( * example . Thing ); ok { return output , err } return nil , err } // internal/service/{service}/{thing}.go function ExampleThingUpdate ( d * schema . ResourceData , meta interface {}) error { // ... d . HasChange ( \"attribute\" ) { // ... AWS Go SDK logic to update attribute ... if _ , err := ThingAttributeUpdated ( conn , d . Id (), d . Get ( \"attribute\" ).( string )); err != nil { return fmt . Errorf ( \"waiting for Example Thing (%s) attribute update: %w\" , d . Id (), err ) } } // ... } Asynchronous Operations # When you initiate a long-running operation, an AWS service may return a successful response immediately and continue working on the request asynchronously. A resource can track the status with a component-level field (e.g., CREATING , UPDATING , etc.) or an explicit tracking identifier. Terraform resources should wait for these background operations to complete. Failing to do so can introduce incomplete state information and downstream errors in other resources. In rare scenarios involving very long-running operations, operators may request a flag to skip the waiting. However, these should only be implemented case-by-case to prevent those previously mentioned confusing issues. AWS Go SDK Waiters # In limited cases, the AWS service API model includes the information to automatically generate a waiter function in the AWS Go SDK for an operation. These are typically named with the prefix WaitUntil... . If available, these functions can be used for an initial resource implementation. For example: if err := conn . WaitUntilEndpointInService ( input ); err != nil { return fmt . Errorf ( \"waiting for Example Thing (%s) ...: %w\" , d . Id (), err ) } If it is necessary to customize the timeouts and polling, we generally prefer using Resource Lifecycle Waiters instead since they are more commonly used throughout the codebase. Resource Lifecycle Waiters # Most of the codebase uses resource.StateChangeConf and resource.RefreshStateFunc handlers for tracking either component level status fields or explicit tracking identifiers. These should be placed in the internal/service/{SERVICE} package and split into separate functions. For example: // internal/service/example/status.go (created if does not exist) // ThingStatus fetches the Thing and its Status func ThingStatus ( conn * example . Example , id string ) resource . StateRefreshFunc { return func () ( interface {}, string , error ) { output , err := /* ... AWS Go SDK operation to fetch resource/status ... */ if tfawserr . ErrCodeEquals ( err , example . ErrCodeResourceNotFoundException ) { return nil , \"\" , nil } if err != nil { return nil , \"\" , err } if output == nil { return nil , \"\" , nil } return output , aws . StringValue ( output . Status ), nil } } // internal/service/example/wait.go (created if does not exist) const ( ThingCreationTimeout = 2 * time . Minute ThingDeletionTimeout = 5 * time . Minute ) // ThingCreated is a resource waiter for Thing creation func ThingCreated ( conn * example . Example , id string ) ( * example . Thing , error ) { stateConf := & resource . StateChangeConf { Pending : [] string { example . StatusCreating }, Target : [] string { example . StatusCreated }, Refresh : ThingStatus ( conn , id ), Timeout : ThingCreationTimeout , } outputRaw , err := stateConf . WaitForState () if output , ok := outputRaw .( * example . Thing ); ok { return output , err } return nil , err } // ThingDeleted is a resource waiter for Thing deletion func ThingDeleted ( conn * example . Example , id string ) ( * example . Thing , error ) { stateConf := & resource . StateChangeConf { Pending : [] string { example . StatusDeleting }, Target : [] string {}, // Use empty list if the resource disappears and does not have \"deleted\" status Refresh : ThingStatus ( conn , id ), Timeout : ThingDeletionTimeout , } outputRaw , err := stateConf . WaitForState () if output , ok := outputRaw .( * example . Thing ); ok { return output , err } return nil , err } // internal/service/{service}/{thing}.go function ExampleThingCreate ( d * schema . ResourceData , meta interface {}) error { // ... AWS Go SDK logic to create resource ... if _ , err := ThingCreated ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for Example Thing (%s) creation: %w\" , d . Id (), err ) } return ExampleThingRead ( d , meta ) } function ExampleThingDelete ( d * schema . ResourceData , meta interface {}) error { // ... AWS Go SDK logic to delete resource ... if _ , err := ThingDeleted ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for Example Thing (%s) deletion: %w\" , d . Id (), err ) } return ExampleThingRead ( d , meta ) } Typically, the AWS Go SDK should include constants for various status field values (e.g., StatusCreating for CREATING ). If not, create them in a file named internal/service/{SERVICE}/consts.go .","title":"Retries and Waiters"},{"location":"retries-and-waiters/#retries-and-waiters","text":"Terraform plugins may run into situations where calling the remote system after an operation may be necessary. These typically fall under three classes where: The request never reaches the remote system. The request reaches the remote system and responds that it cannot handle the request temporarily. The implementation of the remote system requires additional requests to ensure success. This guide describes the behavior of the Terraform AWS Provider and provides code implementations that help ensure success in each of these situations.","title":"Retries and Waiters"},{"location":"retries-and-waiters/#terraform-plugin-sdk-functionality","text":"The Terraform Plugin SDK , which the AWS Provider uses, provides vital tools for handling consistency: the resource.StateChangeConf{} struct, and the retry functions, resource.Retry() and resource.RetryContext() . We will discuss these throughout the rest of this guide. Since they help keep the AWS Provider code consistent, we heavily prefer them over custom implementations. This guide goes beyond the Terraform Plugin SDK v2 documentation by providing additional context and emergent implementations specific to the Terraform AWS Provider.","title":"Terraform Plugin SDK Functionality"},{"location":"retries-and-waiters/#state-change-configuration-and-functions","text":"The resource.StateChangeConf type along with its receiver methods WaitForState() and WaitForStateContext() is a generic primitive for repeating operations in Terraform resource logic until desired value(s) are received. The \"state change\" in this case is generic to any value and not specific to the Terraform State. Among other functionality, it supports some of these desirable optional properties: Expecting specific value(s) while waiting for the target value(s) to be reached. Unexpected values are returned as an error which can be augmented with additional details. Expecting the target value(s) to be returned multiple times in succession. Allowing various polling configurations such as delaying the initial request and setting the time between polls.","title":"State Change Configuration and Functions"},{"location":"retries-and-waiters/#retry-functions","text":"The resource.Retry() and resource.RetryContext() functions provide a simplified retry implementation around resource.StateChangeConf . Their most common use is for simple error-based retries.","title":"Retry Functions"},{"location":"retries-and-waiters/#aws-request-handling","text":"The Terraform AWS Provider's requests to AWS service APIs happen on top of Hypertext Transfer Protocol (HTTP). The following is a simplified description of the layers and handling that requests pass through: A Terraform resource calls an AWS Go SDK function. The AWS Go SDK generates an AWS-compatible HTTP request using the Go standard library net/http package . This includes the following: Adding HTTP headers for authentication and signing of requests to ensure authenticity. Converting operation inputs into required HTTP URI parameters and/or request body type (XML or JSON). If debug logging is enabled, logging of the HTTP request. The AWS Go SDK transmits the net/http request using Go's standard handling of the Operating System (OS) and Domain Name System (DNS) configuration. The AWS service potentially receives the request and responds, typically adding a request identifier HTTP header which can be used for AWS Support cases. The OS and Go net/http receive the response and pass it to the AWS Go SDK. The AWS Go SDK attempts to handle the response. This may include: Parsing output Converting errors into operation errors (Go error type of wrapped awserr.Error type ). Converting response elements into operation outputs (AWS Go SDK operation-specific types). Triggering automatic request retries based on default and custom logic. The Terraform resource receives the response, including any output and errors, from the AWS Go SDK. The Terraform AWS Provider specific configuration for AWS Go SDK operation handling can be found in aws/conns/conns.go in this codebase and the hashicorp/aws-sdk-go-base codebase . NOTE: The section descibes the current handling with version 1 of the AWS Go SDK. In the future, this codebase will be migrated to version 2 of the AWS Go SDK. The newer version implements a very similar request flow but uses a simpler credential and request handling configuration. As such, the aws-sdk-go-base dependency will likely not receive further updates and will be removed after that migration.","title":"AWS Request Handling"},{"location":"retries-and-waiters/#default-aws-go-sdk-retries","text":"In some situations, while handling a response, the AWS Go SDK automatically retries a request before returning the output and error. The retry mechanism implements an exponential backoff algorithm. The default conditions triggering automatic retries (implemented through client.DefaultRetryer ) include: Certain network errors. A common exception to this is connection reset errors. HTTP status codes 429 and 5xx. Certain API error codes, which are common across various AWS services (e.g., ThrottledException ). However, not all AWS services implement these error codes consistently. A common exception to this is certain expired credentials errors. By default, the Terraform AWS Provider sets the maximum number of AWS Go SDK retries based on the max_retries provider configuration . The provider configuration defaults to 25 and the exponential backoff roughly equates to one hour of retries. This very high default value was present before the Terraform AWS Provider codebase was split from Terraform CLI in version 0.10. NOTE: The section describes the current handling with version 1 of the AWS Go SDK. In the future, this codebase will be migrated to version 2 of the AWS Go SDK. The newer version implements additional retry conditions by default, such as consistently retrying all common network errors. NOTE: The section describes the current handling with Terraform Plugin SDK resource signatures without context.Context . In the future, this codebase will be migrated to the context-aware resource signatures which currently enforce a 20-minute default timeout that conflicts with the timeout with the default max_retries value. The Terraform Plugin SDK may be updated to support removing this default 20-minute timeout or the default retry mechanism described here will be updated to prevent context cancellation errors where possible.","title":"Default AWS Go SDK Retries"},{"location":"retries-and-waiters/#lower-network-error-retries","text":"Given the very high default number of AWS Go SDK retries configured in the Terraform AWS Provider and the excessive wait that practitioners would face, the hashicorp/aws-sdk-go-base codebase lowers retries to 10 for certain network errors that typically cannot be remediated via retries. This roughly equates to 30 seconds of retries.","title":"Lower Network Error Retries"},{"location":"retries-and-waiters/#terraform-aws-provider-service-retries","text":"The AWS Go SDK provides hooks for injecting custom logic into the service client handlers. We prefer this handling in situations where contributors would need to apply the retry behavior to many resources. For example, in cases where the AWS service API does not mark an error code as automatically retriable. The AWS Provider includes other retry-changing behaviors using this method. You can find them in the aws/config.go file. For example: client . kafkaconn . Handlers . Retry . PushBack ( func ( r * request . Request ) { if tfawserr . ErrMessageContains ( r . Error , kafka . ErrCodeTooManyRequestsException , \"Too Many Requests\" ) { r . Retryable = aws . Bool ( true ) } })","title":"Terraform AWS Provider Service Retries"},{"location":"retries-and-waiters/#eventual-consistency","text":"Eventual consistency is a temporary condition where the remote system can return outdated information or errors due to not being strongly read-after-write consistent. This is a pattern found in remote systems that must be highly scaled for broad usage. Terraform expects any planned resource lifecycle change (create, update, destroy of the resource itself) and planned resource attribute value change to match after being applied. Conversely, operators typically expect that Terraform resources also implement the concept of drift detection for resources and their attributes, which requires reading information back from the remote system after an operation. A common implementation is refreshing the Terraform State information ( d.Set() ) during the Read function of a resource after Create and Update . These two concepts conflict with each other and require additional handling in Terraform resource logic as shown in the following sections. These issues are not reliably reproducible, especially in the case of writing acceptance testing, so they can be elusive with false positives to verify fixes.","title":"Eventual Consistency"},{"location":"retries-and-waiters/#operation-specific-error-retries","text":"Even given a properly ordered Terraform configuration, eventual consistency can unexpectedly prevent downstream operations from succeeding. A simple retry after a few seconds resolves many of these issues. To reduce frustrating behavior for operators, wrap AWS Go SDK operations with the resource.Retry() or resource.RetryContext() functions. These retries should have a reasonably low timeout (typically two minutes but up to five minutes). Save them in a constant for reusability. These functions are preferably in line with the associated resource logic to remove any indirection with the code. Do not use this type of logic to overcome improperly ordered Terraform configurations. The approach may not work in larger environments. // internal/service/example/wait.go (created if does not exist) const ( // Maximum amount of time to wait for Thing operation eventual consistency ThingOperationTimeout = 2 * time . Minute ) // internal/service/{service}/{thing}.go // ... Create, Read, Update, or Delete function ... err := resource . Retry ( ThingOperationTimeout , func () * resource . RetryError { _ , err := conn . /* ... AWS Go SDK operation with eventual consistency errors ... */ // Retryable conditions which can be checked. // These must be updated to match the AWS service API error code and message. if tfawserr . ErrMessageContains ( err , /* error code */ , /* error message */ ) { return resource . RetryableError ( err ) } if err != nil { return resource . NonRetryableError ( err ) } return nil }) // This check is important - it handles when the AWS Go SDK operation retries without returning. // e.g., any automatic retries due to network or throttling errors. if tfresource . TimedOut ( err ) { // The use of equals assignment (over colon equals) is also important here. // This overwrites the error variable to simplify logic. _ , err = conn . /* ... AWS Go SDK operation with IAM eventual consistency errors ... */ } if err != nil { return fmt . Errorf ( \"... error message context ... : %w\" , err ) } NOTE: The section descibes the current handling with version 1 of the AWS Go SDK. In the future, this codebase will be migrated to version 2 of the AWS Go SDK. The newer version natively supports operation-specific retries in a more friendly manner, which may replace this type of implementation.","title":"Operation Specific Error Retries"},{"location":"retries-and-waiters/#iam-error-retries","text":"A common eventual consistency issue is an error returned due to IAM permissions. The IAM service itself is eventually consistent along with the propagation of its components and permissions to other AWS services. For example, if the following operations occur in quick succession: Create an IAM Role Attach an IAM Policy to the IAM Role Reference the new IAM Role in another AWS service, such as creating a Lambda Function The last operation can receive varied API errors ranging from: IAM Role being reported as not existing IAM Role being reported as not having permissions for the other service to use it (assume role permissions) IAM Role being reported as not having sufficient permissions (inline or attached role permissions) Each AWS service API (and sometimes even operations within the same API) varies in the implementation of these errors. To handle them, it is recommended to use the Operation Specific Error Retries pattern. The Terraform AWS Provider implements a standard timeout constant of two minutes in the internal/service/iam package which should be used for all retry timeouts associated with IAM errors. This timeout was derived from years of Terraform operational experience with all AWS APIs. // internal/service/{service}/{thing}.go import ( // ... other imports ... tfiam \"github.com/hashicorp/terraform-provider-aws/internal/service/iam\" ) // ... Create and typically Update function ... err := resource . Retry ( iamwaiter . PropagationTimeout , func () * resource . RetryError { _ , err := conn . /* ... AWS Go SDK operation with IAM eventual consistency errors ... */ // Example retryable condition // This must be updated to match the AWS service API error code and message. if tfawserr . ErrMessageContains ( err , /* error code */ , /* error message */ ) { return resource . RetryableError ( err ) } if err != nil { return resource . NonRetryableError ( err ) } return nil }) if tfresource . TimedOut ( err ) { _ , err = conn . /* ... AWS Go SDK operation with IAM eventual consistency errors ... */ } if err != nil { return fmt . Errorf ( \"... error message context ... : %w\" , err ) }","title":"IAM Error Retries"},{"location":"retries-and-waiters/#asynchronous-operation-error-retries","text":"Some remote system operations run asynchronously as detailed in the Asynchronous Operations section . In these cases, it is possible that the initial operation will immediately return as successful, but potentially return a retryable failure while checking the operation status that requires starting everything over. The handling for these is complicated by the fact that there are two timeouts, one for the retryable failure and one for the asynchronous operation status checking. The below code example highlights this situation for a resource creation that also exhibited IAM eventual consistency. // internal/service/{service}/{thing}.go import ( // ... other imports ... tfiam \"github.com/hashicorp/terraform-provider-aws/internal/service/iam\" ) // ... Create function ... // Underlying IAM eventual consistency errors can occur after the creation // operation. The goal is only retry these types of errors up to the IAM // timeout. Since the creation process is asynchronous and can take up to // its own timeout, we store a stop time upfront for checking. iamwaiterStopTime := time . Now (). Add ( tfiam . PropagationTimeout ) // Ensure to add IAM eventual consistency timeout in case of retries err = resource . Retry ( tfiam . PropagationTimeout + ThingOperationTimeout , func () * resource . RetryError { // Only retry IAM eventual consistency errors up to that timeout iamwaiterRetry := time . Now (). Before ( iamwaiterStopTime ) _ , err := conn . /* ... AWS Go SDK operation without eventual consistency errors ... */ if err != nil { return resource . NonRetryableError ( err ) } _ , err = ThingOperation ( conn , d . Id ()) if err != nil { if iamwaiterRetry && /* eventual consistency error checking */ { return resource . RetryableError ( err ) } return resource . NonRetryableError ( err ) } return nil }) if tfresource . TimedOut ( err ) { _ , err = conn . /* ... AWS Go SDK operation without eventual consistency errors ... */ if err != nil { return err } _ , err = ThingOperation ( conn , d . Id ()) if err != nil { return err } }","title":"Asynchronous Operation Error Retries"},{"location":"retries-and-waiters/#resource-lifecycle-retries","text":"Resource lifecycle eventual consistency is a type of consistency issue that relates to the existence or state of an AWS infrastructure component. For example, if you create a resource and immediately try to get information about it, some AWS services and operations will return a \"not found\" error. Depending on the service and general AWS load, these errors can be frequent or rare. In order to avoid this issue, identify operations that make changes. Then, when calling any other operations that rely on the changes, account for the possibility that the AWS service has not yet fully realized them. A typical example is creating an AWS component. After creation, when attempting to read the component's information, provide logic to retry the read if the AWS service returns a \"not found\" error. The pattern that most resources should follow is to have the Create function return calling the Read function. This fills in computed attributes and ensures that the AWS service applied the configuration correctly. Add retry logic to the Read function to overcome the temporary condition on resource creation. Note that for eventually consistent resources, \"not found\" errors can still occur in the Read function even after implementing Resource Lifecycle Waiters for the Create function. // internal/service/example/wait.go (created if does not exist) const ( // Maximum amount of time to wait for Thing eventual consistency on creation ThingCreationTimeout = 2 * time . Minute ) // internal/service/{service}/{thing}.go function ExampleThingCreate ( d * schema . ResourceData , meta interface {}) error { // ... return ExampleThingRead ( d , meta ) } function ExampleThingRead ( d * schema . ResourceData , meta interface {}) error { conn := meta .( * AWSClient ). exampleconn input := & example . OperationInput { /* ... */ } var output * example . OperationOutput err := resource . Retry ( ThingCreationTimeout , func () * resource . RetryError { var err error output , err = conn . Operation ( input ) // Retry on any API \"not found\" errors, but only on new resources. if d . IsNewResource () && tfawserr . ErrorCodeEquals ( err , example . ErrCodeResourceNotFoundException ) { return resource . RetryableError ( err ) } if err != nil { return resource . NonRetryableError ( err ) } return nil }) // Retry AWS Go SDK operation if no response from automatic retries. if tfresource . TimedOut ( err ) { output , err = exampleconn . Operation ( input ) } // Prevent confusing Terraform error messaging to operators by // Only ignoring API \"not found\" errors if not a new resource. if ! d . IsNewResource () && tfawserr . ErrorCodeEquals ( err , example . ErrCodeNoSuchEntityException ) { log . Printf ( \"[WARN] Example Thing (%s) not found, removing from state\" , d . Id ()) d . SetId ( \"\" ) return nil } if err != nil { return fmt . Errorf ( \"reading Example Thing (%s): %w\" , d . Id (), err ) } // Prevent panics. if output == nil { return fmt . Errorf ( \"reading Example Thing (%s): empty response\" , d . Id ()) } // ... refresh Terraform state as normal ... d . Set ( \"arn\" , output . Arn ) } Some other general guidelines are: If the Create function uses resource.StateChangeConf , the underlying resource.RefreshStateFunc should return nil, \"\", nil instead of the API \"not found\" error. This way the StateChangeConf logic will automatically retry. If the Create function uses resource.Retry() , the API \"not found\" error should be caught and return resource.RetryableError(err) to automatically retry. In rare cases, it may be easier to duplicate all Read function logic in the Create function to handle all retries in one place.","title":"Resource Lifecycle Retries"},{"location":"retries-and-waiters/#resource-attribute-value-waiters","text":"An emergent solution for handling eventual consistency with attribute values on updates is to introduce a custom resource.StateChangeConf and resource.RefreshStateFunc handlers. For example: // internal/service/example/status.go (created if does not exist) // ThingAttribute fetches the Thing and its Attribute func ThingAttribute ( conn * example . Example , id string ) resource . StateRefreshFunc { return func () ( interface {}, string , error ) { output , err := /* ... AWS Go SDK operation to fetch resource/value ... */ if tfawserr . ErrCodeEquals ( err , example . ErrCodeResourceNotFoundException ) { return nil , \"\" , nil } if err != nil { return nil , \"\" , err } if output == nil { return nil , \"\" , nil } return output , aws . StringValue ( output . Attribute ), nil } } // internal/service/example/wait.go (created if does not exist) const ( ThingAttributePropagationTimeout = 2 * time . Minute ) // ThingAttributeUpdated is an attribute waiter for ThingAttribute func ThingAttributeUpdated ( conn * example . Example , id string , expectedValue string ) ( * example . Thing , error ) { stateConf := & resource . StateChangeConf { Target : [] string { expectedValue }, Refresh : ThingAttribute ( conn , id ), Timeout : ThingAttributePropagationTimeout , } outputRaw , err := stateConf . WaitForState () if output , ok := outputRaw .( * example . Thing ); ok { return output , err } return nil , err } // internal/service/{service}/{thing}.go function ExampleThingUpdate ( d * schema . ResourceData , meta interface {}) error { // ... d . HasChange ( \"attribute\" ) { // ... AWS Go SDK logic to update attribute ... if _ , err := ThingAttributeUpdated ( conn , d . Id (), d . Get ( \"attribute\" ).( string )); err != nil { return fmt . Errorf ( \"waiting for Example Thing (%s) attribute update: %w\" , d . Id (), err ) } } // ... }","title":"Resource Attribute Value Waiters"},{"location":"retries-and-waiters/#asynchronous-operations","text":"When you initiate a long-running operation, an AWS service may return a successful response immediately and continue working on the request asynchronously. A resource can track the status with a component-level field (e.g., CREATING , UPDATING , etc.) or an explicit tracking identifier. Terraform resources should wait for these background operations to complete. Failing to do so can introduce incomplete state information and downstream errors in other resources. In rare scenarios involving very long-running operations, operators may request a flag to skip the waiting. However, these should only be implemented case-by-case to prevent those previously mentioned confusing issues.","title":"Asynchronous Operations"},{"location":"retries-and-waiters/#aws-go-sdk-waiters","text":"In limited cases, the AWS service API model includes the information to automatically generate a waiter function in the AWS Go SDK for an operation. These are typically named with the prefix WaitUntil... . If available, these functions can be used for an initial resource implementation. For example: if err := conn . WaitUntilEndpointInService ( input ); err != nil { return fmt . Errorf ( \"waiting for Example Thing (%s) ...: %w\" , d . Id (), err ) } If it is necessary to customize the timeouts and polling, we generally prefer using Resource Lifecycle Waiters instead since they are more commonly used throughout the codebase.","title":"AWS Go SDK Waiters"},{"location":"retries-and-waiters/#resource-lifecycle-waiters","text":"Most of the codebase uses resource.StateChangeConf and resource.RefreshStateFunc handlers for tracking either component level status fields or explicit tracking identifiers. These should be placed in the internal/service/{SERVICE} package and split into separate functions. For example: // internal/service/example/status.go (created if does not exist) // ThingStatus fetches the Thing and its Status func ThingStatus ( conn * example . Example , id string ) resource . StateRefreshFunc { return func () ( interface {}, string , error ) { output , err := /* ... AWS Go SDK operation to fetch resource/status ... */ if tfawserr . ErrCodeEquals ( err , example . ErrCodeResourceNotFoundException ) { return nil , \"\" , nil } if err != nil { return nil , \"\" , err } if output == nil { return nil , \"\" , nil } return output , aws . StringValue ( output . Status ), nil } } // internal/service/example/wait.go (created if does not exist) const ( ThingCreationTimeout = 2 * time . Minute ThingDeletionTimeout = 5 * time . Minute ) // ThingCreated is a resource waiter for Thing creation func ThingCreated ( conn * example . Example , id string ) ( * example . Thing , error ) { stateConf := & resource . StateChangeConf { Pending : [] string { example . StatusCreating }, Target : [] string { example . StatusCreated }, Refresh : ThingStatus ( conn , id ), Timeout : ThingCreationTimeout , } outputRaw , err := stateConf . WaitForState () if output , ok := outputRaw .( * example . Thing ); ok { return output , err } return nil , err } // ThingDeleted is a resource waiter for Thing deletion func ThingDeleted ( conn * example . Example , id string ) ( * example . Thing , error ) { stateConf := & resource . StateChangeConf { Pending : [] string { example . StatusDeleting }, Target : [] string {}, // Use empty list if the resource disappears and does not have \"deleted\" status Refresh : ThingStatus ( conn , id ), Timeout : ThingDeletionTimeout , } outputRaw , err := stateConf . WaitForState () if output , ok := outputRaw .( * example . Thing ); ok { return output , err } return nil , err } // internal/service/{service}/{thing}.go function ExampleThingCreate ( d * schema . ResourceData , meta interface {}) error { // ... AWS Go SDK logic to create resource ... if _ , err := ThingCreated ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for Example Thing (%s) creation: %w\" , d . Id (), err ) } return ExampleThingRead ( d , meta ) } function ExampleThingDelete ( d * schema . ResourceData , meta interface {}) error { // ... AWS Go SDK logic to delete resource ... if _ , err := ThingDeleted ( conn , d . Id ()); err != nil { return fmt . Errorf ( \"waiting for Example Thing (%s) deletion: %w\" , d . Id (), err ) } return ExampleThingRead ( d , meta ) } Typically, the AWS Go SDK should include constants for various status field values (e.g., StatusCreating for CREATING ). If not, create them in a file named internal/service/{SERVICE}/consts.go .","title":"Resource Lifecycle Waiters"},{"location":"running-and-writing-acceptance-tests/","text":"Running and Writing Acceptance Tests # Terraform includes an acceptance test harness that does most of the repetitive work involved in testing a resource. For additional information about testing Terraform Providers, see the SDKv2 documentation . Acceptance Tests Often Cost Money to Run # Our acceptance test suite creates real resources, and as a result they cost real money to run. Because the resources only exist for a short period of time, the total amount of money required is usually a relatively small amount. That said there are particular services which are very expensive to run and its important to be prepared for those costs. Some services which can be cost prohibitive include (among others): WorkSpaces Glue OpenSearch RDS ACM (Amazon Certificate Manager) FSx Kinesis Analytics EC2 ElastiCache Storage Gateway We don't want financial limitations to be a barrier to contribution, so if you are unable to pay to run acceptance tests for your contribution, mention this in your pull request. We will happily accept \"best effort\" implementations of acceptance tests and run them for you on our side. This might mean that your PR takes a bit longer to merge, but it most definitely is not a blocker for contributions. Running an Acceptance Test # Acceptance tests can be run using the testacc target in the Terraform Makefile . The individual tests to run can be controlled using a regular expression. Prior to running the tests provider configuration details such as access keys must be made available as environment variables. For example, to run an acceptance test against the Amazon Web Services provider, the following environment variables must be set: # Using a profile export AWS_PROFILE = ... # Otherwise export AWS_ACCESS_KEY_ID = ... export AWS_SECRET_ACCESS_KEY = ... export AWS_DEFAULT_REGION = ... Please note that the default region for the testing is us-west-2 and must be overridden via the AWS_DEFAULT_REGION environment variable, if necessary. This is especially important for testing AWS GovCloud (US), which requires: export AWS_DEFAULT_REGION = us-gov-west-1 Tests can then be run by specifying a regular expression defining the tests to run and the package in which the tests are defined: $ make testacc TESTS = TestAccCloudWatchDashboard_updateName PKG = cloudwatch == > Checking that code complies with gofmt requirements... TF_ACC = 1 go test ./internal/service/cloudwatch/... -v -count 1 -parallel 20 -run = TestAccCloudWatchDashboard_updateName -timeout 180m === RUN TestAccCloudWatchDashboard_updateName === PAUSE TestAccCloudWatchDashboard_updateName === CONT TestAccCloudWatchDashboard_updateName --- PASS: TestAccCloudWatchDashboard_updateName ( 25 .33s ) PASS ok github.com/hashicorp/terraform-provider-aws/internal/service/cloudwatch 25 .387s Entire resource test suites can be targeted by using the naming convention to write the regular expression. For example, to run all tests of the aws_cloudwatch_dashboard resource rather than just the updateName test, you can start testing like this: $ make testacc TESTS = TestAccCloudWatchDashboard PKG = cloudwatch == > Checking that code complies with gofmt requirements... TF_ACC = 1 go test ./internal/service/cloudwatch/... -v -count 1 -parallel 20 -run = TestAccCloudWatchDashboard -timeout 180m === RUN TestAccCloudWatchDashboard_basic === PAUSE TestAccCloudWatchDashboard_basic === RUN TestAccCloudWatchDashboard_update === PAUSE TestAccCloudWatchDashboard_update === RUN TestAccCloudWatchDashboard_updateName === PAUSE TestAccCloudWatchDashboard_updateName === CONT TestAccCloudWatchDashboard_basic === CONT TestAccCloudWatchDashboard_updateName === CONT TestAccCloudWatchDashboard_update --- PASS: TestAccCloudWatchDashboard_basic ( 15 .83s ) --- PASS: TestAccCloudWatchDashboard_updateName ( 26 .69s ) --- PASS: TestAccCloudWatchDashboard_update ( 27 .72s ) PASS ok github.com/hashicorp/terraform-provider-aws/internal/service/cloudwatch 27 .783s Running acceptance tests requires version 0.12.26 or higher of the Terraform CLI to be installed. For advanced developers, the acceptance testing framework accepts some additional environment variables that can be used to control Terraform CLI binary selection, logging, and other behaviors. See the SDKv2 documentation for more information. Please Note: On macOS 10.14 and later (and some Linux distributions), the default user open file limit is 256. This may cause unexpected issues when running the acceptance testing since this can prevent various operations from occurring such as opening network connections to AWS. To view this limit, the ulimit -n command can be run. To update this limit, run ulimit -n 1024 (or higher). Running Cross-Account Tests # Certain testing requires multiple AWS accounts. This additional setup is not typically required and the testing will return an error (shown below) if your current setup does not have the secondary AWS configuration: $ make testacc TESTS = TestAccRDSInstance_DBSubnetGroupName_ramShared PKG = rds TF_ACC=1 go test ./internal/service/rds/... -v -count 1 -parallel 20 -run=TestAccRDSInstance_DBSubnetGroupName_ramShared -timeout 180m === RUN TestAccRDSInstance_DBSubnetGroupName_ramShared === PAUSE TestAccRDSInstance_DBSubnetGroupName_ramShared === CONT TestAccRDSInstance_DBSubnetGroupName_ramShared acctest.go:674: skipping test because at least one environment variable of [AWS_ALTERNATE_PROFILE AWS_ALTERNATE_ACCESS_KEY_ID] must be set. Usage: credentials for running acceptance testing in alternate AWS account. --- SKIP: TestAccRDSInstance_DBSubnetGroupName_ramShared (0.85s) PASS ok github.com/hashicorp/terraform-provider-aws/internal/service/rds 0.888s Running these acceptance tests is the same as before, except the following additional AWS credential information is required: # Using a profile export AWS_ALTERNATE_PROFILE = ... # Otherwise export AWS_ALTERNATE_ACCESS_KEY_ID = ... export AWS_ALTERNATE_SECRET_ACCESS_KEY = ... Running Cross-Region Tests # Certain testing requires multiple AWS regions. Additional setup is not typically required because the testing defaults the second AWS region to us-east-1 and the third AWS region to us-east-2 . Running these acceptance tests is the same as before, but if you wish to override the second and third regions: export AWS_ALTERNATE_REGION = ... export AWS_THIRD_REGION = ... Running Only Short Tests # Some tests have been manually marked as long-running (longer than 300 seconds) and can be skipped using the -short flag. However, we are adding long-running guards little by little and many services have no guarded tests. Where guards have been implemented, do not always skip long-running tests. However, for intermediate test runs during development, or to verify functionality unrelated to the specific long-running tests, skipping long-running tests makes work more efficient. We recommend that for the final test run before submitting a PR that you run affected tests without the -short flag. If you want to run only short-running tests, you can use either one of these equivalent statements. Note the use of -short . For example: $ make testacc TESTS = 'TestAccECSTaskDefinition_' PKG = ecs TESTARGS = -short Or: $ TF_ACC = 1 go test ./internal/service/ecs/... -v -count 1 -parallel 20 -run = 'TestAccECSTaskDefinition_' -short -timeout 180m Writing an Acceptance Test # Terraform has a framework for writing acceptance tests which minimizes the amount of boilerplate code necessary to use common testing patterns. This guide is meant to augment the general SDKv2 documentation with Terraform AWS Provider specific conventions and helpers. Anatomy of an Acceptance Test # This section describes in detail how the Terraform acceptance testing framework operates with respect to the Terraform AWS Provider. We recommend those unfamiliar with this provider, or Terraform resource testing in general, take a look here first to generally understand how we interact with AWS and the resource code to verify functionality. The entry point to the framework is the resource.ParallelTest() function. This wraps our testing to work with the standard Go testing framework, while also preventing unexpected usage of AWS by requiring the TF_ACC=1 environment variable. This function accepts a TestCase parameter, which has all the details about the test itself. For example, this includes the test steps ( TestSteps ) and how to verify resource deletion in the API after all steps have been run ( CheckDestroy ). Each TestStep proceeds by applying some Terraform configuration using the provider under test, and then verifying that results are as expected by making assertions using the provider API. It is common for a single test function to exercise both the creation of and updates to a single resource. Most tests follow a similar structure. Pre-flight checks are made to ensure that sufficient provider configuration is available to be able to proceed - for example in an acceptance test targeting AWS, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY must be set prior to running acceptance tests. This is common to all tests exercising a single provider. Most assertion functions are defined out of band with the tests. This keeps the tests readable, and allows reuse of assertion functions across different tests of the same type of resource. The definition of a complete test looks like this: func TestAccCloudWatchDashboard_basic ( t * testing . T ) { var dashboard cloudwatch . GetDashboardOutput rInt := acctest . RandInt () resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , cloudwatch . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckDashboardDestroy , Steps : [] resource . TestStep { { Config : testAccDashboardConfig ( rInt ), Check : resource . ComposeTestCheckFunc ( testAccCheckDashboardExists ( \"aws_cloudwatch_dashboard.foobar\" , & dashboard ), resource . TestCheckResourceAttr ( \"aws_cloudwatch_dashboard.foobar\" , \"dashboard_name\" , testAccDashboardName ( rInt )), ), }, }, }) } When executing the test, the following steps are taken for each TestStep : The Terraform configuration required for the test is applied. This is responsible for configuring the resource under test, and any dependencies it may have. For example, to test the aws_cloudwatch_dashboard resource, a valid configuration with the requisite fields is required. This results in configuration which looks like this: resource \"aws_cloudwatch_dashboard\" \"foobar\" { dashboard_name = \"terraform-test-dashboard-%d\" dashboard_body = << EOF { \"widgets\": [{ \"type\": \"text\", \"x\": 0, \"y\": 0, \"width\": 6, \"height\": 6, \"properties\": { \"markdown\": \"Hi there from Terraform: CloudWatch\" } }] } EOF } Assertions are run using the provider API. These use the provider API directly rather than asserting against the resource state. For example, to verify that the aws_cloudwatch_dashboard described above was created successfully, a test function like this is used: func testAccCheckDashboardExists ( n string , dashboard * cloudwatch . GetDashboardOutput ) resource . TestCheckFunc { return func ( s * terraform . State ) error { rs , ok := s . RootModule (). Resources [ n ] if ! ok { return fmt . Errorf ( \"Not found: %s\" , n ) } conn := acctest . Provider . Meta ().( * conns . AWSClient ). CloudWatchConn params := cloudwatch . GetDashboardInput { DashboardName : aws . String ( rs . Primary . ID ), } resp , err := conn . GetDashboard ( & params ) if err != nil { return err } * dashboard = * resp return nil } } Notice that the only information used from the Terraform state is the ID of the resource. For computed properties, we instead assert that the value saved in the Terraform state was the expected value if possible. The testing framework provides helper functions for several common types of check - for example: ```go resource.TestCheckResourceAttr(\"aws_cloudwatch_dashboard.foobar\", \"dashboard_name\", testAccDashboardName(rInt)), ``` The resources created by the test are destroyed. This step happens automatically, and is the equivalent of calling terraform destroy . Assertions are made against the provider API to verify that the resources have indeed been removed. If these checks fail, the test fails and reports \"dangling resources\". The code to ensure that the aws_cloudwatch_dashboard shown above has been destroyed looks like this: func testAccCheckDashboardDestroy ( s * terraform . State ) error { conn := acctest . Provider . Meta ().( * conns . AWSClient ). CloudWatchConn for _ , rs := range s . RootModule (). Resources { if rs . Type != \"aws_cloudwatch_dashboard\" { continue } params := cloudwatch . GetDashboardInput { DashboardName : aws . String ( rs . Primary . ID ), } _ , err := conn . GetDashboard ( & params ) if err == nil { return fmt . Errorf ( \"Dashboard still exists: %s\" , rs . Primary . ID ) } if ! isDashboardNotFoundErr ( err ) { return err } } return nil } These functions usually test only for the resource directly under test. Resource Acceptance Testing # Most resources that implement standard Create, Read, Update, and Delete functionality should follow the pattern below. Each test type has a section that describes them in more detail: basic : This represents the bare minimum verification that the resource can be created, read, deleted, and optionally imported. disappears : A test that verifies Terraform will offer to recreate a resource if it is deleted outside of Terraform (e.g., via the Console) instead of returning an error that it cannot be found. Per Attribute : A test that verifies the resource with a single additional argument can be created, read, optionally updated (or force resource recreation), deleted, and optionally imported. The leading sections below highlight additional recommended patterns. Test Configurations # Most of the existing test configurations you will find in the Terraform AWS Provider are written in the following function-based style: func TestAccExampleThing_basic ( t * testing . T ) { // ... omitted for brevity ... resource . ParallelTest ( t , resource . TestCase { // ... omitted for brevity ... Steps : [] resource . TestStep { { Config : testAccExampleThingConfig (), // ... omitted for brevity ... }, }, }) } func testAccExampleThingConfig () string { return ` resource \"aws_example_thing\" \"test\" { # ... omitted for brevity ... } ` } Even when no values need to be passed in to the test configuration, we have found this setup to be the most flexible for allowing that to be easily implemented. Any configurable values are handled via fmt.Sprintf() . Using text/template or other templating styles is explicitly forbidden. For consistency, resources in the test configuration should be named resource \"...\" \"test\" unless multiple of that resource are necessary. We discourage re-using test configurations across test files (except for some common configuration helpers we provide) as it is much harder to discover potential testing regressions. Please also note that the newline on the first line of the configuration (before resource ) and the newline after the last line of configuration (after } ) are important to allow test configurations to be easily combined without generating Terraform configuration language syntax errors. Combining Test Configurations # We include a helper function, acctest.ConfigCompose() for iteratively building and chaining test configurations together. It accepts any number of configurations to combine them. This simplifies a single resource's testing by allowing the creation of a \"base\" test configuration for all the other test configurations (if necessary) and also allows the maintainers to curate common configurations. Each of these is described in more detail in below sections. Please note that we do discourage excessive chaining of configurations such as implementing multiple layers of \"base\" configurations. Usually these configurations are harder for maintainers and other future readers to understand due to the multiple levels of indirection. Base Test Configurations # If a resource requires the same Terraform configuration as a prerequisite for all test configurations, then a common pattern is implementing a \"base\" test configuration that is combined with each test configuration. For example: func testAccExampleThingConfigBase () string { return ` resource \"aws_iam_role\" \"test\" { # ... omitted for brevity ... } resource \"aws_iam_role_policy\" \"test\" { # ... omitted for brevity ... } ` } func testAccExampleThingConfig () string { return acctest . ConfigCompose ( testAccExampleThingConfigBase (), ` resource \"aws_example_thing\" \"test\" { # ... omitted for brevity ... } ` ) } Available Common Test Configurations # These test configurations are typical implementations we have found or allow testing to implement best practices easier, since the Terraform AWS Provider testing is expected to run against various AWS Regions and Partitions. acctest.AvailableEC2InstanceTypeForRegion(\"type1\", \"type2\", ...) : Typically used to replace hardcoded EC2 Instance Types. Uses aws_ec2_instance_type_offering data source to return an available EC2 Instance Type in preferred ordering. Reference the instance type via: data.aws_ec2_instance_type_offering.available.instance_type . Use acctest.AvailableEC2InstanceTypeForRegionNamed(\"name\", \"type1\", \"type2\", ...) to specify a name for the data source acctest.ConfigLatestAmazonLinuxHVMEBSAMI() : Typically used to replace hardcoded EC2 Image IDs ( ami-12345678 ). Uses aws_ami data source to find the latest Amazon Linux image. Reference the AMI ID via: data.aws_ami.amzn-ami-minimal-hvm-ebs.id Randomized Naming # For AWS resources that require unique naming, the tests should implement a randomized name, typically coded as a rName variable in the test and passed as a parameter to creating the test configuration. For example: func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) // ... omitted for brevity ... resource . ParallelTest ( t , resource . TestCase { // ... omitted for brevity ... Steps : [] resource . TestStep { { Config : testAccExampleThingConfigName ( rName ), // ... omitted for brevity ... }, }, }) } func testAccExampleThingConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q } ` , rName ) } Typically the rName is always the first argument to the test configuration function, if used, for consistency. Note that if rName (or any other variable) is used multiple times in the fmt.Sprintf() statement, do not repeat rName in the fmt.Sprintf() arguments. Using fmt.Sprintf(..., rName, rName) , for example, would not be correct. Instead, use the indexed %[1]q (or %[x]q , %[x]s , %[x]t , or %[x]d , where x represents the index number) verb multiple times. For example: func testAccExampleThingConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q tags = { Name = %[1]q } } ` , rName ) } Other Recommended Variables # We also typically recommend saving a resourceName variable in the test that contains the resource reference, e.g., aws_example_thing.test , which is repeatedly used in the checks. For example: func TestAccExampleThing_basic ( t * testing . T ) { // ... omitted for brevity ... resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { // ... omitted for brevity ... Steps : [] resource . TestStep { { // ... omitted for brevity ... Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), acctest . CheckResourceAttrRegionalARN ( resourceName , \"arn\" , \"example\" , fmt . Sprintf ( \"thing/%s\" , rName )), resource . TestCheckResourceAttr ( resourceName , \"description\" , \"\" ), resource . TestCheckResourceAttr ( resourceName , \"name\" , rName ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } // below all TestAcc functions func testAccExampleThingConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q } ` , rName ) } Basic Acceptance Tests # Usually this test is implemented first. The test configuration should contain only required arguments ( Required: true attributes) and it should check the values of all read-only attributes ( Computed: true without Optional: true ). If the resource supports it, it verifies import. It should NOT perform other TestStep such as updates or verify recreation. These are typically named TestAcc{SERVICE}{THING}_basic , e.g., TestAccCloudWatchDashboard_basic For example: func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingConfigName ( rName ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), acctest . CheckResourceAttrRegionalARN ( resourceName , \"arn\" , \"example\" , fmt . Sprintf ( \"thing/%s\" , rName )), resource . TestCheckResourceAttr ( resourceName , \"description\" , \"\" ), resource . TestCheckResourceAttr ( resourceName , \"name\" , rName ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } // below all TestAcc functions func testAccExampleThingConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q } ` , rName ) } PreChecks # Acceptance test cases have a PreCheck. The PreCheck ensures that the testing environment meets certain preconditions. If the environment does not meet the preconditions, Go skips the test. Skipping a test avoids reporting a failure and wasting resources where the test cannot succeed. Here is an example of the default PreCheck: func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, // ... additional checks follow ... }) } Extend the default PreCheck by adding calls to functions in the anonymous PreCheck function. The functions can be existing functions in the provider or custom functions you add for new capabilities. Standard Provider PreChecks # If you add a new test that has preconditions which are checked by an existing provider function, use that standard PreCheck instead of creating a new one. Some existing tests are missing standard PreChecks and you can help by adding them where appropriate. These are some of the standard provider PreChecks: acctest.PreCheckPartitionHasService(serviceId string, t *testing.T) checks whether the current partition lists the service as part of its offerings. Note: AWS may not add new or public preview services to the service list immediately. This function will return a false positive in that case. acctest.PreCheckOrganizationsAccount(t *testing.T) checks whether the current account can perform AWS Organizations tests. acctest.PreCheckAlternateAccount(t *testing.T) checks whether the environment is set up for tests across accounts. acctest.PreCheckMultipleRegion(t *testing.T, regions int) checks whether the environment is set up for tests across regions. This is an example of using a standard PreCheck function. For an established service, such as WAF or FSx, use acctest.PreCheckPartitionHasService() and the service endpoint ID to check that a partition supports the service. func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ); acctest . PreCheckPartitionHasService ( waf . EndpointsID , t ) }, // ... additional checks follow ... }) } Custom PreChecks # In situations where standard PreChecks do not test for the required preconditions, create a custom PreCheck. Below is an example of adding a custom PreCheck function. For a new or preview service that AWS does not include in the partition service list yet, you can verify the existence of the service with a simple read-only request (e.g., list all X service things). (For acceptance tests of established services, use acctest.PreCheckPartitionHasService() instead.) func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ), testAccPreCheckExample ( t ) }, // ... additional checks follow ... }) } func testAccPreCheckExample ( t * testing . T ) { conn := acctest . Provider . Meta ().( * conns . AWSClient ). ExampleConn input := & example . ListThingsInput {} _ , err := conn . ListThings ( input ) if testAccPreCheckSkipError ( err ) { t . Skipf ( \"skipping acceptance testing: %s\" , err ) } if err != nil { t . Fatalf ( \"unexpected PreCheck error: %s\" , err ) } } ErrorChecks # Acceptance test cases have an ErrorCheck. The ErrorCheck provides a chance to take a look at errors before the test fails. While most errors should result in test failure, some should not. For example, an error that indicates an API operation is not supported in a particular region should cause the test to skip instead of fail. Since errors should flow through the ErrorCheck, do not handle the vast majority of failing conditions. Instead, in ErrorCheck, focus on the rare errors that should cause a test to skip, or in other words, be ignored. Common ErrorCheck # In many situations, the common ErrorCheck is sufficient. It will skip tests for several normal occurrences such as when AWS reports a feature is not supported in the current region. Here is an example of the common ErrorCheck: func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { // PreCheck ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), // ... additional checks follow ... }) } Service-Specific ErrorChecks # However, some services have special conditions that aren't caught by the common ErrorCheck. In these cases, you can create a service-specific ErrorCheck. To add a service-specific ErrorCheck, follow these steps: Make sure there is not already an ErrorCheck for the service you have in mind. For example, search the codebase for acctest.RegisterServiceErrorCheckFunc(service.EndpointsID replacing \"service\" with the package name of the service you're working on (e.g., ec2 ). If there is already an ErrorCheck for the service, add to the existing service-specific ErrorCheck. Create the service-specific ErrorCheck in an _test.go file for the service. See the example below. Register the new service-specific ErrorCheck in the init() at the top of the _test.go file. See the example below. An example of adding a service-specific ErrorCheck: // just after the imports, create or add to the init() function func init () { acctest . RegisterServiceErrorCheck ( service . EndpointsID , testAccErrorCheckSkipService ) } // ... additional code and tests ... // this is the service-specific ErrorCheck func testAccErrorCheckSkipService ( t * testing . T ) resource . ErrorCheckFunc { return acctest . ErrorCheckSkipMessagesContaining ( t , \"Error message specific to the service that indicates unsupported features\" , \"You can include from one to many portions of error messages\" , \"Be careful to not inadvertently capture errors that should not be skipped\" , ) } Long-Running Test Guards # For any acceptance tests that typically run longer than 300 seconds (5 minutes), add a -short test guard at the top of the test function. For example: func TestAccExampleThing_longRunningTest ( t * testing . T ) { if testing . Short () { t . Skip ( \"skipping long-running test in short mode\" ) } // ... omitted for brevity ... resource . ParallelTest ( t , resource . TestCase { // ... omitted for brevity ... }) } When running acceptances tests, tests with these guards can be skipped using the Go -short flag. See Running Only Short Tests for examples. Disappears Acceptance Tests # This test is generally implemented second. It is straightforward to setup once the basic test is passing since it can reuse that test configuration. It prevents a common bug report with Terraform resources that error when they can not be found (e.g., deleted outside Terraform). These are typically named TestAcc{SERVICE}{THING}_disappears , e.g., TestAccCloudWatchDashboard_disappears For example: func TestAccExampleThing_disappears ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingConfigName ( rName ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName , & job ), acctest . CheckResourceDisappears ( acctest . Provider , ResourceExampleThing (), resourceName ), ), ExpectNonEmptyPlan : true , }, }, }) } If this test does fail, the fix for this is generally adding error handling immediately after the Read API call that catches the error and tells Terraform to remove the resource before returning the error: output , err := conn . GetThing ( input ) if isAWSErr ( err , example . ErrCodeResourceNotFound , \"\" ) { log . Printf ( \"[WARN] Example Thing (%s) not found, removing from state\" , d . Id ()) d . SetId ( \"\" ) return nil } if err != nil { return fmt . Errorf ( \"reading Example Thing (%s): %w\" , d . Id (), err ) } For children resources that are encapsulated by a parent resource, it is also preferable to verify that removing the parent resource will not generate an error either. These are typically named TestAcc{SERVICE}{THING}_disappears_{PARENT} , e.g., TestAccRoute53ZoneAssociation_disappears_Vpc func TestAccExampleChildThing_disappears_ParentThing ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) parentResourceName := \"aws_example_parent_thing.test\" resourceName := \"aws_example_child_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleChildThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingConfigName ( rName ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), acctest . CheckResourceDisappears ( acctest . Provider , ResourceExampleParentThing (), parentResourceName ), ), ExpectNonEmptyPlan : true , }, }, }) } Per Attribute Acceptance Tests # These are typically named TestAcc{SERVICE}{THING}_{ATTRIBUTE} , e.g., TestAccCloudWatchDashboard_Name For example: func TestAccExampleThing_Description ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingConfigDescription ( rName , \"description1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"description\" , \"description1\" ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, { Config : testAccExampleThingConfigDescription ( rName , \"description2\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"description\" , \"description2\" ), ), }, }, }) } // below all TestAcc functions func testAccExampleThingConfigDescription ( rName string , description string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { description = %[2]q name = %[1]q } ` , rName , description ) } Cross-Account Acceptance Tests # When testing requires AWS infrastructure in a second AWS account, the below changes to the normal setup will allow the management or reference of resources and data sources across accounts: In the PreCheck function, include acctest.PreCheckOrganizationsAccount(t) to ensure a standardized set of information is required for cross-account testing credentials Switch usage of ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories to ProtoV5ProviderFactories: acctest.ProtoV5FactoriesAlternate(t) Add acctest.ConfigAlternateAccountProvider() to the test configuration and use provider = awsalternate for cross-account resources. The resource that is the focus of the acceptance test should not use the alternate provider identification to simplify the testing setup. For any TestStep that includes ImportState: true , add the Config that matches the previous TestStep Config An example acceptance test implementation can be seen below: func TestAccExample_basic ( t * testing . T ) { resourceName := \"aws_example.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) acctest . PreCheckOrganizationsAccount ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5FactoriesAlternate ( t ), CheckDestroy : testAccCheckExampleDestroy , Steps : [] resource . TestStep { { Config : testAccExampleConfig (), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleExists ( resourceName ), // ... additional checks ... ), }, { Config : testAccExampleConfig (), ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func testAccExampleConfig () string { return acctest . ConfigAlternateAccountProvider () + fmt . Sprintf ( ` # Cross account resources should be handled by the cross account provider. # The standardized provider block to use is awsalternate as seen below. resource \"aws_cross_account_example\" \"test\" { provider = awsalternate # ... configuration ... } # The resource that is the focus of the testing should be handled by the default provider, # which is automatically done by not specifying the provider configuration in the resource. resource \"aws_example\" \"test\" { # ... configuration ... } ` ) } Searching for usage of acctest.PreCheckOrganizationsAccount in the codebase will yield real world examples of this setup in action. Cross-Region Acceptance Tests # When testing requires AWS infrastructure in a second or third AWS region, the below changes to the normal setup will allow the management or reference of resources and data sources across regions: In the PreCheck function, include acctest.PreCheckMultipleRegion(t, ###) to ensure a standardized set of information is required for cross-region testing configuration. If the infrastructure in the second AWS region is also in a second AWS account also include acctest.PreCheckOrganizationsAccount(t) Switch usage of ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories to ProtoV5ProviderFactories: acctest.ProtoV5FactoriesMultipleRegions(t, 2) (where the last parameter is number of regions, 2 or 3) Add acctest.ConfigMultipleRegionProvider(###) to the test configuration and use provider = awsalternate (and potentially provider = awsthird ) for cross-region resources. The resource that is the focus of the acceptance test should not use the alternative providers to simplify the testing setup. If the infrastructure in the second AWS region is also in a second AWS account use testAccAlternateAccountAlternateRegionProviderConfig() (EC2) instead For any TestStep that includes ImportState: true , add the Config that matches the previous TestStep Config An example acceptance test implementation can be seen below: func TestAccExample_basic ( t * testing . T ) { var providers [] * schema . Provider resourceName := \"aws_example.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) acctest . PreCheckMultipleRegion ( t , 2 ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5FactoriesMultipleRegions ( t , 2 ), CheckDestroy : testAccCheckExampleDestroy , Steps : [] resource . TestStep { { Config : testAccExampleConfig (), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleExists ( resourceName ), // ... additional checks ... ), }, { Config : testAccExampleConfig (), ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func testAccExampleConfig () string { return acctest . ConfigMultipleRegionProvider ( 2 ) + fmt . Sprintf ( ` # Cross region resources should be handled by the cross region provider. # The standardized provider is awsalternate as seen below. resource \"aws_cross_region_example\" \"test\" { provider = awsalternate # ... configuration ... } # The resource that is the focus of the testing should be handled by the default provider, # which is automatically done by not specifying the provider configuration in the resource. resource \"aws_example\" \"test\" { # ... configuration ... } ` ) } Searching for usage of acctest.PreCheckMultipleRegion in the codebase will yield real world examples of this setup in action. Service-Specific Region Acceptance Testing # Certain AWS service APIs are only available in specific AWS regions. For example as of this writing, the pricing service is available in ap-south-1 and us-east-1 , but no other regions or partitions. When encountering these types of services, the acceptance testing can be setup to automatically detect the correct region(s), while skipping the testing in unsupported partitions. To prepare the shared service functionality, create a file named internal/service/{SERVICE}/acc_test.go . A starting example with the Pricing service ( internal/service/pricing/acc_test.go ): package aws import ( \"context\" \"sync\" \"testing\" \"github.com/aws/aws-sdk-go/aws/endpoints\" \"github.com/aws/aws-sdk-go/service/pricing\" \"github.com/hashicorp/terraform-plugin-sdk/v2/diag\" \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema\" \"github.com/hashicorp/terraform-plugin-sdk/v2/terraform\" \"github.com/hashicorp/terraform-provider-aws/internal/acctest\" \"github.com/hashicorp/terraform-provider-aws/internal/provider\" ) // testAccPricingRegion is the chosen Pricing testing region // // Cached to prevent issues should multiple regions become available. var testAccPricingRegion string // testAccProviderPricing is the Pricing provider instance // // This Provider can be used in testing code for API calls without requiring // the use of saving and referencing specific ProviderFactories instances. // // testAccPreCheckPricing(t) must be called before using this provider instance. var testAccProviderPricing * schema . Provider // testAccProviderPricingConfigure ensures the provider is only configured once var testAccProviderPricingConfigure sync . Once // testAccPreCheckPricing verifies AWS credentials and that Pricing is supported func testAccPreCheckPricing ( t * testing . T ) { acctest . PreCheckPartitionHasService ( pricing . EndpointsID , t ) // Since we are outside the scope of the Terraform configuration we must // call Configure() to properly initialize the provider configuration. testAccProviderPricingConfigure . Do ( func () { testAccProviderPricing = provider . Provider () config := map [ string ] interface {}{ \"region\" : testAccGetPricingRegion (), } diags := testAccProviderPricing . Configure ( context . Background (), terraform . NewResourceConfigRaw ( config )) if diags != nil && diags . HasError () { for _ , d := range diags { if d . Severity == diag . Error { t . Fatalf ( \"error configuring Pricing provider: %s\" , d . Summary ) } } } }) } // testAccPricingRegionProviderConfig is the Terraform provider configuration for Pricing region testing // // Testing Pricing assumes no other provider configurations // are necessary and overwrites the \"aws\" provider configuration. func testAccPricingRegionProviderConfig () string { return acctest . ConfigRegionalProvider ( testAccGetPricingRegion ()) } // testAccGetPricingRegion returns the Pricing region for testing func testAccGetPricingRegion () string { if testAccPricingRegion != \"\" { return testAccPricingRegion } if rs , ok := endpoints . RegionsForService ( endpoints . DefaultPartitions (), testAccGetPartition (), pricing . ServiceName ); ok { // return available region (random if multiple) for regionID := range rs { testAccPricingRegion = regionID return testAccPricingRegion } } testAccPricingRegion = testAccGetRegion () return testAccPricingRegion } For the resource or data source acceptance tests, the key items to adjust are: Ensure TestCase uses ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories instead of ProviderFactories: acctest.ProviderFactories or Providers: acctest.Providers Add the call for the new PreCheck function (keeping acctest.PreCheck(t) ), e.g. PreCheck: func() { acctest.PreCheck(t); testAccPreCheckPricing(t) }, If the testing is for a managed resource with a CheckDestroy function, ensure it uses the new provider instance, e.g. testAccProviderPricing , instead of acctest.Provider . If the testing is for a managed resource with a Check...Exists function, ensure it uses the new provider instance, e.g. testAccProviderPricing , instead of acctest.Provider . In each TestStep configuration, ensure the new provider configuration function is called, e.g. func testAccDataSourcePricingProductConfigRedshift () string { return acctest . ConfigCompose ( testAccPricingRegionProviderConfig (), ` # ... test configuration ... ` ) } If the testing configurations require more than one region, reach out to the maintainers for further assistance. Acceptance Test Concurrency # Certain AWS service APIs allow a limited number of a certain component, while the acceptance testing runs at a default concurrency of twenty tests at a time. For example as of this writing, the SageMaker service only allows one SageMaker Domain per AWS Region. Running the tests with the default concurrency will fail with API errors relating to the component quota being exceeded. When encountering these types of components, the acceptance testing can be setup to limit the available concurrency of that particular component. When limited to one component at a time, this may also be referred to as serializing the acceptance tests. To convert to serialized (one test at a time) acceptance testing: Convert all existing capital T test functions with the limited component to begin with a lowercase t , e.g., TestAccSageMakerDomain_basic becomes testAccSageMakerDomain_basic . This will prevent the test framework from executing these tests directly as the prefix Test is required. In each of these test functions, convert resource.ParallelTest to resource.Test Create a capital T TestAcc{Service}{Thing}_serial test function that then references all the lowercase t test functions. If multiple test files are referenced, this new test be created in a new shared file such as internal/service/{SERVICE}/{SERVICE}_test.go . The contents of this test can be setup like the following: func TestAccExampleThing_serial ( t * testing . T ) { testCases := map [ string ] map [ string ] func ( t * testing . T ){ \"Thing\" : { \"basic\" : testAccExampleThing_basic , \"disappears\" : testAccExampleThing_disappears , // ... potentially other resource tests ... }, // ... potentially other top level resource test groups ... } for group , m := range testCases { m := m t . Run ( group , func ( t * testing . T ) { for name , tc := range m { tc := tc t . Run ( name , func ( t * testing . T ) { tc ( t ) }) } }) } } NOTE: Future iterations of these acceptance testing concurrency instructions will include the ability to handle more than one component at a time including service quota lookup, if supported by the service API. Data Source Acceptance Testing # Writing acceptance testing for data sources is similar to resources, with the biggest changes being: Adding DataSource to the test and configuration naming, such as TestAccExampleThingDataSource_Filter The basic test may be named after the easiest lookup attribute instead, e.g., TestAccExampleThingDataSource_Name No disappears testing Almost all checks should be done with resource.TestCheckResourceAttrPair() to compare the data source attributes to the resource attributes The usage of an additional dataSourceName variable to store a data source reference, e.g., data.aws_example_thing.test Data sources testing should still use the CheckDestroy function of the resource, just to continue verifying that there are no dangling AWS resources after a test is run. Please note that we do not recommend re-using test configurations between resources and their associated data source as it is harder to discover testing regressions. Authors are encouraged to potentially implement similar \"base\" configurations though. For example: func TestAccExampleThingDataSource_Name ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) dataSourceName := \"data.aws_example_thing.test\" resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingDataSourceConfigName ( rName ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), resource . TestCheckResourceAttrPair ( resourceName , \"arn\" , dataSourceName , \"arn\" ), resource . TestCheckResourceAttrPair ( resourceName , \"description\" , dataSourceName , \"description\" ), resource . TestCheckResourceAttrPair ( resourceName , \"name\" , dataSourceName , \"name\" ), ), }, }, }) } // below all TestAcc functions func testAccExampleThingDataSourceConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q } data \"aws_example_thing\" \"test\" { name = aws_example_thing.test.name } ` , rName ) } Acceptance Test Sweepers # When running the acceptance tests, especially when developing or troubleshooting Terraform resources, its possible for code bugs or other issues to prevent the proper destruction of AWS infrastructure. To prevent lingering resources from consuming quota or causing unexpected billing, the Terraform Plugin SDK supports the test sweeper framework to clear out an AWS region of all resources. This section is meant to augment the SDKv2 documentation on test sweepers with Terraform AWS Provider specific details. Running Test Sweepers # WARNING: Test Sweepers will destroy AWS infrastructure and backups in the target AWS account and region! These are designed to override any API deletion protection. Never run these outside a development AWS account that should be completely empty of resources. To run the sweepers for all resources in us-west-2 and us-east-1 (default testing regions): $ make sweep To run a specific resource sweeper: $ SWEEPARGS = -sweep-run = aws_example_thing make sweep To run sweepers with an assumed role, use the following additional environment variables: TF_AWS_ASSUME_ROLE_ARN - Required. TF_AWS_ASSUME_ROLE_DURATION - Optional, defaults to 1 hour (3600). TF_AWS_ASSUME_ROLE_EXTERNAL_ID - Optional. TF_AWS_ASSUME_ROLE_SESSION_NAME - Optional. Sweeper Checklists # Add Service To Sweeper List : To allow sweeping for a given service, it needs to be registered in the list of services to be swept, at internal/sweep/sweep_test.go . Add Resource Sweeper Implementation : See Writing Test Sweepers . Writing Test Sweepers # The first step is to initialize the resource into the test sweeper framework: func init () { resource . AddTestSweepers ( \"aws_example_thing\" , & resource . Sweeper { Name : \"aws_example_thing\" , F : sweepThings , // Optionally Dependencies : [] string { \"aws_other_thing\" , }, }) } Then add the actual implementation. Preferably, if a paginated SDK call is available: func sweepThings ( region string ) error { client , err := sweep . SharedRegionalSweepClient ( region ) if err != nil { return fmt . Errorf ( \"error getting client: %w\" , err ) } conn := client .( * conns . AWSClient ). ExampleConn sweepResources := make ([] sweep . Sweepable , 0 ) var errs * multierror . Error input := & example . ListThingsInput {} err = conn . ListThingsPages ( input , func ( page * example . ListThingsOutput , lastPage bool ) bool { if page == nil { return ! lastPage } for _ , thing := range page . Things { r := ResourceThing () d := r . Data ( nil ) id := aws . StringValue ( thing . Id ) d . SetId ( id ) // Perform resource specific pre-sweep setup. // For example, you may need to perform one or more of these types of pre-sweep tasks, specific to the resource: // // err := r.Read(d, client) // fill in data // d.Set(\"skip_final_snapshot\", true) // set an argument in order to delete // This \"if\" is only needed if the pre-sweep setup can produce errors. // Otherwise, do not include it. if err != nil { err := fmt . Errorf ( \"error reading Example Thing (%s): %w\" , id , err ) log . Printf ( \"[ERROR] %s\" , err ) errs = multierror . Append ( errs , err ) continue } sweepResources = append ( sweepResources , sweep . NewSweepResource ( r , d , client )) } return ! lastPage }) if err != nil { errs = multierror . Append ( errs , fmt . Errorf ( \"error listing Example Thing for %s: %w\" , region , err )) } if err := sweep . SweepOrchestrator ( sweepResources ); err != nil { errs = multierror . Append ( errs , fmt . Errorf ( \"error sweeping Example Thing for %s: %w\" , region , err )) } if sweep . SkipSweepError ( err ) { log . Printf ( \"[WARN] Skipping Example Thing sweep for %s: %s\" , region , errs ) return nil } return errs . ErrorOrNil () } Otherwise, if no paginated SDK call is available: func sweepThings ( region string ) error { client , err := sweep . SharedRegionalSweepClient ( region ) if err != nil { return fmt . Errorf ( \"error getting client: %w\" , err ) } conn := client .( * conns . AWSClient ). ExampleConn sweepResources := make ([] sweep . Sweepable , 0 ) var errs * multierror . Error input := & example . ListThingsInput {} for { output , err := conn . ListThings ( input ) for _ , thing := range output . Things { r := ResourceThing () d := r . Data ( nil ) id := aws . StringValue ( thing . Id ) d . SetId ( id ) // Perform resource specific pre-sweep setup. // For example, you may need to perform one or more of these types of pre-sweep tasks, specific to the resource: // // err := r.Read(d, client) // fill in data // d.Set(\"skip_final_snapshot\", true) // set an argument in order to delete // This \"if\" is only needed if the pre-sweep setup can produce errors. // Otherwise, do not include it. if err != nil { err := fmt . Errorf ( \"error reading Example Thing (%s): %w\" , id , err ) log . Printf ( \"[ERROR] %s\" , err ) errs = multierror . Append ( errs , err ) continue } sweepResources = append ( sweepResources , sweep . NewSweepResource ( r , d , client )) } if aws . StringValue ( output . NextToken ) == \"\" { break } input . NextToken = output . NextToken } if err := sweep . SweepOrchestrator ( sweepResources ); err != nil { errs = multierror . Append ( errs , fmt . Errorf ( \"error sweeping Example Thing for %s: %w\" , region , err )) } if sweep . SkipSweepError ( err ) { log . Printf ( \"[WARN] Skipping Example Thing sweep for %s: %s\" , region , errs ) return nil } return errs . ErrorOrNil () } Acceptance Test Checklists # There are several aspects to writing good acceptance tests. These checklists will help ensure effective testing from the design stage through to implementation details. Basic Acceptance Test Design # These are basic principles to help guide the creation of acceptance tests. Covers Changes : Every line of resource or data source code added or changed should be covered by one or more tests. For example, if a resource has two ways of functioning, tests should cover both possible paths. Nearly every codebase change needs test coverage to ensure functionality and prevent future regressions. If a bug or other problem prompted a fix, a test should be added that previously would have failed, especially if the report included a configuration. Follows the Single Responsibility Principle : Every test should have a single responsibility and effectively test that responsibility. This may include individual tests for verifying basic functionality of the resource (Create, Read, Delete), separately verifying using and updating a single attribute in a resource, or separately changing between two attributes to verify two \"modes\"/\"types\" possible with a resource configuration. In following this principle, test configurations should be as simple as possible. For example, not including extra configuration unless it is necessary for the specific test. Test Implementation # The below are required items that will be noted during submission review and prevent immediate merging: Implements CheckDestroy : Resource testing should include a CheckDestroy function (typically named testAccCheck{SERVICE}{RESOURCE}Destroy ) that calls the API to verify that the Terraform resource has been deleted or disassociated as appropriate. More information about CheckDestroy functions can be found in the SDKv2 TestCase documentation . Implements Exists Check Function : Resource testing should include a TestCheckFunc function (typically named testAccCheck{SERVICE}{RESOURCE}Exists ) that calls the API to verify that the Terraform resource has been created or associated as appropriate. Preferably, this function will also accept a pointer to an API object representing the Terraform resource from the API response that can be set for potential usage in later TestCheckFunc . More information about these functions can be found in the SDKv2 Custom Check Functions documentation . Excludes Provider Declarations : Test configurations should not include provider \"aws\" {...} declarations. If necessary, only the provider declarations in acctest.go should be used for multiple account/region or otherwise specialized testing. Passes in us-west-2 Region : Tests default to running in us-west-2 and at a minimum should pass in that region or include necessary PreCheck functions to skip the test when ran outside an expected environment. Includes ErrorCheck : All acceptance tests should include a call to the common ErrorCheck ( ErrorCheck: acctest.ErrorCheck(t, service.EndpointsID), ). Uses resource.ParallelTest : Tests should use resource.ParallelTest() instead of resource.Test() except where serialized testing is absolutely required. [ ] Uses fmt.Sprintf() : Test configurations preferably should to be separated into their own functions (typically named testAcc{SERVICE}{RESOURCE}Config{PURPOSE} ) that call fmt.Sprintf() for variable injection or a string const for completely static configurations. Test configurations should avoid var or other variable injection functionality such as text/template . Uses Randomized Infrastructure Naming : Test configurations that use resources where a unique name is required should generate a random name. Typically this is created via rName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix) in the acceptance test function before generating the configuration. Prevents S3 Bucket Deletion Errors : Test configurations that use aws_s3_bucket resources as a logging destination should include the force_destroy = true configuration. This is to prevent race conditions where logging objects may be written during the testing duration which will cause BucketNotEmpty errors during deletion. For resources that support import, the additional item below is required that will be noted during submission review and prevent immediate merging: Implements ImportState Testing : Tests should include an additional TestStep configuration that verifies resource import via ImportState: true and ImportStateVerify: true . This TestStep should be added to all possible tests for the resource to ensure that all infrastructure configurations are properly imported into Terraform. The below are style-based items that may be noted during review and are recommended for simplicity, consistency, and quality assurance: Uses Builtin Check Functions : Tests should use already available check functions, e.g. resource.TestCheckResourceAttr() , to verify values in the Terraform state over creating custom TestCheckFunc . More information about these functions can be found in the SDKv2 Builtin Check Functions documentation . Uses TestCheckResourceAttrPair() for Data Sources : Tests should use resource.TestCheckResourceAttrPair() to verify values in the Terraform state for data sources attributes to compare them with their expected resource attributes. Excludes Timeouts Configurations : Test configurations should not include timeouts {...} configuration blocks except for explicit testing of customizable timeouts (typically very short timeouts with ExpectError ). Implements Default and Zero Value Validation : The basic test for a resource (typically named TestAcc{SERVICE}{RESOURCE}_basic ) should use available check functions, e.g. resource.TestCheckResourceAttr() , to verify default and zero values in the Terraform state for all attributes. Empty/missing configuration blocks can be verified with resource.TestCheckResourceAttr(resourceName, \"{ATTRIBUTE}.#\", \"0\") and empty maps with resource.TestCheckResourceAttr(resourceName, \"{ATTRIBUTE}.%\", \"0\") Avoid Hard Coding # Avoid hard coding values in acceptance test checks and configurations for consistency and testing flexibility. Resource testing is expected to pass across multiple AWS environments supported by the Terraform AWS Provider (e.g., AWS Standard and AWS GovCloud (US)). Contributors are not expected or required to perform testing outside of AWS Standard, e.g., running only in the us-west-2 region is perfectly acceptable. However, contributors are expected to avoid hard coding with these guidelines. Hardcoded Account IDs # Uses Account Data Sources : Any hardcoded account numbers in configuration, e.g., 137112412989 , should be replaced with a data source. Depending on the situation, there are several data sources for account IDs including: aws_caller_identity data source , aws_canonical_user_id data source , aws_billing_service_account data source , and aws_sagemaker_prebuilt_ecr_image data source . Uses Account Test Checks : Any check required to verify an AWS Account ID of the current testing account or another account should use one of the following available helper functions over the usage of resource.TestCheckResourceAttrSet() and resource.TestMatchResourceAttr() : acctest.CheckResourceAttrAccountID() : Validates the state value equals the AWS Account ID of the current account running the test. This is the most common implementation. acctest.MatchResourceAttrAccountID() : Validates the state value matches any AWS Account ID (e.g. a 12 digit number). This is typically only used in data source testing of AWS managed components. Here's an example of using aws_caller_identity : data \"aws_caller_identity\" \"current\" {} resource \"aws_backup_selection\" \"test\" { plan_id = aws_backup_plan.test.id name = \"tf_acc_test_backup_selection_%[1]d\" iam_role_arn = \"arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/service-role/AWSBackupDefaultServiceRole\" } Hardcoded AMI IDs # Uses aws_ami Data Source : Any hardcoded AMI ID configuration, e.g. ami-12345678 , should be replaced with the aws_ami data source pointing to an Amazon Linux image. The package internal/acctest includes test configuration helper functions to simplify these lookups: acctest.ConfigLatestAmazonLinuxHVMEBSAMI() : The recommended AMI for most situations, using Amazon Linux, HVM virtualization, and EBS storage. To reference the AMI ID in the test configuration: data.aws_ami.amzn-ami-minimal-hvm-ebs.id . testAccLatestAmazonLinuxHVMInstanceStoreAMIConfig() (EC2): AMI lookup using Amazon Linux, HVM virtualization, and Instance Store storage. Should only be used in testing that requires Instance Store storage rather than EBS. To reference the AMI ID in the test configuration: data.aws_ami.amzn-ami-minimal-hvm-instance-store.id . testAccLatestAmazonLinuxPVEBSAMIConfig() (EC2): AMI lookup using Amazon Linux, Paravirtual virtualization, and EBS storage. Should only be used in testing that requires Paravirtual over Hardware Virtual Machine (HVM) virtualization. To reference the AMI ID in the test configuration: data.aws_ami.amzn-ami-minimal-pv-ebs.id . configLatestAmazonLinuxPvInstanceStoreAmi (EC2): AMI lookup using Amazon Linux, Paravirtual virtualization, and Instance Store storage. Should only be used in testing that requires Paravirtual virtualization over HVM and Instance Store storage over EBS. To reference the AMI ID in the test configuration: data.aws_ami.amzn-ami-minimal-pv-instance-store.id . testAccLatestWindowsServer2016CoreAMIConfig() (EC2): AMI lookup using Windows Server 2016 Core, HVM virtualization, and EBS storage. Should only be used in testing that requires Windows. To reference the AMI ID in the test configuration: data.aws_ami.win2016core-ami.id . Here's an example of using acctest.ConfigLatestAmazonLinuxHVMEBSAMI() and data.aws_ami.amzn-ami-minimal-hvm-ebs.id : func testAccLaunchConfigurationDataSourceConfig_basic ( rName string ) string { return acctest . ConfigCompose ( acctest . ConfigLatestAmazonLinuxHVMEBSAMI (), fmt . Sprintf ( ` resource \"aws_launch_configuration\" \"test\" { name = %[1]q image_id = data.aws_ami.amzn-ami-minimal-hvm-ebs.id instance_type = \"m1.small\" } ` , rName )) } Hardcoded Availability Zones # Uses aws_availability_zones Data Source : Any hardcoded AWS Availability Zone configuration, e.g. us-west-2a , should be replaced with the aws_availability_zones data source . Use the convenience function called acctest.ConfigAvailableAZsNoOptIn() (defined in internal/acctest/acctest.go ) to declare data \"aws_availability_zones\" \"available\" {...} . You can then reference the data source via data.aws_availability_zones.available.names[0] or data.aws_availability_zones.available.names[count.index] in resources using count . Here's an example of using acctest.ConfigAvailableAZsNoOptIn() and data.aws_availability_zones.available.names[0] : func testAccInstanceVpcConfigBasic ( rName string ) string { return acctest . ConfigCompose ( acctest . ConfigAvailableAZsNoOptIn (), fmt . Sprintf ( ` resource \"aws_subnet\" \"test\" { availability_zone = data.aws_availability_zones.available.names[0] cidr_block = \"10.0.0.0/24\" vpc_id = aws_vpc.test.id } ` , rName )) } Hardcoded Database Versions # Uses Database Version Data Sources : Hardcoded database versions, e.g., RDS MySQL Engine Version 5.7.42 , should be removed (which means the AWS-defined default version will be used) or replaced with a list of preferred versions using a data source. Because versions change over times and version offerings vary from region to region and partition to partition, using the default version or providing a list of preferences ensures a version will be available. Depending on the situation, there are several data sources for versions, including: aws_rds_engine_version data source , aws_docdb_engine_version data source , and aws_neptune_engine_version data source . Here's an example of using aws_rds_engine_version and data.aws_rds_engine_version.default.version : data \"aws_rds_engine_version\" \"default\" { engine = \"mysql\" } data \"aws_rds_orderable_db_instance\" \"test\" { engine = data.aws_rds_engine_version.default.engine engine_version = data.aws_rds_engine_version.default.version preferred_instance_classes = [ \"db.t3.small\", \"db.t2.small\", \"db.t2.medium\" ] } resource \"aws_db_instance\" \"bar\" { engine = data.aws_rds_engine_version.default.engine engine_version = data.aws_rds_engine_version.default.version instance_class = data.aws_rds_orderable_db_instance.test.instance_class skip_final_snapshot = true parameter_group_name = \"default.${data.aws_rds_engine_version.default.parameter_group_family}\" } Hardcoded Direct Connect Locations # Uses aws_dx_locations Data Source : Hardcoded AWS Direct Connect locations, e.g., EqSe2 , should be replaced with the aws_dx_locations data source . Here's an example using data.aws_dx_locations.test.location_codes : data \"aws_dx_locations\" \"test\" {} resource \"aws_dx_lag\" \"test\" { name = \"Test LAG\" connections_bandwidth = \"1Gbps\" location = tolist ( data.aws_dx_locations.test.location_codes )[ 0 ] force_destroy = true } Hardcoded Instance Types # Uses Instance Type Data Source : Singular hardcoded instance types and classes, e.g., t2.micro and db.t2.micro , should be replaced with a list of preferences using a data source. Because offerings vary from region to region and partition to partition, providing a list of preferences dramatically improves the likelihood that one of the options will be available. Depending on the situation, there are several data sources for instance types and classes, including: aws_ec2_instance_type_offering data source - Convenience functions declare configurations that are referenced with data.aws_ec2_instance_type_offering.available including: The acctest.AvailableEC2InstanceTypeForAvailabilityZone() function for test configurations using an EC2 Subnet which is inherently within a single Availability Zone The acctest.AvailableEC2InstanceTypeForRegion() function for test configurations that do not include specific Availability Zones aws_rds_orderable_db_instance data source , aws_neptune_orderable_db_instance data source , and aws_docdb_orderable_db_instance data source . Here's an example of using acctest.AvailableEC2InstanceTypeForRegion() and data.aws_ec2_instance_type_offering.available.instance_type : func testAccSpotInstanceRequestConfig ( rInt int ) string { return acctest . ConfigCompose ( acctest . AvailableEC2InstanceTypeForRegion ( \"t3.micro\" , \"t2.micro\" ), fmt . Sprintf ( ` resource \"aws_spot_instance_request\" \"test\" { instance_type = data.aws_ec2_instance_type_offering.available.instance_type spot_price = \"0.05\" wait_for_fulfillment = true } ` , rInt )) } Here's an example of using aws_rds_orderable_db_instance and data.aws_rds_orderable_db_instance.test.instance_class : data \"aws_rds_orderable_db_instance\" \"test\" { engine = \"mysql\" engine_version = \"5.7.31\" preferred_instance_classes = [ \"db.t3.micro\", \"db.t2.micro\", \"db.t3.small\" ] } resource \"aws_db_instance\" \"test\" { engine = data.aws_rds_orderable_db_instance.test.engine engine_version = data.aws_rds_orderable_db_instance.test.engine_version instance_class = data.aws_rds_orderable_db_instance.test.instance_class skip_final_snapshot = true username = \"test\" } Hardcoded Partition DNS Suffix # Uses aws_partition Data Source : Any hardcoded DNS suffix configuration, e.g., the amazonaws.com in a ec2.amazonaws.com service principal, should be replaced with the aws_partition data source . A common pattern is declaring data \"aws_partition\" \"current\" {} and referencing it via data.aws_partition.current.dns_suffix . Here's an example of using aws_partition and data.aws_partition.current.dns_suffix : data \"aws_partition\" \"current\" {} resource \"aws_iam_role\" \"test\" { assume_role_policy = << POLICY { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"cloudtrail.${data.aws_partition.current.dns_suffix}\" }, \"Action\": \"sts:AssumeRole\" } ] } POLICY } Hardcoded Partition in ARN # Uses aws_partition Data Source : Any hardcoded AWS Partition configuration, e.g. the aws in a arn:aws:SERVICE:REGION:ACCOUNT:RESOURCE ARN, should be replaced with the aws_partition data source . A common pattern is declaring data \"aws_partition\" \"current\" {} and referencing it via data.aws_partition.current.partition . Uses Builtin ARN Check Functions : Tests should use available ARN check functions to validate ARN attribute values in the Terraform state over resource.TestCheckResourceAttrSet() and resource.TestMatchResourceAttr() : acctest.CheckResourceAttrRegionalARN() verifies that an ARN matches the account ID and region of the test execution with an exact resource value acctest.MatchResourceAttrRegionalARN() verifies that an ARN matches the account ID and region of the test execution with a regular expression of the resource value acctest.CheckResourceAttrGlobalARN() verifies that an ARN matches the account ID of the test execution with an exact resource value acctest.MatchResourceAttrGlobalARN() verifies that an ARN matches the account ID of the test execution with a regular expression of the resource value acctest.CheckResourceAttrRegionalARNNoAccount() verifies than an ARN has no account ID and matches the current region of the test execution with an exact resource value acctest.CheckResourceAttrGlobalARNNoAccount() verifies than an ARN has no account ID and matches an exact resource value acctest.CheckResourceAttrRegionalARNAccountID() verifies than an ARN matches a specific account ID and the current region of the test execution with an exact resource value acctest.CheckResourceAttrGlobalARNAccountID() verifies than an ARN matches a specific account ID with an exact resource value Here's an example of using aws_partition and data.aws_partition.current.partition : data \"aws_partition\" \"current\" {} resource \"aws_iam_role_policy_attachment\" \"test\" { policy_arn = \"arn:${data.aws_partition.current.partition}:iam::aws:policy/service-role/AmazonRDSEnhancedMonitoringRole\" role = aws_iam_role.test.name } Hardcoded Region # Uses aws_region Data Source : Any hardcoded AWS Region configuration, e.g., us-west-2 , should be replaced with the aws_region data source . A common pattern is declaring data \"aws_region\" \"current\" {} and referencing it via data.aws_region.current.name Here's an example of using aws_region and data.aws_region.current.name : data \"aws_region\" \"current\" {} resource \"aws_route53_zone\" \"test\" { vpc { vpc_id = aws_vpc.test.id vpc_region = data.aws_region.current.name } } Hardcoded Spot Price # Uses aws_ec2_spot_price Data Source : Any hardcoded spot prices, e.g., 0.05 , should be replaced with the aws_ec2_spot_price data source . A common pattern is declaring data \"aws_ec2_spot_price\" \"current\" {} and referencing it via data.aws_ec2_spot_price.current.spot_price . Here's an example of using aws_ec2_spot_price and data.aws_ec2_spot_price.current.spot_price : data \"aws_ec2_spot_price\" \"current\" { instance_type = \"t3.medium\" filter { name = \"product-description\" values = [ \"Linux/UNIX\" ] } } resource \"aws_spot_fleet_request\" \"test\" { spot_price = data.aws_ec2_spot_price.current.spot_price target_capacity = 2 } Hardcoded SSH Keys # Uses acctest.RandSSHKeyPair() or RandSSHKeyPairSize() Functions : Any hardcoded SSH keys should be replaced with random SSH keys generated by either the acceptance testing framework's function RandSSHKeyPair() or the provider function RandSSHKeyPairSize() . RandSSHKeyPair() generates 1024-bit keys. Here's an example using aws_key_pair func TestAccKeyPair_basic ( t * testing . T ) { ... rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) publicKey , _ , err := acctest . RandSSHKeyPair ( acctest . DefaultEmailAddress ) if err != nil { t . Fatalf ( \"error generating random SSH key: %s\" , err ) } resource . ParallelTest ( t , resource . TestCase { ... Steps : [] resource . TestStep { { Config : testAccKeyPairConfig ( rName , publicKey ), ... }, }, }) } func testAccKeyPairConfig ( rName , publicKey string ) string { return fmt . Sprintf ( ` resource \"aws_key_pair\" \"test\" { key_name = %[1]q public_key = %[2]q } ` , rName , publicKey ) } Hardcoded Email Addresses # Uses either acctest.DefaultEmailAddress Constant or acctest.RandomEmailAddress() Function : Any hardcoded email addresses should replaced with either the constant acctest.DefaultEmailAddress or the function acctest.RandomEmailAddress() . Using acctest.DefaultEmailAddress is preferred when using a single email address in an acceptance test. Here's an example using acctest.DefaultEmailAddress func TestAccSNSTopicSubscription_email ( t * testing . T ) { ... rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resource . ParallelTest ( t , resource . TestCase { ... Steps : [] resource . TestStep { { Config : testAccTopicSubscriptionEmailConfig ( rName , acctest . DefaultEmailAddress ), Check : resource . ComposeTestCheckFunc ( ... resource . TestCheckResourceAttr ( resourceName , \"endpoint\" , acctest . DefaultEmailAddress ), ), }, }, }) } Here's an example using acctest.RandomEmailAddress() func TestAccPinpointEmailChannel_basic ( t * testing . T ) { ... domain := acctest . RandomDomainName () address1 := acctest . RandomEmailAddress ( domain ) address2 := acctest . RandomEmailAddress ( domain ) resource . ParallelTest ( t , resource . TestCase { ... Steps : [] resource . TestStep { { Config : testAccEmailChannelConfig_FromAddress ( domain , address1 ), Check : resource . ComposeTestCheckFunc ( ... resource . TestCheckResourceAttr ( resourceName , \"from_address\" , address1 ), ), }, { Config : testAccEmailChannelConfig_FromAddress ( domain , address2 ), Check : resource . ComposeTestCheckFunc ( ... resource . TestCheckResourceAttr ( resourceName , \"from_address\" , address2 ), ), }, }, }) }","title":"Acceptance Tests"},{"location":"running-and-writing-acceptance-tests/#running-and-writing-acceptance-tests","text":"Terraform includes an acceptance test harness that does most of the repetitive work involved in testing a resource. For additional information about testing Terraform Providers, see the SDKv2 documentation .","title":"Running and Writing Acceptance Tests"},{"location":"running-and-writing-acceptance-tests/#acceptance-tests-often-cost-money-to-run","text":"Our acceptance test suite creates real resources, and as a result they cost real money to run. Because the resources only exist for a short period of time, the total amount of money required is usually a relatively small amount. That said there are particular services which are very expensive to run and its important to be prepared for those costs. Some services which can be cost prohibitive include (among others): WorkSpaces Glue OpenSearch RDS ACM (Amazon Certificate Manager) FSx Kinesis Analytics EC2 ElastiCache Storage Gateway We don't want financial limitations to be a barrier to contribution, so if you are unable to pay to run acceptance tests for your contribution, mention this in your pull request. We will happily accept \"best effort\" implementations of acceptance tests and run them for you on our side. This might mean that your PR takes a bit longer to merge, but it most definitely is not a blocker for contributions.","title":"Acceptance Tests Often Cost Money to Run"},{"location":"running-and-writing-acceptance-tests/#running-an-acceptance-test","text":"Acceptance tests can be run using the testacc target in the Terraform Makefile . The individual tests to run can be controlled using a regular expression. Prior to running the tests provider configuration details such as access keys must be made available as environment variables. For example, to run an acceptance test against the Amazon Web Services provider, the following environment variables must be set: # Using a profile export AWS_PROFILE = ... # Otherwise export AWS_ACCESS_KEY_ID = ... export AWS_SECRET_ACCESS_KEY = ... export AWS_DEFAULT_REGION = ... Please note that the default region for the testing is us-west-2 and must be overridden via the AWS_DEFAULT_REGION environment variable, if necessary. This is especially important for testing AWS GovCloud (US), which requires: export AWS_DEFAULT_REGION = us-gov-west-1 Tests can then be run by specifying a regular expression defining the tests to run and the package in which the tests are defined: $ make testacc TESTS = TestAccCloudWatchDashboard_updateName PKG = cloudwatch == > Checking that code complies with gofmt requirements... TF_ACC = 1 go test ./internal/service/cloudwatch/... -v -count 1 -parallel 20 -run = TestAccCloudWatchDashboard_updateName -timeout 180m === RUN TestAccCloudWatchDashboard_updateName === PAUSE TestAccCloudWatchDashboard_updateName === CONT TestAccCloudWatchDashboard_updateName --- PASS: TestAccCloudWatchDashboard_updateName ( 25 .33s ) PASS ok github.com/hashicorp/terraform-provider-aws/internal/service/cloudwatch 25 .387s Entire resource test suites can be targeted by using the naming convention to write the regular expression. For example, to run all tests of the aws_cloudwatch_dashboard resource rather than just the updateName test, you can start testing like this: $ make testacc TESTS = TestAccCloudWatchDashboard PKG = cloudwatch == > Checking that code complies with gofmt requirements... TF_ACC = 1 go test ./internal/service/cloudwatch/... -v -count 1 -parallel 20 -run = TestAccCloudWatchDashboard -timeout 180m === RUN TestAccCloudWatchDashboard_basic === PAUSE TestAccCloudWatchDashboard_basic === RUN TestAccCloudWatchDashboard_update === PAUSE TestAccCloudWatchDashboard_update === RUN TestAccCloudWatchDashboard_updateName === PAUSE TestAccCloudWatchDashboard_updateName === CONT TestAccCloudWatchDashboard_basic === CONT TestAccCloudWatchDashboard_updateName === CONT TestAccCloudWatchDashboard_update --- PASS: TestAccCloudWatchDashboard_basic ( 15 .83s ) --- PASS: TestAccCloudWatchDashboard_updateName ( 26 .69s ) --- PASS: TestAccCloudWatchDashboard_update ( 27 .72s ) PASS ok github.com/hashicorp/terraform-provider-aws/internal/service/cloudwatch 27 .783s Running acceptance tests requires version 0.12.26 or higher of the Terraform CLI to be installed. For advanced developers, the acceptance testing framework accepts some additional environment variables that can be used to control Terraform CLI binary selection, logging, and other behaviors. See the SDKv2 documentation for more information. Please Note: On macOS 10.14 and later (and some Linux distributions), the default user open file limit is 256. This may cause unexpected issues when running the acceptance testing since this can prevent various operations from occurring such as opening network connections to AWS. To view this limit, the ulimit -n command can be run. To update this limit, run ulimit -n 1024 (or higher).","title":"Running an Acceptance Test"},{"location":"running-and-writing-acceptance-tests/#running-cross-account-tests","text":"Certain testing requires multiple AWS accounts. This additional setup is not typically required and the testing will return an error (shown below) if your current setup does not have the secondary AWS configuration: $ make testacc TESTS = TestAccRDSInstance_DBSubnetGroupName_ramShared PKG = rds TF_ACC=1 go test ./internal/service/rds/... -v -count 1 -parallel 20 -run=TestAccRDSInstance_DBSubnetGroupName_ramShared -timeout 180m === RUN TestAccRDSInstance_DBSubnetGroupName_ramShared === PAUSE TestAccRDSInstance_DBSubnetGroupName_ramShared === CONT TestAccRDSInstance_DBSubnetGroupName_ramShared acctest.go:674: skipping test because at least one environment variable of [AWS_ALTERNATE_PROFILE AWS_ALTERNATE_ACCESS_KEY_ID] must be set. Usage: credentials for running acceptance testing in alternate AWS account. --- SKIP: TestAccRDSInstance_DBSubnetGroupName_ramShared (0.85s) PASS ok github.com/hashicorp/terraform-provider-aws/internal/service/rds 0.888s Running these acceptance tests is the same as before, except the following additional AWS credential information is required: # Using a profile export AWS_ALTERNATE_PROFILE = ... # Otherwise export AWS_ALTERNATE_ACCESS_KEY_ID = ... export AWS_ALTERNATE_SECRET_ACCESS_KEY = ...","title":"Running Cross-Account Tests"},{"location":"running-and-writing-acceptance-tests/#running-cross-region-tests","text":"Certain testing requires multiple AWS regions. Additional setup is not typically required because the testing defaults the second AWS region to us-east-1 and the third AWS region to us-east-2 . Running these acceptance tests is the same as before, but if you wish to override the second and third regions: export AWS_ALTERNATE_REGION = ... export AWS_THIRD_REGION = ...","title":"Running Cross-Region Tests"},{"location":"running-and-writing-acceptance-tests/#running-only-short-tests","text":"Some tests have been manually marked as long-running (longer than 300 seconds) and can be skipped using the -short flag. However, we are adding long-running guards little by little and many services have no guarded tests. Where guards have been implemented, do not always skip long-running tests. However, for intermediate test runs during development, or to verify functionality unrelated to the specific long-running tests, skipping long-running tests makes work more efficient. We recommend that for the final test run before submitting a PR that you run affected tests without the -short flag. If you want to run only short-running tests, you can use either one of these equivalent statements. Note the use of -short . For example: $ make testacc TESTS = 'TestAccECSTaskDefinition_' PKG = ecs TESTARGS = -short Or: $ TF_ACC = 1 go test ./internal/service/ecs/... -v -count 1 -parallel 20 -run = 'TestAccECSTaskDefinition_' -short -timeout 180m","title":"Running Only Short Tests"},{"location":"running-and-writing-acceptance-tests/#writing-an-acceptance-test","text":"Terraform has a framework for writing acceptance tests which minimizes the amount of boilerplate code necessary to use common testing patterns. This guide is meant to augment the general SDKv2 documentation with Terraform AWS Provider specific conventions and helpers.","title":"Writing an Acceptance Test"},{"location":"running-and-writing-acceptance-tests/#anatomy-of-an-acceptance-test","text":"This section describes in detail how the Terraform acceptance testing framework operates with respect to the Terraform AWS Provider. We recommend those unfamiliar with this provider, or Terraform resource testing in general, take a look here first to generally understand how we interact with AWS and the resource code to verify functionality. The entry point to the framework is the resource.ParallelTest() function. This wraps our testing to work with the standard Go testing framework, while also preventing unexpected usage of AWS by requiring the TF_ACC=1 environment variable. This function accepts a TestCase parameter, which has all the details about the test itself. For example, this includes the test steps ( TestSteps ) and how to verify resource deletion in the API after all steps have been run ( CheckDestroy ). Each TestStep proceeds by applying some Terraform configuration using the provider under test, and then verifying that results are as expected by making assertions using the provider API. It is common for a single test function to exercise both the creation of and updates to a single resource. Most tests follow a similar structure. Pre-flight checks are made to ensure that sufficient provider configuration is available to be able to proceed - for example in an acceptance test targeting AWS, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY must be set prior to running acceptance tests. This is common to all tests exercising a single provider. Most assertion functions are defined out of band with the tests. This keeps the tests readable, and allows reuse of assertion functions across different tests of the same type of resource. The definition of a complete test looks like this: func TestAccCloudWatchDashboard_basic ( t * testing . T ) { var dashboard cloudwatch . GetDashboardOutput rInt := acctest . RandInt () resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , cloudwatch . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckDashboardDestroy , Steps : [] resource . TestStep { { Config : testAccDashboardConfig ( rInt ), Check : resource . ComposeTestCheckFunc ( testAccCheckDashboardExists ( \"aws_cloudwatch_dashboard.foobar\" , & dashboard ), resource . TestCheckResourceAttr ( \"aws_cloudwatch_dashboard.foobar\" , \"dashboard_name\" , testAccDashboardName ( rInt )), ), }, }, }) } When executing the test, the following steps are taken for each TestStep : The Terraform configuration required for the test is applied. This is responsible for configuring the resource under test, and any dependencies it may have. For example, to test the aws_cloudwatch_dashboard resource, a valid configuration with the requisite fields is required. This results in configuration which looks like this: resource \"aws_cloudwatch_dashboard\" \"foobar\" { dashboard_name = \"terraform-test-dashboard-%d\" dashboard_body = << EOF { \"widgets\": [{ \"type\": \"text\", \"x\": 0, \"y\": 0, \"width\": 6, \"height\": 6, \"properties\": { \"markdown\": \"Hi there from Terraform: CloudWatch\" } }] } EOF } Assertions are run using the provider API. These use the provider API directly rather than asserting against the resource state. For example, to verify that the aws_cloudwatch_dashboard described above was created successfully, a test function like this is used: func testAccCheckDashboardExists ( n string , dashboard * cloudwatch . GetDashboardOutput ) resource . TestCheckFunc { return func ( s * terraform . State ) error { rs , ok := s . RootModule (). Resources [ n ] if ! ok { return fmt . Errorf ( \"Not found: %s\" , n ) } conn := acctest . Provider . Meta ().( * conns . AWSClient ). CloudWatchConn params := cloudwatch . GetDashboardInput { DashboardName : aws . String ( rs . Primary . ID ), } resp , err := conn . GetDashboard ( & params ) if err != nil { return err } * dashboard = * resp return nil } } Notice that the only information used from the Terraform state is the ID of the resource. For computed properties, we instead assert that the value saved in the Terraform state was the expected value if possible. The testing framework provides helper functions for several common types of check - for example: ```go resource.TestCheckResourceAttr(\"aws_cloudwatch_dashboard.foobar\", \"dashboard_name\", testAccDashboardName(rInt)), ``` The resources created by the test are destroyed. This step happens automatically, and is the equivalent of calling terraform destroy . Assertions are made against the provider API to verify that the resources have indeed been removed. If these checks fail, the test fails and reports \"dangling resources\". The code to ensure that the aws_cloudwatch_dashboard shown above has been destroyed looks like this: func testAccCheckDashboardDestroy ( s * terraform . State ) error { conn := acctest . Provider . Meta ().( * conns . AWSClient ). CloudWatchConn for _ , rs := range s . RootModule (). Resources { if rs . Type != \"aws_cloudwatch_dashboard\" { continue } params := cloudwatch . GetDashboardInput { DashboardName : aws . String ( rs . Primary . ID ), } _ , err := conn . GetDashboard ( & params ) if err == nil { return fmt . Errorf ( \"Dashboard still exists: %s\" , rs . Primary . ID ) } if ! isDashboardNotFoundErr ( err ) { return err } } return nil } These functions usually test only for the resource directly under test.","title":"Anatomy of an Acceptance Test"},{"location":"running-and-writing-acceptance-tests/#resource-acceptance-testing","text":"Most resources that implement standard Create, Read, Update, and Delete functionality should follow the pattern below. Each test type has a section that describes them in more detail: basic : This represents the bare minimum verification that the resource can be created, read, deleted, and optionally imported. disappears : A test that verifies Terraform will offer to recreate a resource if it is deleted outside of Terraform (e.g., via the Console) instead of returning an error that it cannot be found. Per Attribute : A test that verifies the resource with a single additional argument can be created, read, optionally updated (or force resource recreation), deleted, and optionally imported. The leading sections below highlight additional recommended patterns.","title":"Resource Acceptance Testing"},{"location":"running-and-writing-acceptance-tests/#test-configurations","text":"Most of the existing test configurations you will find in the Terraform AWS Provider are written in the following function-based style: func TestAccExampleThing_basic ( t * testing . T ) { // ... omitted for brevity ... resource . ParallelTest ( t , resource . TestCase { // ... omitted for brevity ... Steps : [] resource . TestStep { { Config : testAccExampleThingConfig (), // ... omitted for brevity ... }, }, }) } func testAccExampleThingConfig () string { return ` resource \"aws_example_thing\" \"test\" { # ... omitted for brevity ... } ` } Even when no values need to be passed in to the test configuration, we have found this setup to be the most flexible for allowing that to be easily implemented. Any configurable values are handled via fmt.Sprintf() . Using text/template or other templating styles is explicitly forbidden. For consistency, resources in the test configuration should be named resource \"...\" \"test\" unless multiple of that resource are necessary. We discourage re-using test configurations across test files (except for some common configuration helpers we provide) as it is much harder to discover potential testing regressions. Please also note that the newline on the first line of the configuration (before resource ) and the newline after the last line of configuration (after } ) are important to allow test configurations to be easily combined without generating Terraform configuration language syntax errors.","title":"Test Configurations"},{"location":"running-and-writing-acceptance-tests/#combining-test-configurations","text":"We include a helper function, acctest.ConfigCompose() for iteratively building and chaining test configurations together. It accepts any number of configurations to combine them. This simplifies a single resource's testing by allowing the creation of a \"base\" test configuration for all the other test configurations (if necessary) and also allows the maintainers to curate common configurations. Each of these is described in more detail in below sections. Please note that we do discourage excessive chaining of configurations such as implementing multiple layers of \"base\" configurations. Usually these configurations are harder for maintainers and other future readers to understand due to the multiple levels of indirection.","title":"Combining Test Configurations"},{"location":"running-and-writing-acceptance-tests/#base-test-configurations","text":"If a resource requires the same Terraform configuration as a prerequisite for all test configurations, then a common pattern is implementing a \"base\" test configuration that is combined with each test configuration. For example: func testAccExampleThingConfigBase () string { return ` resource \"aws_iam_role\" \"test\" { # ... omitted for brevity ... } resource \"aws_iam_role_policy\" \"test\" { # ... omitted for brevity ... } ` } func testAccExampleThingConfig () string { return acctest . ConfigCompose ( testAccExampleThingConfigBase (), ` resource \"aws_example_thing\" \"test\" { # ... omitted for brevity ... } ` ) }","title":"Base Test Configurations"},{"location":"running-and-writing-acceptance-tests/#available-common-test-configurations","text":"These test configurations are typical implementations we have found or allow testing to implement best practices easier, since the Terraform AWS Provider testing is expected to run against various AWS Regions and Partitions. acctest.AvailableEC2InstanceTypeForRegion(\"type1\", \"type2\", ...) : Typically used to replace hardcoded EC2 Instance Types. Uses aws_ec2_instance_type_offering data source to return an available EC2 Instance Type in preferred ordering. Reference the instance type via: data.aws_ec2_instance_type_offering.available.instance_type . Use acctest.AvailableEC2InstanceTypeForRegionNamed(\"name\", \"type1\", \"type2\", ...) to specify a name for the data source acctest.ConfigLatestAmazonLinuxHVMEBSAMI() : Typically used to replace hardcoded EC2 Image IDs ( ami-12345678 ). Uses aws_ami data source to find the latest Amazon Linux image. Reference the AMI ID via: data.aws_ami.amzn-ami-minimal-hvm-ebs.id","title":"Available Common Test Configurations"},{"location":"running-and-writing-acceptance-tests/#randomized-naming","text":"For AWS resources that require unique naming, the tests should implement a randomized name, typically coded as a rName variable in the test and passed as a parameter to creating the test configuration. For example: func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) // ... omitted for brevity ... resource . ParallelTest ( t , resource . TestCase { // ... omitted for brevity ... Steps : [] resource . TestStep { { Config : testAccExampleThingConfigName ( rName ), // ... omitted for brevity ... }, }, }) } func testAccExampleThingConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q } ` , rName ) } Typically the rName is always the first argument to the test configuration function, if used, for consistency. Note that if rName (or any other variable) is used multiple times in the fmt.Sprintf() statement, do not repeat rName in the fmt.Sprintf() arguments. Using fmt.Sprintf(..., rName, rName) , for example, would not be correct. Instead, use the indexed %[1]q (or %[x]q , %[x]s , %[x]t , or %[x]d , where x represents the index number) verb multiple times. For example: func testAccExampleThingConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q tags = { Name = %[1]q } } ` , rName ) }","title":"Randomized Naming"},{"location":"running-and-writing-acceptance-tests/#other-recommended-variables","text":"We also typically recommend saving a resourceName variable in the test that contains the resource reference, e.g., aws_example_thing.test , which is repeatedly used in the checks. For example: func TestAccExampleThing_basic ( t * testing . T ) { // ... omitted for brevity ... resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { // ... omitted for brevity ... Steps : [] resource . TestStep { { // ... omitted for brevity ... Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), acctest . CheckResourceAttrRegionalARN ( resourceName , \"arn\" , \"example\" , fmt . Sprintf ( \"thing/%s\" , rName )), resource . TestCheckResourceAttr ( resourceName , \"description\" , \"\" ), resource . TestCheckResourceAttr ( resourceName , \"name\" , rName ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } // below all TestAcc functions func testAccExampleThingConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q } ` , rName ) }","title":"Other Recommended Variables"},{"location":"running-and-writing-acceptance-tests/#basic-acceptance-tests","text":"Usually this test is implemented first. The test configuration should contain only required arguments ( Required: true attributes) and it should check the values of all read-only attributes ( Computed: true without Optional: true ). If the resource supports it, it verifies import. It should NOT perform other TestStep such as updates or verify recreation. These are typically named TestAcc{SERVICE}{THING}_basic , e.g., TestAccCloudWatchDashboard_basic For example: func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingConfigName ( rName ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), acctest . CheckResourceAttrRegionalARN ( resourceName , \"arn\" , \"example\" , fmt . Sprintf ( \"thing/%s\" , rName )), resource . TestCheckResourceAttr ( resourceName , \"description\" , \"\" ), resource . TestCheckResourceAttr ( resourceName , \"name\" , rName ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } // below all TestAcc functions func testAccExampleThingConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q } ` , rName ) }","title":"Basic Acceptance Tests"},{"location":"running-and-writing-acceptance-tests/#prechecks","text":"Acceptance test cases have a PreCheck. The PreCheck ensures that the testing environment meets certain preconditions. If the environment does not meet the preconditions, Go skips the test. Skipping a test avoids reporting a failure and wasting resources where the test cannot succeed. Here is an example of the default PreCheck: func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, // ... additional checks follow ... }) } Extend the default PreCheck by adding calls to functions in the anonymous PreCheck function. The functions can be existing functions in the provider or custom functions you add for new capabilities.","title":"PreChecks"},{"location":"running-and-writing-acceptance-tests/#standard-provider-prechecks","text":"If you add a new test that has preconditions which are checked by an existing provider function, use that standard PreCheck instead of creating a new one. Some existing tests are missing standard PreChecks and you can help by adding them where appropriate. These are some of the standard provider PreChecks: acctest.PreCheckPartitionHasService(serviceId string, t *testing.T) checks whether the current partition lists the service as part of its offerings. Note: AWS may not add new or public preview services to the service list immediately. This function will return a false positive in that case. acctest.PreCheckOrganizationsAccount(t *testing.T) checks whether the current account can perform AWS Organizations tests. acctest.PreCheckAlternateAccount(t *testing.T) checks whether the environment is set up for tests across accounts. acctest.PreCheckMultipleRegion(t *testing.T, regions int) checks whether the environment is set up for tests across regions. This is an example of using a standard PreCheck function. For an established service, such as WAF or FSx, use acctest.PreCheckPartitionHasService() and the service endpoint ID to check that a partition supports the service. func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ); acctest . PreCheckPartitionHasService ( waf . EndpointsID , t ) }, // ... additional checks follow ... }) }","title":"Standard Provider PreChecks"},{"location":"running-and-writing-acceptance-tests/#custom-prechecks","text":"In situations where standard PreChecks do not test for the required preconditions, create a custom PreCheck. Below is an example of adding a custom PreCheck function. For a new or preview service that AWS does not include in the partition service list yet, you can verify the existence of the service with a simple read-only request (e.g., list all X service things). (For acceptance tests of established services, use acctest.PreCheckPartitionHasService() instead.) func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ), testAccPreCheckExample ( t ) }, // ... additional checks follow ... }) } func testAccPreCheckExample ( t * testing . T ) { conn := acctest . Provider . Meta ().( * conns . AWSClient ). ExampleConn input := & example . ListThingsInput {} _ , err := conn . ListThings ( input ) if testAccPreCheckSkipError ( err ) { t . Skipf ( \"skipping acceptance testing: %s\" , err ) } if err != nil { t . Fatalf ( \"unexpected PreCheck error: %s\" , err ) } }","title":"Custom PreChecks"},{"location":"running-and-writing-acceptance-tests/#errorchecks","text":"Acceptance test cases have an ErrorCheck. The ErrorCheck provides a chance to take a look at errors before the test fails. While most errors should result in test failure, some should not. For example, an error that indicates an API operation is not supported in a particular region should cause the test to skip instead of fail. Since errors should flow through the ErrorCheck, do not handle the vast majority of failing conditions. Instead, in ErrorCheck, focus on the rare errors that should cause a test to skip, or in other words, be ignored.","title":"ErrorChecks"},{"location":"running-and-writing-acceptance-tests/#common-errorcheck","text":"In many situations, the common ErrorCheck is sufficient. It will skip tests for several normal occurrences such as when AWS reports a feature is not supported in the current region. Here is an example of the common ErrorCheck: func TestAccExampleThing_basic ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { // PreCheck ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), // ... additional checks follow ... }) }","title":"Common ErrorCheck"},{"location":"running-and-writing-acceptance-tests/#service-specific-errorchecks","text":"However, some services have special conditions that aren't caught by the common ErrorCheck. In these cases, you can create a service-specific ErrorCheck. To add a service-specific ErrorCheck, follow these steps: Make sure there is not already an ErrorCheck for the service you have in mind. For example, search the codebase for acctest.RegisterServiceErrorCheckFunc(service.EndpointsID replacing \"service\" with the package name of the service you're working on (e.g., ec2 ). If there is already an ErrorCheck for the service, add to the existing service-specific ErrorCheck. Create the service-specific ErrorCheck in an _test.go file for the service. See the example below. Register the new service-specific ErrorCheck in the init() at the top of the _test.go file. See the example below. An example of adding a service-specific ErrorCheck: // just after the imports, create or add to the init() function func init () { acctest . RegisterServiceErrorCheck ( service . EndpointsID , testAccErrorCheckSkipService ) } // ... additional code and tests ... // this is the service-specific ErrorCheck func testAccErrorCheckSkipService ( t * testing . T ) resource . ErrorCheckFunc { return acctest . ErrorCheckSkipMessagesContaining ( t , \"Error message specific to the service that indicates unsupported features\" , \"You can include from one to many portions of error messages\" , \"Be careful to not inadvertently capture errors that should not be skipped\" , ) }","title":"Service-Specific ErrorChecks"},{"location":"running-and-writing-acceptance-tests/#long-running-test-guards","text":"For any acceptance tests that typically run longer than 300 seconds (5 minutes), add a -short test guard at the top of the test function. For example: func TestAccExampleThing_longRunningTest ( t * testing . T ) { if testing . Short () { t . Skip ( \"skipping long-running test in short mode\" ) } // ... omitted for brevity ... resource . ParallelTest ( t , resource . TestCase { // ... omitted for brevity ... }) } When running acceptances tests, tests with these guards can be skipped using the Go -short flag. See Running Only Short Tests for examples.","title":"Long-Running Test Guards"},{"location":"running-and-writing-acceptance-tests/#disappears-acceptance-tests","text":"This test is generally implemented second. It is straightforward to setup once the basic test is passing since it can reuse that test configuration. It prevents a common bug report with Terraform resources that error when they can not be found (e.g., deleted outside Terraform). These are typically named TestAcc{SERVICE}{THING}_disappears , e.g., TestAccCloudWatchDashboard_disappears For example: func TestAccExampleThing_disappears ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingConfigName ( rName ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName , & job ), acctest . CheckResourceDisappears ( acctest . Provider , ResourceExampleThing (), resourceName ), ), ExpectNonEmptyPlan : true , }, }, }) } If this test does fail, the fix for this is generally adding error handling immediately after the Read API call that catches the error and tells Terraform to remove the resource before returning the error: output , err := conn . GetThing ( input ) if isAWSErr ( err , example . ErrCodeResourceNotFound , \"\" ) { log . Printf ( \"[WARN] Example Thing (%s) not found, removing from state\" , d . Id ()) d . SetId ( \"\" ) return nil } if err != nil { return fmt . Errorf ( \"reading Example Thing (%s): %w\" , d . Id (), err ) } For children resources that are encapsulated by a parent resource, it is also preferable to verify that removing the parent resource will not generate an error either. These are typically named TestAcc{SERVICE}{THING}_disappears_{PARENT} , e.g., TestAccRoute53ZoneAssociation_disappears_Vpc func TestAccExampleChildThing_disappears_ParentThing ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) parentResourceName := \"aws_example_parent_thing.test\" resourceName := \"aws_example_child_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleChildThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingConfigName ( rName ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), acctest . CheckResourceDisappears ( acctest . Provider , ResourceExampleParentThing (), parentResourceName ), ), ExpectNonEmptyPlan : true , }, }, }) }","title":"Disappears Acceptance Tests"},{"location":"running-and-writing-acceptance-tests/#per-attribute-acceptance-tests","text":"These are typically named TestAcc{SERVICE}{THING}_{ATTRIBUTE} , e.g., TestAccCloudWatchDashboard_Name For example: func TestAccExampleThing_Description ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingConfigDescription ( rName , \"description1\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"description\" , \"description1\" ), ), }, { ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, { Config : testAccExampleThingConfigDescription ( rName , \"description2\" ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), resource . TestCheckResourceAttr ( resourceName , \"description\" , \"description2\" ), ), }, }, }) } // below all TestAcc functions func testAccExampleThingConfigDescription ( rName string , description string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { description = %[2]q name = %[1]q } ` , rName , description ) }","title":"Per Attribute Acceptance Tests"},{"location":"running-and-writing-acceptance-tests/#cross-account-acceptance-tests","text":"When testing requires AWS infrastructure in a second AWS account, the below changes to the normal setup will allow the management or reference of resources and data sources across accounts: In the PreCheck function, include acctest.PreCheckOrganizationsAccount(t) to ensure a standardized set of information is required for cross-account testing credentials Switch usage of ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories to ProtoV5ProviderFactories: acctest.ProtoV5FactoriesAlternate(t) Add acctest.ConfigAlternateAccountProvider() to the test configuration and use provider = awsalternate for cross-account resources. The resource that is the focus of the acceptance test should not use the alternate provider identification to simplify the testing setup. For any TestStep that includes ImportState: true , add the Config that matches the previous TestStep Config An example acceptance test implementation can be seen below: func TestAccExample_basic ( t * testing . T ) { resourceName := \"aws_example.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) acctest . PreCheckOrganizationsAccount ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5FactoriesAlternate ( t ), CheckDestroy : testAccCheckExampleDestroy , Steps : [] resource . TestStep { { Config : testAccExampleConfig (), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleExists ( resourceName ), // ... additional checks ... ), }, { Config : testAccExampleConfig (), ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func testAccExampleConfig () string { return acctest . ConfigAlternateAccountProvider () + fmt . Sprintf ( ` # Cross account resources should be handled by the cross account provider. # The standardized provider block to use is awsalternate as seen below. resource \"aws_cross_account_example\" \"test\" { provider = awsalternate # ... configuration ... } # The resource that is the focus of the testing should be handled by the default provider, # which is automatically done by not specifying the provider configuration in the resource. resource \"aws_example\" \"test\" { # ... configuration ... } ` ) } Searching for usage of acctest.PreCheckOrganizationsAccount in the codebase will yield real world examples of this setup in action.","title":"Cross-Account Acceptance Tests"},{"location":"running-and-writing-acceptance-tests/#cross-region-acceptance-tests","text":"When testing requires AWS infrastructure in a second or third AWS region, the below changes to the normal setup will allow the management or reference of resources and data sources across regions: In the PreCheck function, include acctest.PreCheckMultipleRegion(t, ###) to ensure a standardized set of information is required for cross-region testing configuration. If the infrastructure in the second AWS region is also in a second AWS account also include acctest.PreCheckOrganizationsAccount(t) Switch usage of ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories to ProtoV5ProviderFactories: acctest.ProtoV5FactoriesMultipleRegions(t, 2) (where the last parameter is number of regions, 2 or 3) Add acctest.ConfigMultipleRegionProvider(###) to the test configuration and use provider = awsalternate (and potentially provider = awsthird ) for cross-region resources. The resource that is the focus of the acceptance test should not use the alternative providers to simplify the testing setup. If the infrastructure in the second AWS region is also in a second AWS account use testAccAlternateAccountAlternateRegionProviderConfig() (EC2) instead For any TestStep that includes ImportState: true , add the Config that matches the previous TestStep Config An example acceptance test implementation can be seen below: func TestAccExample_basic ( t * testing . T ) { var providers [] * schema . Provider resourceName := \"aws_example.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) acctest . PreCheckMultipleRegion ( t , 2 ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5FactoriesMultipleRegions ( t , 2 ), CheckDestroy : testAccCheckExampleDestroy , Steps : [] resource . TestStep { { Config : testAccExampleConfig (), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleExists ( resourceName ), // ... additional checks ... ), }, { Config : testAccExampleConfig (), ResourceName : resourceName , ImportState : true , ImportStateVerify : true , }, }, }) } func testAccExampleConfig () string { return acctest . ConfigMultipleRegionProvider ( 2 ) + fmt . Sprintf ( ` # Cross region resources should be handled by the cross region provider. # The standardized provider is awsalternate as seen below. resource \"aws_cross_region_example\" \"test\" { provider = awsalternate # ... configuration ... } # The resource that is the focus of the testing should be handled by the default provider, # which is automatically done by not specifying the provider configuration in the resource. resource \"aws_example\" \"test\" { # ... configuration ... } ` ) } Searching for usage of acctest.PreCheckMultipleRegion in the codebase will yield real world examples of this setup in action.","title":"Cross-Region Acceptance Tests"},{"location":"running-and-writing-acceptance-tests/#service-specific-region-acceptance-testing","text":"Certain AWS service APIs are only available in specific AWS regions. For example as of this writing, the pricing service is available in ap-south-1 and us-east-1 , but no other regions or partitions. When encountering these types of services, the acceptance testing can be setup to automatically detect the correct region(s), while skipping the testing in unsupported partitions. To prepare the shared service functionality, create a file named internal/service/{SERVICE}/acc_test.go . A starting example with the Pricing service ( internal/service/pricing/acc_test.go ): package aws import ( \"context\" \"sync\" \"testing\" \"github.com/aws/aws-sdk-go/aws/endpoints\" \"github.com/aws/aws-sdk-go/service/pricing\" \"github.com/hashicorp/terraform-plugin-sdk/v2/diag\" \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema\" \"github.com/hashicorp/terraform-plugin-sdk/v2/terraform\" \"github.com/hashicorp/terraform-provider-aws/internal/acctest\" \"github.com/hashicorp/terraform-provider-aws/internal/provider\" ) // testAccPricingRegion is the chosen Pricing testing region // // Cached to prevent issues should multiple regions become available. var testAccPricingRegion string // testAccProviderPricing is the Pricing provider instance // // This Provider can be used in testing code for API calls without requiring // the use of saving and referencing specific ProviderFactories instances. // // testAccPreCheckPricing(t) must be called before using this provider instance. var testAccProviderPricing * schema . Provider // testAccProviderPricingConfigure ensures the provider is only configured once var testAccProviderPricingConfigure sync . Once // testAccPreCheckPricing verifies AWS credentials and that Pricing is supported func testAccPreCheckPricing ( t * testing . T ) { acctest . PreCheckPartitionHasService ( pricing . EndpointsID , t ) // Since we are outside the scope of the Terraform configuration we must // call Configure() to properly initialize the provider configuration. testAccProviderPricingConfigure . Do ( func () { testAccProviderPricing = provider . Provider () config := map [ string ] interface {}{ \"region\" : testAccGetPricingRegion (), } diags := testAccProviderPricing . Configure ( context . Background (), terraform . NewResourceConfigRaw ( config )) if diags != nil && diags . HasError () { for _ , d := range diags { if d . Severity == diag . Error { t . Fatalf ( \"error configuring Pricing provider: %s\" , d . Summary ) } } } }) } // testAccPricingRegionProviderConfig is the Terraform provider configuration for Pricing region testing // // Testing Pricing assumes no other provider configurations // are necessary and overwrites the \"aws\" provider configuration. func testAccPricingRegionProviderConfig () string { return acctest . ConfigRegionalProvider ( testAccGetPricingRegion ()) } // testAccGetPricingRegion returns the Pricing region for testing func testAccGetPricingRegion () string { if testAccPricingRegion != \"\" { return testAccPricingRegion } if rs , ok := endpoints . RegionsForService ( endpoints . DefaultPartitions (), testAccGetPartition (), pricing . ServiceName ); ok { // return available region (random if multiple) for regionID := range rs { testAccPricingRegion = regionID return testAccPricingRegion } } testAccPricingRegion = testAccGetRegion () return testAccPricingRegion } For the resource or data source acceptance tests, the key items to adjust are: Ensure TestCase uses ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories instead of ProviderFactories: acctest.ProviderFactories or Providers: acctest.Providers Add the call for the new PreCheck function (keeping acctest.PreCheck(t) ), e.g. PreCheck: func() { acctest.PreCheck(t); testAccPreCheckPricing(t) }, If the testing is for a managed resource with a CheckDestroy function, ensure it uses the new provider instance, e.g. testAccProviderPricing , instead of acctest.Provider . If the testing is for a managed resource with a Check...Exists function, ensure it uses the new provider instance, e.g. testAccProviderPricing , instead of acctest.Provider . In each TestStep configuration, ensure the new provider configuration function is called, e.g. func testAccDataSourcePricingProductConfigRedshift () string { return acctest . ConfigCompose ( testAccPricingRegionProviderConfig (), ` # ... test configuration ... ` ) } If the testing configurations require more than one region, reach out to the maintainers for further assistance.","title":"Service-Specific Region Acceptance Testing"},{"location":"running-and-writing-acceptance-tests/#acceptance-test-concurrency","text":"Certain AWS service APIs allow a limited number of a certain component, while the acceptance testing runs at a default concurrency of twenty tests at a time. For example as of this writing, the SageMaker service only allows one SageMaker Domain per AWS Region. Running the tests with the default concurrency will fail with API errors relating to the component quota being exceeded. When encountering these types of components, the acceptance testing can be setup to limit the available concurrency of that particular component. When limited to one component at a time, this may also be referred to as serializing the acceptance tests. To convert to serialized (one test at a time) acceptance testing: Convert all existing capital T test functions with the limited component to begin with a lowercase t , e.g., TestAccSageMakerDomain_basic becomes testAccSageMakerDomain_basic . This will prevent the test framework from executing these tests directly as the prefix Test is required. In each of these test functions, convert resource.ParallelTest to resource.Test Create a capital T TestAcc{Service}{Thing}_serial test function that then references all the lowercase t test functions. If multiple test files are referenced, this new test be created in a new shared file such as internal/service/{SERVICE}/{SERVICE}_test.go . The contents of this test can be setup like the following: func TestAccExampleThing_serial ( t * testing . T ) { testCases := map [ string ] map [ string ] func ( t * testing . T ){ \"Thing\" : { \"basic\" : testAccExampleThing_basic , \"disappears\" : testAccExampleThing_disappears , // ... potentially other resource tests ... }, // ... potentially other top level resource test groups ... } for group , m := range testCases { m := m t . Run ( group , func ( t * testing . T ) { for name , tc := range m { tc := tc t . Run ( name , func ( t * testing . T ) { tc ( t ) }) } }) } } NOTE: Future iterations of these acceptance testing concurrency instructions will include the ability to handle more than one component at a time including service quota lookup, if supported by the service API.","title":"Acceptance Test Concurrency"},{"location":"running-and-writing-acceptance-tests/#data-source-acceptance-testing","text":"Writing acceptance testing for data sources is similar to resources, with the biggest changes being: Adding DataSource to the test and configuration naming, such as TestAccExampleThingDataSource_Filter The basic test may be named after the easiest lookup attribute instead, e.g., TestAccExampleThingDataSource_Name No disappears testing Almost all checks should be done with resource.TestCheckResourceAttrPair() to compare the data source attributes to the resource attributes The usage of an additional dataSourceName variable to store a data source reference, e.g., data.aws_example_thing.test Data sources testing should still use the CheckDestroy function of the resource, just to continue verifying that there are no dangling AWS resources after a test is run. Please note that we do not recommend re-using test configurations between resources and their associated data source as it is harder to discover testing regressions. Authors are encouraged to potentially implement similar \"base\" configurations though. For example: func TestAccExampleThingDataSource_Name ( t * testing . T ) { rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) dataSourceName := \"data.aws_example_thing.test\" resourceName := \"aws_example_thing.test\" resource . ParallelTest ( t , resource . TestCase { PreCheck : func () { acctest . PreCheck ( t ) }, ErrorCheck : acctest . ErrorCheck ( t , service . EndpointsID ), ProtoV5ProviderFactories : acctest . ProtoV5ProviderFactories , CheckDestroy : testAccCheckExampleThingDestroy , Steps : [] resource . TestStep { { Config : testAccExampleThingDataSourceConfigName ( rName ), Check : resource . ComposeTestCheckFunc ( testAccCheckExampleThingExists ( resourceName ), resource . TestCheckResourceAttrPair ( resourceName , \"arn\" , dataSourceName , \"arn\" ), resource . TestCheckResourceAttrPair ( resourceName , \"description\" , dataSourceName , \"description\" ), resource . TestCheckResourceAttrPair ( resourceName , \"name\" , dataSourceName , \"name\" ), ), }, }, }) } // below all TestAcc functions func testAccExampleThingDataSourceConfigName ( rName string ) string { return fmt . Sprintf ( ` resource \"aws_example_thing\" \"test\" { name = %[1]q } data \"aws_example_thing\" \"test\" { name = aws_example_thing.test.name } ` , rName ) }","title":"Data Source Acceptance Testing"},{"location":"running-and-writing-acceptance-tests/#acceptance-test-sweepers","text":"When running the acceptance tests, especially when developing or troubleshooting Terraform resources, its possible for code bugs or other issues to prevent the proper destruction of AWS infrastructure. To prevent lingering resources from consuming quota or causing unexpected billing, the Terraform Plugin SDK supports the test sweeper framework to clear out an AWS region of all resources. This section is meant to augment the SDKv2 documentation on test sweepers with Terraform AWS Provider specific details.","title":"Acceptance Test Sweepers"},{"location":"running-and-writing-acceptance-tests/#running-test-sweepers","text":"WARNING: Test Sweepers will destroy AWS infrastructure and backups in the target AWS account and region! These are designed to override any API deletion protection. Never run these outside a development AWS account that should be completely empty of resources. To run the sweepers for all resources in us-west-2 and us-east-1 (default testing regions): $ make sweep To run a specific resource sweeper: $ SWEEPARGS = -sweep-run = aws_example_thing make sweep To run sweepers with an assumed role, use the following additional environment variables: TF_AWS_ASSUME_ROLE_ARN - Required. TF_AWS_ASSUME_ROLE_DURATION - Optional, defaults to 1 hour (3600). TF_AWS_ASSUME_ROLE_EXTERNAL_ID - Optional. TF_AWS_ASSUME_ROLE_SESSION_NAME - Optional.","title":"Running Test Sweepers"},{"location":"running-and-writing-acceptance-tests/#sweeper-checklists","text":"Add Service To Sweeper List : To allow sweeping for a given service, it needs to be registered in the list of services to be swept, at internal/sweep/sweep_test.go . Add Resource Sweeper Implementation : See Writing Test Sweepers .","title":"Sweeper Checklists"},{"location":"running-and-writing-acceptance-tests/#writing-test-sweepers","text":"The first step is to initialize the resource into the test sweeper framework: func init () { resource . AddTestSweepers ( \"aws_example_thing\" , & resource . Sweeper { Name : \"aws_example_thing\" , F : sweepThings , // Optionally Dependencies : [] string { \"aws_other_thing\" , }, }) } Then add the actual implementation. Preferably, if a paginated SDK call is available: func sweepThings ( region string ) error { client , err := sweep . SharedRegionalSweepClient ( region ) if err != nil { return fmt . Errorf ( \"error getting client: %w\" , err ) } conn := client .( * conns . AWSClient ). ExampleConn sweepResources := make ([] sweep . Sweepable , 0 ) var errs * multierror . Error input := & example . ListThingsInput {} err = conn . ListThingsPages ( input , func ( page * example . ListThingsOutput , lastPage bool ) bool { if page == nil { return ! lastPage } for _ , thing := range page . Things { r := ResourceThing () d := r . Data ( nil ) id := aws . StringValue ( thing . Id ) d . SetId ( id ) // Perform resource specific pre-sweep setup. // For example, you may need to perform one or more of these types of pre-sweep tasks, specific to the resource: // // err := r.Read(d, client) // fill in data // d.Set(\"skip_final_snapshot\", true) // set an argument in order to delete // This \"if\" is only needed if the pre-sweep setup can produce errors. // Otherwise, do not include it. if err != nil { err := fmt . Errorf ( \"error reading Example Thing (%s): %w\" , id , err ) log . Printf ( \"[ERROR] %s\" , err ) errs = multierror . Append ( errs , err ) continue } sweepResources = append ( sweepResources , sweep . NewSweepResource ( r , d , client )) } return ! lastPage }) if err != nil { errs = multierror . Append ( errs , fmt . Errorf ( \"error listing Example Thing for %s: %w\" , region , err )) } if err := sweep . SweepOrchestrator ( sweepResources ); err != nil { errs = multierror . Append ( errs , fmt . Errorf ( \"error sweeping Example Thing for %s: %w\" , region , err )) } if sweep . SkipSweepError ( err ) { log . Printf ( \"[WARN] Skipping Example Thing sweep for %s: %s\" , region , errs ) return nil } return errs . ErrorOrNil () } Otherwise, if no paginated SDK call is available: func sweepThings ( region string ) error { client , err := sweep . SharedRegionalSweepClient ( region ) if err != nil { return fmt . Errorf ( \"error getting client: %w\" , err ) } conn := client .( * conns . AWSClient ). ExampleConn sweepResources := make ([] sweep . Sweepable , 0 ) var errs * multierror . Error input := & example . ListThingsInput {} for { output , err := conn . ListThings ( input ) for _ , thing := range output . Things { r := ResourceThing () d := r . Data ( nil ) id := aws . StringValue ( thing . Id ) d . SetId ( id ) // Perform resource specific pre-sweep setup. // For example, you may need to perform one or more of these types of pre-sweep tasks, specific to the resource: // // err := r.Read(d, client) // fill in data // d.Set(\"skip_final_snapshot\", true) // set an argument in order to delete // This \"if\" is only needed if the pre-sweep setup can produce errors. // Otherwise, do not include it. if err != nil { err := fmt . Errorf ( \"error reading Example Thing (%s): %w\" , id , err ) log . Printf ( \"[ERROR] %s\" , err ) errs = multierror . Append ( errs , err ) continue } sweepResources = append ( sweepResources , sweep . NewSweepResource ( r , d , client )) } if aws . StringValue ( output . NextToken ) == \"\" { break } input . NextToken = output . NextToken } if err := sweep . SweepOrchestrator ( sweepResources ); err != nil { errs = multierror . Append ( errs , fmt . Errorf ( \"error sweeping Example Thing for %s: %w\" , region , err )) } if sweep . SkipSweepError ( err ) { log . Printf ( \"[WARN] Skipping Example Thing sweep for %s: %s\" , region , errs ) return nil } return errs . ErrorOrNil () }","title":"Writing Test Sweepers"},{"location":"running-and-writing-acceptance-tests/#acceptance-test-checklists","text":"There are several aspects to writing good acceptance tests. These checklists will help ensure effective testing from the design stage through to implementation details.","title":"Acceptance Test Checklists"},{"location":"running-and-writing-acceptance-tests/#basic-acceptance-test-design","text":"These are basic principles to help guide the creation of acceptance tests. Covers Changes : Every line of resource or data source code added or changed should be covered by one or more tests. For example, if a resource has two ways of functioning, tests should cover both possible paths. Nearly every codebase change needs test coverage to ensure functionality and prevent future regressions. If a bug or other problem prompted a fix, a test should be added that previously would have failed, especially if the report included a configuration. Follows the Single Responsibility Principle : Every test should have a single responsibility and effectively test that responsibility. This may include individual tests for verifying basic functionality of the resource (Create, Read, Delete), separately verifying using and updating a single attribute in a resource, or separately changing between two attributes to verify two \"modes\"/\"types\" possible with a resource configuration. In following this principle, test configurations should be as simple as possible. For example, not including extra configuration unless it is necessary for the specific test.","title":"Basic Acceptance Test Design"},{"location":"running-and-writing-acceptance-tests/#test-implementation","text":"The below are required items that will be noted during submission review and prevent immediate merging: Implements CheckDestroy : Resource testing should include a CheckDestroy function (typically named testAccCheck{SERVICE}{RESOURCE}Destroy ) that calls the API to verify that the Terraform resource has been deleted or disassociated as appropriate. More information about CheckDestroy functions can be found in the SDKv2 TestCase documentation . Implements Exists Check Function : Resource testing should include a TestCheckFunc function (typically named testAccCheck{SERVICE}{RESOURCE}Exists ) that calls the API to verify that the Terraform resource has been created or associated as appropriate. Preferably, this function will also accept a pointer to an API object representing the Terraform resource from the API response that can be set for potential usage in later TestCheckFunc . More information about these functions can be found in the SDKv2 Custom Check Functions documentation . Excludes Provider Declarations : Test configurations should not include provider \"aws\" {...} declarations. If necessary, only the provider declarations in acctest.go should be used for multiple account/region or otherwise specialized testing. Passes in us-west-2 Region : Tests default to running in us-west-2 and at a minimum should pass in that region or include necessary PreCheck functions to skip the test when ran outside an expected environment. Includes ErrorCheck : All acceptance tests should include a call to the common ErrorCheck ( ErrorCheck: acctest.ErrorCheck(t, service.EndpointsID), ). Uses resource.ParallelTest : Tests should use resource.ParallelTest() instead of resource.Test() except where serialized testing is absolutely required. [ ] Uses fmt.Sprintf() : Test configurations preferably should to be separated into their own functions (typically named testAcc{SERVICE}{RESOURCE}Config{PURPOSE} ) that call fmt.Sprintf() for variable injection or a string const for completely static configurations. Test configurations should avoid var or other variable injection functionality such as text/template . Uses Randomized Infrastructure Naming : Test configurations that use resources where a unique name is required should generate a random name. Typically this is created via rName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix) in the acceptance test function before generating the configuration. Prevents S3 Bucket Deletion Errors : Test configurations that use aws_s3_bucket resources as a logging destination should include the force_destroy = true configuration. This is to prevent race conditions where logging objects may be written during the testing duration which will cause BucketNotEmpty errors during deletion. For resources that support import, the additional item below is required that will be noted during submission review and prevent immediate merging: Implements ImportState Testing : Tests should include an additional TestStep configuration that verifies resource import via ImportState: true and ImportStateVerify: true . This TestStep should be added to all possible tests for the resource to ensure that all infrastructure configurations are properly imported into Terraform. The below are style-based items that may be noted during review and are recommended for simplicity, consistency, and quality assurance: Uses Builtin Check Functions : Tests should use already available check functions, e.g. resource.TestCheckResourceAttr() , to verify values in the Terraform state over creating custom TestCheckFunc . More information about these functions can be found in the SDKv2 Builtin Check Functions documentation . Uses TestCheckResourceAttrPair() for Data Sources : Tests should use resource.TestCheckResourceAttrPair() to verify values in the Terraform state for data sources attributes to compare them with their expected resource attributes. Excludes Timeouts Configurations : Test configurations should not include timeouts {...} configuration blocks except for explicit testing of customizable timeouts (typically very short timeouts with ExpectError ). Implements Default and Zero Value Validation : The basic test for a resource (typically named TestAcc{SERVICE}{RESOURCE}_basic ) should use available check functions, e.g. resource.TestCheckResourceAttr() , to verify default and zero values in the Terraform state for all attributes. Empty/missing configuration blocks can be verified with resource.TestCheckResourceAttr(resourceName, \"{ATTRIBUTE}.#\", \"0\") and empty maps with resource.TestCheckResourceAttr(resourceName, \"{ATTRIBUTE}.%\", \"0\")","title":"Test Implementation"},{"location":"running-and-writing-acceptance-tests/#avoid-hard-coding","text":"Avoid hard coding values in acceptance test checks and configurations for consistency and testing flexibility. Resource testing is expected to pass across multiple AWS environments supported by the Terraform AWS Provider (e.g., AWS Standard and AWS GovCloud (US)). Contributors are not expected or required to perform testing outside of AWS Standard, e.g., running only in the us-west-2 region is perfectly acceptable. However, contributors are expected to avoid hard coding with these guidelines.","title":"Avoid Hard Coding"},{"location":"running-and-writing-acceptance-tests/#hardcoded-account-ids","text":"Uses Account Data Sources : Any hardcoded account numbers in configuration, e.g., 137112412989 , should be replaced with a data source. Depending on the situation, there are several data sources for account IDs including: aws_caller_identity data source , aws_canonical_user_id data source , aws_billing_service_account data source , and aws_sagemaker_prebuilt_ecr_image data source . Uses Account Test Checks : Any check required to verify an AWS Account ID of the current testing account or another account should use one of the following available helper functions over the usage of resource.TestCheckResourceAttrSet() and resource.TestMatchResourceAttr() : acctest.CheckResourceAttrAccountID() : Validates the state value equals the AWS Account ID of the current account running the test. This is the most common implementation. acctest.MatchResourceAttrAccountID() : Validates the state value matches any AWS Account ID (e.g. a 12 digit number). This is typically only used in data source testing of AWS managed components. Here's an example of using aws_caller_identity : data \"aws_caller_identity\" \"current\" {} resource \"aws_backup_selection\" \"test\" { plan_id = aws_backup_plan.test.id name = \"tf_acc_test_backup_selection_%[1]d\" iam_role_arn = \"arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/service-role/AWSBackupDefaultServiceRole\" }","title":"Hardcoded Account IDs"},{"location":"running-and-writing-acceptance-tests/#hardcoded-ami-ids","text":"Uses aws_ami Data Source : Any hardcoded AMI ID configuration, e.g. ami-12345678 , should be replaced with the aws_ami data source pointing to an Amazon Linux image. The package internal/acctest includes test configuration helper functions to simplify these lookups: acctest.ConfigLatestAmazonLinuxHVMEBSAMI() : The recommended AMI for most situations, using Amazon Linux, HVM virtualization, and EBS storage. To reference the AMI ID in the test configuration: data.aws_ami.amzn-ami-minimal-hvm-ebs.id . testAccLatestAmazonLinuxHVMInstanceStoreAMIConfig() (EC2): AMI lookup using Amazon Linux, HVM virtualization, and Instance Store storage. Should only be used in testing that requires Instance Store storage rather than EBS. To reference the AMI ID in the test configuration: data.aws_ami.amzn-ami-minimal-hvm-instance-store.id . testAccLatestAmazonLinuxPVEBSAMIConfig() (EC2): AMI lookup using Amazon Linux, Paravirtual virtualization, and EBS storage. Should only be used in testing that requires Paravirtual over Hardware Virtual Machine (HVM) virtualization. To reference the AMI ID in the test configuration: data.aws_ami.amzn-ami-minimal-pv-ebs.id . configLatestAmazonLinuxPvInstanceStoreAmi (EC2): AMI lookup using Amazon Linux, Paravirtual virtualization, and Instance Store storage. Should only be used in testing that requires Paravirtual virtualization over HVM and Instance Store storage over EBS. To reference the AMI ID in the test configuration: data.aws_ami.amzn-ami-minimal-pv-instance-store.id . testAccLatestWindowsServer2016CoreAMIConfig() (EC2): AMI lookup using Windows Server 2016 Core, HVM virtualization, and EBS storage. Should only be used in testing that requires Windows. To reference the AMI ID in the test configuration: data.aws_ami.win2016core-ami.id . Here's an example of using acctest.ConfigLatestAmazonLinuxHVMEBSAMI() and data.aws_ami.amzn-ami-minimal-hvm-ebs.id : func testAccLaunchConfigurationDataSourceConfig_basic ( rName string ) string { return acctest . ConfigCompose ( acctest . ConfigLatestAmazonLinuxHVMEBSAMI (), fmt . Sprintf ( ` resource \"aws_launch_configuration\" \"test\" { name = %[1]q image_id = data.aws_ami.amzn-ami-minimal-hvm-ebs.id instance_type = \"m1.small\" } ` , rName )) }","title":"Hardcoded AMI IDs"},{"location":"running-and-writing-acceptance-tests/#hardcoded-availability-zones","text":"Uses aws_availability_zones Data Source : Any hardcoded AWS Availability Zone configuration, e.g. us-west-2a , should be replaced with the aws_availability_zones data source . Use the convenience function called acctest.ConfigAvailableAZsNoOptIn() (defined in internal/acctest/acctest.go ) to declare data \"aws_availability_zones\" \"available\" {...} . You can then reference the data source via data.aws_availability_zones.available.names[0] or data.aws_availability_zones.available.names[count.index] in resources using count . Here's an example of using acctest.ConfigAvailableAZsNoOptIn() and data.aws_availability_zones.available.names[0] : func testAccInstanceVpcConfigBasic ( rName string ) string { return acctest . ConfigCompose ( acctest . ConfigAvailableAZsNoOptIn (), fmt . Sprintf ( ` resource \"aws_subnet\" \"test\" { availability_zone = data.aws_availability_zones.available.names[0] cidr_block = \"10.0.0.0/24\" vpc_id = aws_vpc.test.id } ` , rName )) }","title":"Hardcoded Availability Zones"},{"location":"running-and-writing-acceptance-tests/#hardcoded-database-versions","text":"Uses Database Version Data Sources : Hardcoded database versions, e.g., RDS MySQL Engine Version 5.7.42 , should be removed (which means the AWS-defined default version will be used) or replaced with a list of preferred versions using a data source. Because versions change over times and version offerings vary from region to region and partition to partition, using the default version or providing a list of preferences ensures a version will be available. Depending on the situation, there are several data sources for versions, including: aws_rds_engine_version data source , aws_docdb_engine_version data source , and aws_neptune_engine_version data source . Here's an example of using aws_rds_engine_version and data.aws_rds_engine_version.default.version : data \"aws_rds_engine_version\" \"default\" { engine = \"mysql\" } data \"aws_rds_orderable_db_instance\" \"test\" { engine = data.aws_rds_engine_version.default.engine engine_version = data.aws_rds_engine_version.default.version preferred_instance_classes = [ \"db.t3.small\", \"db.t2.small\", \"db.t2.medium\" ] } resource \"aws_db_instance\" \"bar\" { engine = data.aws_rds_engine_version.default.engine engine_version = data.aws_rds_engine_version.default.version instance_class = data.aws_rds_orderable_db_instance.test.instance_class skip_final_snapshot = true parameter_group_name = \"default.${data.aws_rds_engine_version.default.parameter_group_family}\" }","title":"Hardcoded Database Versions"},{"location":"running-and-writing-acceptance-tests/#hardcoded-direct-connect-locations","text":"Uses aws_dx_locations Data Source : Hardcoded AWS Direct Connect locations, e.g., EqSe2 , should be replaced with the aws_dx_locations data source . Here's an example using data.aws_dx_locations.test.location_codes : data \"aws_dx_locations\" \"test\" {} resource \"aws_dx_lag\" \"test\" { name = \"Test LAG\" connections_bandwidth = \"1Gbps\" location = tolist ( data.aws_dx_locations.test.location_codes )[ 0 ] force_destroy = true }","title":"Hardcoded Direct Connect Locations"},{"location":"running-and-writing-acceptance-tests/#hardcoded-instance-types","text":"Uses Instance Type Data Source : Singular hardcoded instance types and classes, e.g., t2.micro and db.t2.micro , should be replaced with a list of preferences using a data source. Because offerings vary from region to region and partition to partition, providing a list of preferences dramatically improves the likelihood that one of the options will be available. Depending on the situation, there are several data sources for instance types and classes, including: aws_ec2_instance_type_offering data source - Convenience functions declare configurations that are referenced with data.aws_ec2_instance_type_offering.available including: The acctest.AvailableEC2InstanceTypeForAvailabilityZone() function for test configurations using an EC2 Subnet which is inherently within a single Availability Zone The acctest.AvailableEC2InstanceTypeForRegion() function for test configurations that do not include specific Availability Zones aws_rds_orderable_db_instance data source , aws_neptune_orderable_db_instance data source , and aws_docdb_orderable_db_instance data source . Here's an example of using acctest.AvailableEC2InstanceTypeForRegion() and data.aws_ec2_instance_type_offering.available.instance_type : func testAccSpotInstanceRequestConfig ( rInt int ) string { return acctest . ConfigCompose ( acctest . AvailableEC2InstanceTypeForRegion ( \"t3.micro\" , \"t2.micro\" ), fmt . Sprintf ( ` resource \"aws_spot_instance_request\" \"test\" { instance_type = data.aws_ec2_instance_type_offering.available.instance_type spot_price = \"0.05\" wait_for_fulfillment = true } ` , rInt )) } Here's an example of using aws_rds_orderable_db_instance and data.aws_rds_orderable_db_instance.test.instance_class : data \"aws_rds_orderable_db_instance\" \"test\" { engine = \"mysql\" engine_version = \"5.7.31\" preferred_instance_classes = [ \"db.t3.micro\", \"db.t2.micro\", \"db.t3.small\" ] } resource \"aws_db_instance\" \"test\" { engine = data.aws_rds_orderable_db_instance.test.engine engine_version = data.aws_rds_orderable_db_instance.test.engine_version instance_class = data.aws_rds_orderable_db_instance.test.instance_class skip_final_snapshot = true username = \"test\" }","title":"Hardcoded Instance Types"},{"location":"running-and-writing-acceptance-tests/#hardcoded-partition-dns-suffix","text":"Uses aws_partition Data Source : Any hardcoded DNS suffix configuration, e.g., the amazonaws.com in a ec2.amazonaws.com service principal, should be replaced with the aws_partition data source . A common pattern is declaring data \"aws_partition\" \"current\" {} and referencing it via data.aws_partition.current.dns_suffix . Here's an example of using aws_partition and data.aws_partition.current.dns_suffix : data \"aws_partition\" \"current\" {} resource \"aws_iam_role\" \"test\" { assume_role_policy = << POLICY { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"cloudtrail.${data.aws_partition.current.dns_suffix}\" }, \"Action\": \"sts:AssumeRole\" } ] } POLICY }","title":"Hardcoded Partition DNS Suffix"},{"location":"running-and-writing-acceptance-tests/#hardcoded-partition-in-arn","text":"Uses aws_partition Data Source : Any hardcoded AWS Partition configuration, e.g. the aws in a arn:aws:SERVICE:REGION:ACCOUNT:RESOURCE ARN, should be replaced with the aws_partition data source . A common pattern is declaring data \"aws_partition\" \"current\" {} and referencing it via data.aws_partition.current.partition . Uses Builtin ARN Check Functions : Tests should use available ARN check functions to validate ARN attribute values in the Terraform state over resource.TestCheckResourceAttrSet() and resource.TestMatchResourceAttr() : acctest.CheckResourceAttrRegionalARN() verifies that an ARN matches the account ID and region of the test execution with an exact resource value acctest.MatchResourceAttrRegionalARN() verifies that an ARN matches the account ID and region of the test execution with a regular expression of the resource value acctest.CheckResourceAttrGlobalARN() verifies that an ARN matches the account ID of the test execution with an exact resource value acctest.MatchResourceAttrGlobalARN() verifies that an ARN matches the account ID of the test execution with a regular expression of the resource value acctest.CheckResourceAttrRegionalARNNoAccount() verifies than an ARN has no account ID and matches the current region of the test execution with an exact resource value acctest.CheckResourceAttrGlobalARNNoAccount() verifies than an ARN has no account ID and matches an exact resource value acctest.CheckResourceAttrRegionalARNAccountID() verifies than an ARN matches a specific account ID and the current region of the test execution with an exact resource value acctest.CheckResourceAttrGlobalARNAccountID() verifies than an ARN matches a specific account ID with an exact resource value Here's an example of using aws_partition and data.aws_partition.current.partition : data \"aws_partition\" \"current\" {} resource \"aws_iam_role_policy_attachment\" \"test\" { policy_arn = \"arn:${data.aws_partition.current.partition}:iam::aws:policy/service-role/AmazonRDSEnhancedMonitoringRole\" role = aws_iam_role.test.name }","title":"Hardcoded Partition in ARN"},{"location":"running-and-writing-acceptance-tests/#hardcoded-region","text":"Uses aws_region Data Source : Any hardcoded AWS Region configuration, e.g., us-west-2 , should be replaced with the aws_region data source . A common pattern is declaring data \"aws_region\" \"current\" {} and referencing it via data.aws_region.current.name Here's an example of using aws_region and data.aws_region.current.name : data \"aws_region\" \"current\" {} resource \"aws_route53_zone\" \"test\" { vpc { vpc_id = aws_vpc.test.id vpc_region = data.aws_region.current.name } }","title":"Hardcoded Region"},{"location":"running-and-writing-acceptance-tests/#hardcoded-spot-price","text":"Uses aws_ec2_spot_price Data Source : Any hardcoded spot prices, e.g., 0.05 , should be replaced with the aws_ec2_spot_price data source . A common pattern is declaring data \"aws_ec2_spot_price\" \"current\" {} and referencing it via data.aws_ec2_spot_price.current.spot_price . Here's an example of using aws_ec2_spot_price and data.aws_ec2_spot_price.current.spot_price : data \"aws_ec2_spot_price\" \"current\" { instance_type = \"t3.medium\" filter { name = \"product-description\" values = [ \"Linux/UNIX\" ] } } resource \"aws_spot_fleet_request\" \"test\" { spot_price = data.aws_ec2_spot_price.current.spot_price target_capacity = 2 }","title":"Hardcoded Spot Price"},{"location":"running-and-writing-acceptance-tests/#hardcoded-ssh-keys","text":"Uses acctest.RandSSHKeyPair() or RandSSHKeyPairSize() Functions : Any hardcoded SSH keys should be replaced with random SSH keys generated by either the acceptance testing framework's function RandSSHKeyPair() or the provider function RandSSHKeyPairSize() . RandSSHKeyPair() generates 1024-bit keys. Here's an example using aws_key_pair func TestAccKeyPair_basic ( t * testing . T ) { ... rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) publicKey , _ , err := acctest . RandSSHKeyPair ( acctest . DefaultEmailAddress ) if err != nil { t . Fatalf ( \"error generating random SSH key: %s\" , err ) } resource . ParallelTest ( t , resource . TestCase { ... Steps : [] resource . TestStep { { Config : testAccKeyPairConfig ( rName , publicKey ), ... }, }, }) } func testAccKeyPairConfig ( rName , publicKey string ) string { return fmt . Sprintf ( ` resource \"aws_key_pair\" \"test\" { key_name = %[1]q public_key = %[2]q } ` , rName , publicKey ) }","title":"Hardcoded SSH Keys"},{"location":"running-and-writing-acceptance-tests/#hardcoded-email-addresses","text":"Uses either acctest.DefaultEmailAddress Constant or acctest.RandomEmailAddress() Function : Any hardcoded email addresses should replaced with either the constant acctest.DefaultEmailAddress or the function acctest.RandomEmailAddress() . Using acctest.DefaultEmailAddress is preferred when using a single email address in an acceptance test. Here's an example using acctest.DefaultEmailAddress func TestAccSNSTopicSubscription_email ( t * testing . T ) { ... rName := sdkacctest . RandomWithPrefix ( acctest . ResourcePrefix ) resource . ParallelTest ( t , resource . TestCase { ... Steps : [] resource . TestStep { { Config : testAccTopicSubscriptionEmailConfig ( rName , acctest . DefaultEmailAddress ), Check : resource . ComposeTestCheckFunc ( ... resource . TestCheckResourceAttr ( resourceName , \"endpoint\" , acctest . DefaultEmailAddress ), ), }, }, }) } Here's an example using acctest.RandomEmailAddress() func TestAccPinpointEmailChannel_basic ( t * testing . T ) { ... domain := acctest . RandomDomainName () address1 := acctest . RandomEmailAddress ( domain ) address2 := acctest . RandomEmailAddress ( domain ) resource . ParallelTest ( t , resource . TestCase { ... Steps : [] resource . TestStep { { Config : testAccEmailChannelConfig_FromAddress ( domain , address1 ), Check : resource . ComposeTestCheckFunc ( ... resource . TestCheckResourceAttr ( resourceName , \"from_address\" , address1 ), ), }, { Config : testAccEmailChannelConfig_FromAddress ( domain , address2 ), Check : resource . ComposeTestCheckFunc ( ... resource . TestCheckResourceAttr ( resourceName , \"from_address\" , address2 ), ), }, }, }) }","title":"Hardcoded Email Addresses"},{"location":"service-package-pullrequest-guide/","text":"Service Package Refactor Pull Request Guide # Pull request #21306 has significantly refactored the AWS provider codebase. Specifically, the code for all AWS resources and data sources has been relocated from a single aws directory to a large number of separate directories in internal/service , each corresponding to a particular AWS service. In addition to vastly simplifying the codebase's overall structure, this change has also allowed us to simplify the names of a number of underlying functions -- without encountering namespace collisions. Issue #20000 contains a more complete description of these changes. As a result, nearly every pull request opened prior to the refactoring has merge conflicts; they are attempting to apply changes to files that have since been relocated. Furthermore, any new files or functions introduced must be brought into line with the codebase's new conventions. The following steps are intended to resolve such a conflict -- though it should be noted that this guide is an active work in progress as additional pull requests are amended. These fixes, however, in no way affect the prioritization of a particular pull request. Once a pull request has been selected for review, the necessary changes will be made by a maintainer -- either directly or in collaboration with the pull request author. Fixing a Pre-Refactor Pull Request # git checkout the branch pertaining to the pull request you wish to amend Begin a merge of the latest version of main branch into your local branch: git pull origin main . Merge conflicts are expected. For any new file , rename and move the file to its appropriate service package directory: Resource Files git mv aws/resource_aws_{service_name}_{resource_name}.go \\ internal/service/{service_name}/{resource_name}.go Resource Test Files git mv aws/resource_aws_{service_name}_{resource_name}_test.go \\ internal/service/{service_name}/{resource_name}_test.go Data Source Files git mv aws/data_source_aws_{service_name}_{resource_name}.go \\ internal/service/{service_name}/{resource_name}_data_source.go Data Source Test Files git mv aws/data_source_aws_{service_name}_{resource_name}_test.go \\ internal/service/{service_name}/{resource_name}_data_source_test.go For any new function , rename the function appropriately: Resource Schema Functions func resourceAws{ResourceName}() => func Resource{ResourceName}() Resource Generic Functions func resourceAws{ServiceName}{ResourceName}{FunctionName}() => func resource{ResourceName}{FunctionName}() Resource Acceptance Test Functions func TestAccAWS{ServiceName}{ResourceName}_{testType}() => func TestAcc{ResourceName}_{testType}() Data Source Schema Functions func dataSourceAws{ResourceName}() => func DataSource{ResourceName}() Data Source Generic Functions func dataSourceAws{ServiceName}{ResourceName}{FunctionName}() => func dataSource{ResourceName}{FunctionName}() Data Source Acceptance Test Functions func TestAccDataSourceAWS{ServiceName}{ResourceName}_{testType}() => func TestAcc{ResourceName}DataSource_{testType}() Finder Functions func finder.{FunctionName}() => func Find{FunctionName}() Status Functions func waiter.{FunctionName}Status() => func status{FunctionName}() Waiter Functions func waiter.{FunctionName}() => func wait{FunctionName}() If a file has a package declaration of package aws , you will need to change it to the new package location. For example, if you moved a file to internal/service/ecs , the declaration will now be package ecs . Any file that imports \"github.com/hashicorp/terraform-provider-aws/internal/acctest\" must be in the <package>_test package. For example, internal/service/ecs/account_setting_default_test.go does import the acctest package and must have a package declaration of package ecs_test . If you have made any changes to aws/provider.go , you will have to manually re-enact those changes on the new internal/provider/provider.go file. Most commonly, these changes involve the addition of an entry to either the DataSourcesMap or ResourcesMap . If this is the case for your PR, you will have to adapt your entry to follow our new code conventions. Resources Map Entries \"{aws_terraform_resource_type}\": resourceAws{ServiceName}{ResourceName}(), => \"{aws_terraform_resource_type}\": {serviceName}.Resource{ResourceName}(), Data Source Map Entries \"{aws_terraform_data_source_type}\": dataSourceAws{ServiceName}{ResourceName}(), => \"{aws_terraform_data_source_type}\": {serviceName}.DataSource{ResourceName}(), Some functions, constants, and variables have been moved, removed, or renamed. This table shows some of the common changes you may need to make to fix compile errors. Before Now isAWSErr(\u03b1, \u03b2, \"<message>\") tfawserr.ErrMessageContains(\u03b1, \u03b2, \"<message>\") isAWSErr(\u03b1, \u03b2, \"\") tfawserr.ErrCodeEquals(\u03b1, \u03b2) isResourceNotFoundError(\u03b1) tfresource.NotFound(\u03b1) isResourceTimeoutError(\u03b1) tfresource.TimedOut(\u03b1) testSweepSkipResourceError(\u03b1) tfawserr.ErrCodeContains(\u03b1, \"AccessDenied\") testAccPreCheck(t) acctest.PreCheck(t) testAccProviders acctest.Providers acctest.RandomWithPrefix(\"tf-acc-test\") sdkacctest.RandomWithPrefix(acctest.ResourcePrefix) composeConfig(\u03b1) acctest.ConfigCompose(\u03b1) Use git status to report the state of the merge. Review any merge conflicts -- being sure to adopt the new naming conventions described in the previous step where relevant. Use git add to add any new files to the commit.","title":"Service Packages Refactor"},{"location":"service-package-pullrequest-guide/#service-package-refactor-pull-request-guide","text":"Pull request #21306 has significantly refactored the AWS provider codebase. Specifically, the code for all AWS resources and data sources has been relocated from a single aws directory to a large number of separate directories in internal/service , each corresponding to a particular AWS service. In addition to vastly simplifying the codebase's overall structure, this change has also allowed us to simplify the names of a number of underlying functions -- without encountering namespace collisions. Issue #20000 contains a more complete description of these changes. As a result, nearly every pull request opened prior to the refactoring has merge conflicts; they are attempting to apply changes to files that have since been relocated. Furthermore, any new files or functions introduced must be brought into line with the codebase's new conventions. The following steps are intended to resolve such a conflict -- though it should be noted that this guide is an active work in progress as additional pull requests are amended. These fixes, however, in no way affect the prioritization of a particular pull request. Once a pull request has been selected for review, the necessary changes will be made by a maintainer -- either directly or in collaboration with the pull request author.","title":"Service Package Refactor Pull Request Guide"},{"location":"service-package-pullrequest-guide/#fixing-a-pre-refactor-pull-request","text":"git checkout the branch pertaining to the pull request you wish to amend Begin a merge of the latest version of main branch into your local branch: git pull origin main . Merge conflicts are expected. For any new file , rename and move the file to its appropriate service package directory: Resource Files git mv aws/resource_aws_{service_name}_{resource_name}.go \\ internal/service/{service_name}/{resource_name}.go Resource Test Files git mv aws/resource_aws_{service_name}_{resource_name}_test.go \\ internal/service/{service_name}/{resource_name}_test.go Data Source Files git mv aws/data_source_aws_{service_name}_{resource_name}.go \\ internal/service/{service_name}/{resource_name}_data_source.go Data Source Test Files git mv aws/data_source_aws_{service_name}_{resource_name}_test.go \\ internal/service/{service_name}/{resource_name}_data_source_test.go For any new function , rename the function appropriately: Resource Schema Functions func resourceAws{ResourceName}() => func Resource{ResourceName}() Resource Generic Functions func resourceAws{ServiceName}{ResourceName}{FunctionName}() => func resource{ResourceName}{FunctionName}() Resource Acceptance Test Functions func TestAccAWS{ServiceName}{ResourceName}_{testType}() => func TestAcc{ResourceName}_{testType}() Data Source Schema Functions func dataSourceAws{ResourceName}() => func DataSource{ResourceName}() Data Source Generic Functions func dataSourceAws{ServiceName}{ResourceName}{FunctionName}() => func dataSource{ResourceName}{FunctionName}() Data Source Acceptance Test Functions func TestAccDataSourceAWS{ServiceName}{ResourceName}_{testType}() => func TestAcc{ResourceName}DataSource_{testType}() Finder Functions func finder.{FunctionName}() => func Find{FunctionName}() Status Functions func waiter.{FunctionName}Status() => func status{FunctionName}() Waiter Functions func waiter.{FunctionName}() => func wait{FunctionName}() If a file has a package declaration of package aws , you will need to change it to the new package location. For example, if you moved a file to internal/service/ecs , the declaration will now be package ecs . Any file that imports \"github.com/hashicorp/terraform-provider-aws/internal/acctest\" must be in the <package>_test package. For example, internal/service/ecs/account_setting_default_test.go does import the acctest package and must have a package declaration of package ecs_test . If you have made any changes to aws/provider.go , you will have to manually re-enact those changes on the new internal/provider/provider.go file. Most commonly, these changes involve the addition of an entry to either the DataSourcesMap or ResourcesMap . If this is the case for your PR, you will have to adapt your entry to follow our new code conventions. Resources Map Entries \"{aws_terraform_resource_type}\": resourceAws{ServiceName}{ResourceName}(), => \"{aws_terraform_resource_type}\": {serviceName}.Resource{ResourceName}(), Data Source Map Entries \"{aws_terraform_data_source_type}\": dataSourceAws{ServiceName}{ResourceName}(), => \"{aws_terraform_data_source_type}\": {serviceName}.DataSource{ResourceName}(), Some functions, constants, and variables have been moved, removed, or renamed. This table shows some of the common changes you may need to make to fix compile errors. Before Now isAWSErr(\u03b1, \u03b2, \"<message>\") tfawserr.ErrMessageContains(\u03b1, \u03b2, \"<message>\") isAWSErr(\u03b1, \u03b2, \"\") tfawserr.ErrCodeEquals(\u03b1, \u03b2) isResourceNotFoundError(\u03b1) tfresource.NotFound(\u03b1) isResourceTimeoutError(\u03b1) tfresource.TimedOut(\u03b1) testSweepSkipResourceError(\u03b1) tfawserr.ErrCodeContains(\u03b1, \"AccessDenied\") testAccPreCheck(t) acctest.PreCheck(t) testAccProviders acctest.Providers acctest.RandomWithPrefix(\"tf-acc-test\") sdkacctest.RandomWithPrefix(acctest.ResourcePrefix) composeConfig(\u03b1) acctest.ConfigCompose(\u03b1) Use git status to report the state of the merge. Review any merge conflicts -- being sure to adopt the new naming conventions described in the previous step where relevant. Use git add to add any new files to the commit.","title":"Fixing a Pre-Refactor Pull Request"},{"location":"skaff/","text":"Provider Scaffolding (skaff) # skaff is a Terraform AWS Provider scaffolding command line tool. It generates resource/data source files and accompanying test files which adhere to the latest best practice. These files are heavily commented with instructions so serve as the best way to get started with provider development. Overview workflow steps # Figure out what you're trying to do: Create a resource or a data source? AWS Go SDK v1 or v2 code? Name of the new resource or data source? Use skaff to generate provider code Go through the generated code completing code and customizing for the AWS Go SDK API Run, test, refine Remove \"TIP\" comments Submit code in pull request Running skaff # Use Git to clone the GitHub https://github.com/hashicorp/terraform-provider-aws repository. cd skaff go install . Change directories to the service where your new resource will reside. E.g. , cd ../internal/service/mq . To get help, enter skaff without arguments. Generate a resource. E.g. , skaff resource --name BrokerReboot (or equivalently skaff resource -n BrokerReboot ). Usage # Help # $ skaff --help Usage: skaff [command] Available Commands: completion Generate the autocompletion script for the specified shell datasource Create scaffolding for a data source help Help about any command resource Create scaffolding for a resource Flags: -h, --help help for skaff Autocompletion # Generate the autocompletion script for skaff for the specified shell $ skaff completion --help Usage: skaff completion [command] Available Commands: bash Generate the autocompletion script for bash fish Generate the autocompletion script for fish powershell Generate the autocompletion script for powershell zsh Generate the autocompletion script for zsh Flags: -h, --help help for completion Use \"skaff completion [command] --help\" for more information about a command Data Source # Create scaffolding for a data source $ skaff datasource --help Usage: skaff datasource [flags] Flags: -c, --clear-comments Do not include instructional comments in source -f, --force Force creation, overwriting existing files -h, --help help for datasource -n, --name string Name of the entity -s, --snakename string If skaff doesn't get it right, explicitly give name in snake case (e.g., db_vpc_instance) -o, --v1 Generate code targeting aws-sdk-go v1 (some existing services) Resource # Create scaffolding for a resource $ skaff resource --help Usage: skaff resource [flags] Flags: -c, --clear-comments Do not include instructional comments in source -f, --force Force creation, overwriting existing files -h, --help help for resource -n, --name string Name of the entity -s, --snakename string If skaff doesn't get it right, explicitly give name in snake case (e.g., db_vpc_instance) -o, --v1 Generate code targeting aws-sdk-go v1 (some existing services)","title":"Provider Scaffolding (skaff)"},{"location":"skaff/#provider-scaffolding-skaff","text":"skaff is a Terraform AWS Provider scaffolding command line tool. It generates resource/data source files and accompanying test files which adhere to the latest best practice. These files are heavily commented with instructions so serve as the best way to get started with provider development.","title":"Provider Scaffolding (skaff)"},{"location":"skaff/#overview-workflow-steps","text":"Figure out what you're trying to do: Create a resource or a data source? AWS Go SDK v1 or v2 code? Name of the new resource or data source? Use skaff to generate provider code Go through the generated code completing code and customizing for the AWS Go SDK API Run, test, refine Remove \"TIP\" comments Submit code in pull request","title":"Overview workflow steps"},{"location":"skaff/#running-skaff","text":"Use Git to clone the GitHub https://github.com/hashicorp/terraform-provider-aws repository. cd skaff go install . Change directories to the service where your new resource will reside. E.g. , cd ../internal/service/mq . To get help, enter skaff without arguments. Generate a resource. E.g. , skaff resource --name BrokerReboot (or equivalently skaff resource -n BrokerReboot ).","title":"Running skaff"},{"location":"skaff/#usage","text":"","title":"Usage"},{"location":"skaff/#help","text":"$ skaff --help Usage: skaff [command] Available Commands: completion Generate the autocompletion script for the specified shell datasource Create scaffolding for a data source help Help about any command resource Create scaffolding for a resource Flags: -h, --help help for skaff","title":"Help"},{"location":"skaff/#autocompletion","text":"Generate the autocompletion script for skaff for the specified shell $ skaff completion --help Usage: skaff completion [command] Available Commands: bash Generate the autocompletion script for bash fish Generate the autocompletion script for fish powershell Generate the autocompletion script for powershell zsh Generate the autocompletion script for zsh Flags: -h, --help help for completion Use \"skaff completion [command] --help\" for more information about a command","title":"Autocompletion"},{"location":"skaff/#data-source","text":"Create scaffolding for a data source $ skaff datasource --help Usage: skaff datasource [flags] Flags: -c, --clear-comments Do not include instructional comments in source -f, --force Force creation, overwriting existing files -h, --help help for datasource -n, --name string Name of the entity -s, --snakename string If skaff doesn't get it right, explicitly give name in snake case (e.g., db_vpc_instance) -o, --v1 Generate code targeting aws-sdk-go v1 (some existing services)","title":"Data Source"},{"location":"skaff/#resource","text":"Create scaffolding for a resource $ skaff resource --help Usage: skaff resource [flags] Flags: -c, --clear-comments Do not include instructional comments in source -f, --force Force creation, overwriting existing files -h, --help help for resource -n, --name string Name of the entity -s, --snakename string If skaff doesn't get it right, explicitly give name in snake case (e.g., db_vpc_instance) -o, --v1 Generate code targeting aws-sdk-go v1 (some existing services)","title":"Resource"}]}